{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbf151b-991f-486a-9922-2777dc80ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09322cf1-0843-4be4-8d6f-d7666f21b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd0d167-9c27-488c-82aa-ffe6b7d33e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e2c8c-5474-498d-8a7c-c479c831bb47",
   "metadata": {},
   "source": [
    "## Vector embeddings (Qdrant)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1cca9f-5e31-4dd6-a180-49b7b5d21de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e94b1-0f75-46c1-9361-aef7160f874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b668-c288-465a-a5e1-8874b45b5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.read_json('data/AWSBedrockRAG.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46ab49-79ea-43ea-aa81-1a0ce3cc8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6697a76-4a22-4bfc-a0a8-3d8281653c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d33c471-7b22-432a-8a2b-d2c20ff4a87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'BAAI/bge-base-en',\n",
       "  'sources': {'hf': 'Qdrant/fast-bge-base-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.42,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-base-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-base-en-v1.5-onnx-q',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.21,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-large-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-large-en-v1.5-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-en',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-en-v1.5',\n",
       "  'sources': {'hf': 'qdrant/bge-small-en-v1.5-onnx-q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.067,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'BAAI/bge-small-zh-v1.5',\n",
       "  'sources': {'hf': 'Qdrant/bge-small-zh-v1.5',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "  'sources': {'hf': 'mixedbread-ai/mxbai-embed-large-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-xs',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-xs',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-s',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-s',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m',\n",
       "  'sources': {'hf': 'Snowflake/snowflake-arctic-embed-m',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.43,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-m-long',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 2048 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.54,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'snowflake/snowflake-arctic-embed-l',\n",
       "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-l',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.02,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-clip-v1',\n",
       "  'sources': {'hf': 'jinaai/jina-clip-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/text_model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, Prefixes for queries/documents: not necessary, 2024 year',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.55,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'Qdrant/clip-ViT-B-32-text',\n",
       "  'sources': {'hf': 'Qdrant/clip-ViT-B-32-text',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.25,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sources': {'hf': 'qdrant/all-MiniLM-L6-v2-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-base-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-small-en',\n",
       "  'sources': {'hf': 'xenova/jina-embeddings-v2-small-en',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.12,\n",
       "  'additional_files': [],\n",
       "  'dim': 512,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-de',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-de',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_fp16.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (German, English), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.32,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-code',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-code',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (English, 30 programming languages), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-zh',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Chinese-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v2-base-es',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-es',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), supports mixed Spanish-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.64,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-base',\n",
       "  'sources': {'hf': 'thenlper/gte-base',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'General text embeddings, Unimodal (text), supports English only input text, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.44,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'thenlper/gte-large',\n",
       "  'sources': {'hf': 'qdrant/gte-large-onnx',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 1.2,\n",
       "  'additional_files': [],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1.5-Q',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model_quantized.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'nomic-ai/nomic-embed-text-v1',\n",
       "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.52,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'sources': {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'model_optimized.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2019 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.22,\n",
       "  'additional_files': [],\n",
       "  'dim': 384,\n",
       "  'tasks': {}},\n",
       " {'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'sources': {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 384 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 1.0,\n",
       "  'additional_files': [],\n",
       "  'dim': 768,\n",
       "  'tasks': {}},\n",
       " {'model': 'intfloat/multilingual-e5-large',\n",
       "  'sources': {'hf': 'qdrant/multilingual-e5-large-onnx',\n",
       "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz',\n",
       "   '_deprecated_tar_struct': True},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~100 languages), 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 2.24,\n",
       "  'additional_files': ['model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {}},\n",
       " {'model': 'jinaai/jina-embeddings-v3',\n",
       "  'sources': {'hf': 'jinaai/jina-embeddings-v3',\n",
       "   'url': None,\n",
       "   '_deprecated_tar_struct': False},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'description': 'Multi-task unimodal (text) embedding model, multi-lingual (~100), 1024 tokens truncation, and 8192 sequence length. Prefixes for queries/documents: not necessary, 2024 year.',\n",
       "  'license': 'cc-by-nc-4.0',\n",
       "  'size_in_GB': 2.29,\n",
       "  'additional_files': ['onnx/model.onnx_data'],\n",
       "  'dim': 1024,\n",
       "  'tasks': {'retrieval.query': 0,\n",
       "   'retrieval.passage': 1,\n",
       "   'separation': 2,\n",
       "   'classification': 3,\n",
       "   'text-matching': 4}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc76be65-8bb2-4b45-b237-523e5e032976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"BAAI/bge-small-zh-v1.5\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/bge-small-zh-v1.5\",\n",
      "    \"url\": \"https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz\",\n",
      "    \"_deprecated_tar_struct\": true\n",
      "  },\n",
      "  \"model_file\": \"model_optimized.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.09,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"model.onnx\",\n",
      "  \"description\": \"Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year\",\n",
      "  \"license\": \"mit\",\n",
      "  \"size_in_GB\": 0.25,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n",
      "{\n",
      "  \"model\": \"jinaai/jina-embeddings-v2-small-en\",\n",
      "  \"sources\": {\n",
      "    \"hf\": \"xenova/jina-embeddings-v2-small-en\",\n",
      "    \"url\": null,\n",
      "    \"_deprecated_tar_struct\": false\n",
      "  },\n",
      "  \"model_file\": \"onnx/model.onnx\",\n",
      "  \"description\": \"Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.\",\n",
      "  \"license\": \"apache-2.0\",\n",
      "  \"size_in_GB\": 0.12,\n",
      "  \"additional_files\": [],\n",
      "  \"dim\": 512,\n",
      "  \"tasks\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model['dim'] == EMBEDDING_DIMENSIONALITY:\n",
    "        print(json.dumps(model, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64d6e7c-73c1-4b97-b322-831468da9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cce3a74-3971-475b-b9c1-78ad4c8f6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"bedrock_rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5927acb8-03b3-4125-acb1-502d8f105ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qd_client.delete_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349cef99-c42a-4259-a21c-75a485c219e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
    "        distance=models.Distance.COSINE  # Distance metric for similarity search\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4350165f-3265-4bed-bd49-9c0879435a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"category\",\n",
    "    field_schema=\"keyword\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dafa423d-f4cf-46fa-809e-479135b4b783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=3, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"title\",\n",
    "    field_schema=\"keyword\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ebd8d0-e755-4e7b-bbc4-5883b8ef4ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=5, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"tags\",\n",
    "    field_schema=\"keyword\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44573838-74c0-43a0-a0d4-01ae8d0c2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    text = doc['title'] + ' ' + doc['content']\n",
    "    vector = models.Document(text=text, model=model_handle)\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5346e411-8561-4a93-8fa9-a6e395fff0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=6, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01944085-c347-4f32-9169-57e38a473823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b791ad4-1c08-43fd-95e6-9817cb4a11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(question):\n",
    "    print('vector_search is used')\n",
    "    \n",
    "    query_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=question,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for point in query_points.points:\n",
    "        results.append(point.payload)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fdf2a931-d532-42d1-8776-39b76381d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a aws expert. Answer the QUESTION based on the CONTEXT from our aws core service database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "entry_template = \"\"\"\n",
    "id: {id}\n",
    "service: {service}\n",
    "category: {category}\n",
    "title: {title}\n",
    "content: {content}\n",
    "tags: {tags}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f7eabd5-340f-4563-87fb-a1fbcaad9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    context =\"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + entry_template.format(**doc) + \"\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5c97ff4-18b6-4ab1-95b2-2400cd064c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model):\n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}] }]\n",
    "\n",
    "    inference_config = {\"temperature\": 0.1, \"topP\": 0.9}\n",
    "\n",
    "    response = client.converse(modelId=model, messages=messages, inferenceConfig=inference_config)\n",
    "    \n",
    "    try:\n",
    "        return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02e65161-63cf-4959-9c3d-9916256fc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model):\n",
    "    search_results = vector_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bf0c153-581c-4be1-8e72-459674b220b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context from AWS core service database, the question pertains to the evaluation of multiple approaches in the context of Amazon Bedrock Guardrails. Specifically, the question asks which scenarios involve multiple approaches.\n",
      "\n",
      "The context provided does not explicitly mention any evaluation of multiple approaches for the final LLM (Large Language Model) output. However, it does describe various functionalities related to Amazon Bedrock Guardrails, such as creating versions, adding denied topics, supported regions for safeguard tiers, deleting guardrail versions, and content filters and prompt attacks language support.\n",
      "\n",
      "To answer the question about multiple approaches, we need to infer from the context. The most relevant section that hints at multiple approaches is:\n",
      "\n",
      "**id: 200**\n",
      "service: Amazon Bedrock\n",
      "category: Batch Inference\n",
      "title: Content filters and prompt attacks language support\n",
      "\n",
      "This section discusses the language support for text-based content filters and prompt attacks within Amazon Bedrock Guardrails, distinguishing between the Standard tier and the Classic tier. This implies that there are at least two different approaches (Standard tier and Classic tier) for handling content filters and prompt attacks based on the chosen safeguard tier.\n",
      "\n",
      "Therefore, the multiple approaches mentioned in the context are:\n",
      "\n",
      "1. **Standard tier** for content filters and prompt attacks language support.\n",
      "2. **Classic tier** for content filters and prompt attacks language support.\n",
      "\n",
      "These two tiers represent different approaches to handling content filters and prompt attacks within Amazon Bedrock Guardrails.\n"
     ]
    }
   ],
   "source": [
    "question = \"LLM evaluation 0 points: No evaluation of final LLM output is provided 1 point: Only one approach (e.g., one prompt) is evaluated 2 points: Multiple approaches are evaluated, and the best one is used = can you explain which are multiple appraoches\"\n",
    "model_id = \"amazon.nova-micro-v1:0\"\n",
    "answer = rag(question, model_id)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85babdde-481e-4da2-8e84-0a8c4ec78c01",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c0eba7b-08e9-4d25-808e-bd54f369938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question = pd.read_csv('data/ground-truth-retrieval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec5ecf40-11d6-4ee5-8462-bc7f89d23984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the definition of a foundation model i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>How can a foundation model generate a variety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>What types of data can a foundation model conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>What is required before using an Amazon Bedroc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>What are the different use cases for a foundat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           question\n",
       "0   0  What is the definition of a foundation model i...\n",
       "1   0  How can a foundation model generate a variety ...\n",
       "2   0  What types of data can a foundation model conv...\n",
       "3   0  What is required before using an Amazon Bedroc...\n",
       "4   0  What are the different use cases for a foundat..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_question.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1efd96c-4957-4a8d-bcdc-cd796a68dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_question.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34aaa2ae-7830-418f-a424-4915307ba276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'question': 'What is the definition of a foundation model in Amazon Bedrock?'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fbbe7dc-fb42-4c13-aad9-7feb06d9d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71bb12ea-4a7d-4b87-9cb6-2dc3cdc1f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(question):\n",
    "    #print('vector_search is used')\n",
    "    \n",
    "    query_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=question,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for point in query_points.points:\n",
    "        results.append(point.payload)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e32db751-62ad-412d-aa88-7a3e0639eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f27809f7-68b6-4d7a-9d6c-db4761193188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth, search_function):\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "382ff917-0313-47cc-ad58-c8bb82b190e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "222c8278-2c70-4dd2-8e70-e2b9c1db579f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6eaf4cd76054a0099fed575bbee6c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.10679012345679012, 'mrr': 0.051862139917695484}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: vector_search(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc842596-8145-4899-aa7a-35e9eeea97e9",
   "metadata": {},
   "source": [
    "## RAG evaluation Appraoch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a85d6d40-fa19-4ef4-9203-4be3e8040524",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "Return only valid JSON in this exact format:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\n",
    "Do not include any text before or after the JSON object.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dc6c3a3-f825-4180-90ad-2b99b1e623ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cf5e9ef-3292-4eac-a7a8-577ee1dcbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfe5e0e6-72b9-4c33-b488-9f6e554f9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost for fine-tuning Amazon Titan Text models in Amazon Bedrock depends on the number of epochs used, with each epoch processing the entire training dataset once. Specific hyperparameters like `epochCount` directly impact the customization cost. For precise pricing details, refer to the AWS pricing page for Amazon Bedrock.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1c81d03-2e14-407a-b551-947141c01c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a RAG system.\n",
      "Your task is to analyze the relevance of the generated answer to the given question.\n",
      "Based on the relevance of the generated answer, you will classify it\n",
      "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Question: how much it cost for fine tunning model? answer in concise way.\n",
      "Generated Answer: The cost for fine-tuning Amazon Titan Text models in Amazon Bedrock depends on the number of epochs used, with each epoch processing the entire training dataset once. Specific hyperparameters like `epochCount` directly impact the customization cost. For precise pricing details, refer to the AWS pricing page for Amazon Bedrock.\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the question\n",
      "and provide your evaluation in parsable JSON without using code blocks:\n",
      "\n",
      "Return only valid JSON in this exact format:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n",
      "\n",
      "Do not include any text before or after the JSON object.\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt2_template.format(question=question, answer_llm=answer)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02ad9458-27d9-4518-a8f0-34c4969ad899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89cf14d8-3329-427a-aea1-6061a178a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_question.sample(n=200, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87ba7a7e-dbbc-45ad-8dc7-8ab044e97036",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91c41b41-dee6-4906-9d14-52357ce72e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f99332622024429b4c52663b8c28737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question, model) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt, model)\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62facd85-76c9-4834-b1d2-090a313b3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19628848-9e2e-4c0b-899f-a7e46aa54fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "NON_RELEVANT       0.880\n",
       "PARTLY_RELEVANT    0.085\n",
       "RELEVANT           0.035\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e845df4-9d2e-4298-9f66-1735157a6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('data/rag-eval-claude-3-5-haiku.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b9b004a-c067-4fef-8a51-3e83bebce8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the provided context, there is no spe...</td>\n",
       "      <td>480</td>\n",
       "      <td>What role do custom outputs play in integratin...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Based on the provided CONTEXT, I cannot find s...</td>\n",
       "      <td>543</td>\n",
       "      <td>How does Amazon Bedrock secure data at rest an...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I apologize, but I cannot find specific inform...</td>\n",
       "      <td>121</td>\n",
       "      <td>Which permissions does the service role assume...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I apologize, but based on the provided CONTEXT...</td>\n",
       "      <td>65</td>\n",
       "      <td>Which API operations does the TwelveLabs Pegas...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Based on the provided CONTEXT, I cannot find s...</td>\n",
       "      <td>452</td>\n",
       "      <td>How does Amazon Bedrock Flows facilitate inter...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>I apologize, but based on the provided CONTEXT...</td>\n",
       "      <td>469</td>\n",
       "      <td>What techniques are used for model customizati...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Based on the provided context, I cannot find s...</td>\n",
       "      <td>121</td>\n",
       "      <td>What is the recommended practice for configuri...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>I apologize, but based on the provided CONTEXT...</td>\n",
       "      <td>238</td>\n",
       "      <td>What is required to send a StopEvaluationJob r...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Based on the provided CONTEXT, there is no spe...</td>\n",
       "      <td>99</td>\n",
       "      <td>If I have the `AmazonBedrockFullAccess` policy...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not provide any subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>I apologize, but I cannot find any information...</td>\n",
       "      <td>112</td>\n",
       "      <td>What is the purpose of the example response in...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not provide any subs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                answer   id  \\\n",
       "0    Based on the provided context, there is no spe...  480   \n",
       "1    Based on the provided CONTEXT, I cannot find s...  543   \n",
       "2    I apologize, but I cannot find specific inform...  121   \n",
       "3    I apologize, but based on the provided CONTEXT...   65   \n",
       "4    Based on the provided CONTEXT, I cannot find s...  452   \n",
       "..                                                 ...  ...   \n",
       "195  I apologize, but based on the provided CONTEXT...  469   \n",
       "196  Based on the provided context, I cannot find s...  121   \n",
       "197  I apologize, but based on the provided CONTEXT...  238   \n",
       "198  Based on the provided CONTEXT, there is no spe...   99   \n",
       "199  I apologize, but I cannot find any information...  112   \n",
       "\n",
       "                                              question     relevance  \\\n",
       "0    What role do custom outputs play in integratin...  NON_RELEVANT   \n",
       "1    How does Amazon Bedrock secure data at rest an...  NON_RELEVANT   \n",
       "2    Which permissions does the service role assume...  NON_RELEVANT   \n",
       "3    Which API operations does the TwelveLabs Pegas...  NON_RELEVANT   \n",
       "4    How does Amazon Bedrock Flows facilitate inter...  NON_RELEVANT   \n",
       "..                                                 ...           ...   \n",
       "195  What techniques are used for model customizati...  NON_RELEVANT   \n",
       "196  What is the recommended practice for configuri...  NON_RELEVANT   \n",
       "197  What is required to send a StopEvaluationJob r...  NON_RELEVANT   \n",
       "198  If I have the `AmazonBedrockFullAccess` policy...  NON_RELEVANT   \n",
       "199  What is the purpose of the example response in...  NON_RELEVANT   \n",
       "\n",
       "                                           explanation  \n",
       "0    The generated answer explicitly states that th...  \n",
       "1    The generated answer explicitly states that it...  \n",
       "2    The generated answer explicitly states that it...  \n",
       "3    The generated answer explicitly states that th...  \n",
       "4    The generated answer explicitly states that it...  \n",
       "..                                                 ...  \n",
       "195  The generated answer explicitly states that it...  \n",
       "196  The generated answer explicitly states that it...  \n",
       "197  The generated answer explicitly states that it...  \n",
       "198  The generated answer does not provide any subs...  \n",
       "199  The generated answer does not provide any subs...  \n",
       "\n",
       "[176 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[df_eval.relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73f219ba-4994-4429-b822-51ce25074008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0280060933a348728cedadf8b55952e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"amazon.nova-micro-v1:0\"\n",
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question, model) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt, model)\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33701db6-4c70-4fe8-a59f-e4edde60b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b6121ba-7008-4fd5-b687-fcca13a1dde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "NON_RELEVANT       176\n",
       "PARTLY_RELEVANT     16\n",
       "RELEVANT             8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd6027d4-6dc4-41d2-b619-5ec6655d2590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "NON_RELEVANT       0.88\n",
       "PARTLY_RELEVANT    0.08\n",
       "RELEVANT           0.04\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97ffd491-0b7c-44d4-8e57-78d1ed0fe258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('data/rag-eval-amazon.nova-micro.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fa32f-8442-47c8-9c3a-d324c7949806",
   "metadata": {},
   "source": [
    "## RAG evaluation Appraoch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "64b1aba0-74f5-4d01-ad7f-a68659271e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_2 = \"\"\"\n",
    "You are an expert AWS assistant. Use the context to answer the user query step-by-step and then give a final answer.\n",
    "\n",
    "Example 1:\n",
    "Q: What is S3 used for?\n",
    "A: \n",
    "Step 1: Identify S3 function â object storage.\n",
    "Step 2: Common use cases â backups, static website hosting, data lake.\n",
    "Final Answer: S3 is an object storage service used for backups, hosting, and analytics.\n",
    "\n",
    "Example 2:\n",
    "Q: How to secure an EC2 instance?\n",
    "A:\n",
    "Step 1: Consider network layer â security groups.\n",
    "Step 2: Consider permissions â IAM roles.\n",
    "Step 3: Consider data protection â patching, encryption.\n",
    "Final Answer: Secure EC2 with security groups, IAM roles, patching, and encryption.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "Answer step-by-step, then give Final Answer:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b93a27a3-a5e1-47dc-91d9-2093535961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_2(query, search_results):\n",
    "    context =\"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + entry_template.format(**doc) + \"\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template_2.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "004d0156-d4d7-4378-b884-ec8c24618bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_2(query, model):\n",
    "    search_results = vector_search(query)\n",
    "    prompt = build_prompt_2(query, search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08ee1fb3-f32d-4f60-ac00-a68e7b21cdfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'top_retrieved_passages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM evaluation 0 points: No evaluation of final LLM output is provided 1 point: Only one approach (e.g., one prompt) is evaluated 2 points: Multiple approaches are evaluated, and the best one is used = can you explain which are multiple appraoches\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon.nova-micro-v1:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[1;32mIn[90], line 3\u001b[0m, in \u001b[0;36mrag_2\u001b[1;34m(query, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrag_2\u001b[39m(query, model):\n\u001b[0;32m      2\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m vector_search(query)\n\u001b[1;32m----> 3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_prompt_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     answer \u001b[38;5;241m=\u001b[39m llm(prompt, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[1;32mIn[89], line 7\u001b[0m, in \u001b[0;36mbuild_prompt_2\u001b[1;34m(query, search_results)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[0;32m      5\u001b[0m     context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;241m+\u001b[39m entry_template\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdoc) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_template_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[1;31mKeyError\u001b[0m: 'top_retrieved_passages'"
     ]
    }
   ],
   "source": [
    "question = \"LLM evaluation 0 points: No evaluation of final LLM output is provided 1 point: Only one approach (e.g., one prompt) is evaluated 2 points: Multiple approaches are evaluated, and the best one is used = can you explain which are multiple appraoches\"\n",
    "model_id = \"amazon.nova-micro-v1:0\"\n",
    "answer = rag_2(question, model_id)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458b166-fb06-4b04-82fa-20f1db47c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_2(question, model) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm_2(prompt, model)\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
