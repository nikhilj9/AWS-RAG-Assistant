[
  {
    "id": 0,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Foundation model (FM)",
    "content": "An AI model with a large number of parameters and trained on a massive amount of diverse data. A foundation model can generate a variety of responses for a wide range of use cases. Foundation models can generate text or image, and can also convert input into embeddings. Before you can use an Amazon Bedrock foundation model, you must request access.",
    "tags": [
      "foundation model",
      "FM",
      "AI model"
    ]
  },
  {
    "id": 1,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Base model",
    "content": "A foundation model that is packaged by a provider and ready to use. Amazon Bedrock offers a variety of industry-leading foundation models from leading providers.",
    "tags": [
      "base model",
      "foundation model"
    ]
  },
  {
    "id": 2,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Model inference",
    "content": "The process of a foundation model generating an output (response) from a given input (prompt).",
    "tags": [
      "model inference",
      "inference",
      "response generation"
    ]
  },
  {
    "id": 3,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Prompt",
    "content": "An input provided to a model to guide it to generate an appropriate response or output for the input. A text prompt can be a single line, detailed instructions, or a task for the model. It can contain context, examples of outputs, or text for the model to use in its response. Prompts are used for tasks like classification, question answering, code generation, and creative writing.",
    "tags": [
      "prompt",
      "input",
      "text prompt"
    ]
  },
  {
    "id": 4,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Inference parameters",
    "content": "Values that can be adjusted during model inference to influence a response. These parameters can affect the variety of responses and limit response length or the occurrence of specific sequences.",
    "tags": [
      "inference parameters",
      "model control"
    ]
  },
  {
    "id": 5,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Playground",
    "content": "A user-friendly graphical interface in the AWS Management Console for experimenting with running model inference. It allows users to test different models, configurations, and inference parameters on generated responses for various prompts.",
    "tags": [
      "playground",
      "console",
      "graphical interface",
      "experimentation"
    ]
  },
  {
    "id": 6,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Embedding",
    "content": "The process of condensing information by transforming input into a vector of numerical values, known as embeddings. This allows for comparing the similarity between different objects (e.g., sentences, images, or text and image combinations) using a shared numerical representation.",
    "tags": [
      "embedding",
      "vector",
      "numerical representation"
    ]
  },
  {
    "id": 7,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Retrieval augmented generation (RAG)",
    "content": "A process involving querying and retrieving information from a data source, augmenting a prompt with this information to provide better context to the foundation model, and then obtaining an improved response from the foundation model using that additional context.",
    "tags": [
      "RAG",
      "retrieval augmented generation",
      "data source",
      "context"
    ]
  },
  {
    "id": 8,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Model customization",
    "content": "The process of using training data to adjust the model parameter values in a base model to create a custom model. Examples include Fine-tuning (with labeled data) and Continued Pre-training (with unlabeled data).",
    "tags": [
      "model customization",
      "fine-tuning",
      "continued pre-training",
      "custom model"
    ]
  },
  {
    "id": 9,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Hyperparameters",
    "content": "Values that can be adjusted for model customization to control the training process and, consequently, the output custom model.",
    "tags": [
      "hyperparameters",
      "model training",
      "customization control"
    ]
  },
  {
    "id": 10,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Model evaluation",
    "content": "The process of evaluating and comparing model outputs to determine the model best suited for a particular use case.",
    "tags": [
      "model evaluation",
      "performance",
      "model comparison"
    ]
  },
  {
    "id": 11,
    "service": "Amazon Bedrock",
    "category": "Terminology",
    "title": "Provisioned Throughput",
    "content": "A purchased level of throughput for a base or custom model to increase the amount and/or rate of tokens processed during model inference. Purchasing Provisioned Throughput creates a provisioned model for inference.",
    "tags": [
      "provisioned throughput",
      "throughput",
      "capacity",
      "model invocation"
    ]
  },
  {
    "id": 12,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Getting Started Overview",
    "content": "To begin using Amazon Bedrock, you must sign up for an AWS account if you don't already have one, then create an AWS Identity and Access Management (IAM) role with the necessary permissions for Amazon Bedrock, and request access to the foundation models (FMs) you intend to use. For new AWS users, creating an account involves an online signup process and phone verification, which establishes an AWS account root user. As a security best practice, it's recommended to assign administrative access to a regular IAM user and use the root user only for tasks that specifically require root user access. \n\nRecommended initial steps include familiarizing yourself with Amazon Bedrock's terminology and concepts, understanding its pricing structure, and trying out the Getting started tutorials. These tutorials guide you through using the playgrounds in the Amazon Bedrock console and making calls to the Amazon Bedrock API using the AWS SDK. Finally, you should read the documentation for the specific features you plan to incorporate into your applications.",
    "tags": [
      "getting started",
      "account setup",
      "IAM",
      "foundation models",
      "tutorials",
      "pricing",
      "API",
      "SDK"
    ]
  },
  {
    "id": 13,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Requesting Access to Amazon Bedrock Foundation Models",
    "content": "After configuring your Amazon Bedrock IAM role, you can request access to foundation models through the Amazon Bedrock console. Most FMs require explicit access requests and are not granted by default. However, some models like OpenAI gpt-oss-120b and gpt-oss-20b can be accessed directly by selecting them from the model catalog in the playground.\n\nTo request access:\n1.  Sign in to the AWS Management Console and switch to the Amazon Bedrock role you've set up.\n2.  Open the Amazon Bedrock console at `https://console.aws.amazon.com/bedrock/`.\n3.  Ensure you are in the US East (N. Virginia) (us-east-1) Region for tutorial purposes.\n\nFor Anthropic models, you will need to describe your use case details and agree to the terms. Note that subscribing to a model does not incur charges; pricing applies only when you use the model for inference or other actions. AWS GovCloud (US) users follow a specific process, first enabling model access via their standard AWS account ID in us-east-1 or us-west-2, then logging into their GovCloud account in us-gov-west-1 to grant regional entitlement. If you lack the necessary permissions to request model access, an error banner will appear, and you should contact your account administrator.",
    "tags": [
      "model access",
      "foundation models",
      "console",
      "IAM",
      "prerequisites",
      "Anthropic",
      "OpenAI",
      "GovCloud"
    ]
  },
  {
    "id": 14,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Exploring Amazon Bedrock Features via Console or API",
    "content": "Once you have requested access to the desired foundation models, you are ready to explore the various capabilities Amazon Bedrock offers. You can experiment with prompts and configurations to generate responses by sending prompts with different settings and foundation models. This experimentation can be performed through two main interfaces:\n*   The text, image, and chat playgrounds in the AWS Management Console provide a graphical interface for interactive testing.\n*   The Amazon Bedrock API allows for programmatic exploration and integration into applications.\n\nWhen your experimentation is complete and you are ready to deploy, you can configure your application to make requests to the InvokeModel APIs. For detailed guidance, you can proceed to sections on 'Getting started in the Amazon Bedrock console' for playground usage, 'Getting started with the API' for programmatic access, or 'Using Amazon Bedrock with an AWS SDK' for development with SDKs.",
    "tags": [
      "features",
      "console",
      "API",
      "playgrounds",
      "InvokeModel",
      "experimentation"
    ]
  },
  {
    "id": 15,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Getting Started in the Amazon Bedrock Console",
    "content": "This section guides you on using the playgrounds within the AWS Management Console to interact with Amazon Bedrock foundation models (FMs) and generate text or image responses.\n\nPrerequisites for Console Playgrounds:\n*   You must have an AWS account with permissions to access a role that has the necessary Amazon Bedrock permissions.\n*   You need to have requested access to specific models, such as Amazon Titan Text G1 - Express and Amazon Titan Image Generator G1 V1.\n*   You should be operating in the US East (N. Virginia) (us-east-1) Region.\n\nExplore the Text Playground:\n1.  Sign in to the AWS Management Console and open the Amazon Bedrock console.\n2.  From the left navigation pane, select Text under Playgrounds.\n3.  Choose 'Select model' and pick a provider and model (e.g., Amazon Titan Text G1 - Lite).\n4.  Select a default prompt or enter your own (e.g., 'Describe the purpose of a \"hello world\" program in one line.').\n5.  Choose 'Run' to execute inference on the model and see the generated text.\n\nExplore the Image Playground:\n1.  Sign in to the AWS Management Console and open the Amazon Bedrock console.\n2.  (Implicitly, from the left navigation pane, select Image under Playgrounds to access image generation features.)",
    "tags": [
      "console",
      "playgrounds",
      "text playground",
      "image playground",
      "Titan Text",
      "Titan Image Generator",
      "prerequisites",
      "model inference"
    ]
  },
  {
    "id": 16,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Getting Started with the Amazon Bedrock API",
    "content": "To interact with Amazon Bedrock programmatically, you'll need to set up your environment to make API requests. This typically involves using the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon SageMaker AI notebooks. A crucial step is obtaining credentials to grant programmatic access.\n\nGet Credentials for Programmatic Access:\nProgrammatic access is essential for interacting with AWS outside of the AWS Management Console. Options include long-term access keys for IAM users, temporary credentials for IAM roles, or workforce identity through IAM Identity Center. For IAM users, it's a security best practice to limit access keys with restrictive inline policies, ideally to a maximum of 12 hours, and to manage them securely, avoiding storage in plaintext. AWS recommends using temporary credentials via AWS IAM Identity Center for human users in production environments.\n\nAttach Amazon Bedrock Permissions to a User or Role:\nAfter establishing programmatic access credentials, you must configure permissions. This involves:\n1.  Navigating to the IAM console, selecting your user or role.\n2.  In the Permissions tab, choosing 'Add permissions' and then 'Add AWS managed policy', selecting the `AmazonBedrockFullAccess` AWS managed policy.\n3.  To allow subscription to models, create an inline policy permitting `aws-marketplace:ViewSubscriptions`, `aws-marketplace:Unsubscribe`, and `aws-marketplace:Subscribe` actions across all resources (`\"Resource\": \"*\"`).\n\nRequest Access to Amazon Bedrock Models:\nBefore making API calls, ensure you have requested access to the desired Amazon Bedrock models via the console, as outlined in the 'Requesting Access to Amazon Bedrock Foundation Models' section.\n\nTry Making API Calls to Amazon Bedrock:\nOnce prerequisites are met, you can test model invocation requests using various methods:",
    "tags": [
      "API",
      "AWS CLI",
      "AWS SDKs",
      "SageMaker AI notebooks",
      "programmatic access",
      "credentials",
      "IAM",
      "permissions",
      "security",
      "IAM Identity Center",
      "access keys"
    ]
  },
  {
    "id": 17,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Getting Started with a 30-day Amazon Bedrock API Key",
    "content": "For quick experimentation with Amazon Bedrock, you can generate a long-term Amazon Bedrock API key that expires in 30 days. This method simplifies initial access without requiring complex AWS credential setup. \n\nImportant Warning: Long-term API keys are only recommended for exploration and development purposes. For production applications, you should transition to more secure authentication methods such as IAM roles or temporary credentials.\n\nTo create a 30-day API key:\n1.  Sign in to the AWS Management Console with an IAM principal that has Bedrock console permissions.\n2.  Open the Amazon Bedrock console and select 'API keys' from the left navigation pane.\n3.  In the 'Long-term API keys' tab, choose 'Generate long-term API keys'.\n4.  In the 'API key expiration' section, select '30 days'.\n5.  Choose 'Generate'. The key will be granted permissions defined in the `AmazonBedrockLimitedAccess` policy.\n6.  Copy the generated API key and store it securely, as it is displayed only once.\n\nYou can then use this API key by setting it as an environment variable (`AWS_BEARER_TOKEN_BEDROCK`) or by including it in the `Authorization: Bearer ${api-key}` header of your API requests. Example code snippets are provided for Python, HTTP client (requests package), and cURL to demonstrate making a simple Converse API call. After initial exploration, remember to move to more secure authentication methods for ongoing use.",
    "tags": [
      "API keys",
      "authentication",
      "development",
      "exploration",
      "security",
      "long-term API key",
      "Converse API"
    ]
  },
  {
    "id": 18,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Running Examples with the AWS CLI",
    "content": "To test your Amazon Bedrock permissions and authentication setup using the AWS Command Line Interface (CLI), you can run common operations. \n\nPrerequisites:\n*   An AWS account with a user or role configured for authentication and necessary Amazon Bedrock permissions.\n*   Access to the Amazon Titan Text G1 - Express model.\n*   The AWS CLI installed and authenticated with your credentials.\n\nExamples of CLI commands include:\n*   `aws bedrock list-foundation-models`: Lists available foundation models in your Region.\n*   `aws bedrock-runtime invoke-model`: Submits a text prompt to a model (e.g., `amazon.titan-text-express-v1`) to generate a response, which is written to a specified file.\n*   `aws bedrock-runtime converse`: Submits a text prompt to a model (e.g., `amazon.titan-text-express-v1`) to generate a response. This operation is recommended over `InvokeModel` for unifying inference requests and simplifying multi-turn conversations.",
    "tags": [
      "AWS CLI",
      "examples",
      "model inference",
      "Titan Text",
      "InvokeModel",
      "Converse",
      "foundation models",
      "authentication",
      "permissions"
    ]
  },
  {
    "id": 19,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Running Examples with the AWS SDK for Python (Boto3)",
    "content": "You can test Amazon Bedrock operations with the AWS SDK for Python (Boto3) to verify your permissions and authentication. \n\nPrerequisites:\n*   An AWS account with a user or role configured for authentication and necessary Amazon Bedrock permissions.\n*   Access to the Amazon Titan Text G1 - Express model.\n*   The AWS SDK for Python (Boto3) installed and authenticated with your credentials.\n\nPython script examples demonstrate common operations:\n*   `list_foundation_models(bedrock_client)`: Fetches and prints a list of available FMs in your configured AWS Region.\n*   `InvokeModel` operation: Submits a text prompt to a model like `amazon.titan-text-express-v1` to generate a text response.\n*   `Converse` operation: Submits a text message to a model like `amazon.titan-text-express-v1` for conversational responses. The `Converse` operation is generally recommended over `InvokeModel` for its unified inference request structure and simplified multi-turn conversation management.",
    "tags": [
      "AWS SDK",
      "Python",
      "Boto3",
      "examples",
      "model inference",
      "Titan Text",
      "InvokeModel",
      "Converse",
      "foundation models",
      "authentication",
      "permissions"
    ]
  },
  {
    "id": 20,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Running Examples with an Amazon SageMaker AI Notebook",
    "content": "To test your Amazon Bedrock role permissions, you can run examples using an Amazon SageMaker AI notebook.\n\nPrerequisites:\n*   An AWS account with a role having the necessary Amazon Bedrock permissions.\n*   Access to the Amazon Titan Text G1 - Express model.\n*   IAM permissions for SageMaker AI configured, and a notebook instance created.\n\nSteps to set up IAM for SageMaker AI and create a notebook:\n1.  Modify the trust policy of your Amazon Bedrock role to allow both `bedrock.amazonaws.com` and `sagemaker.amazonaws.com` services to assume the role.\n2.  Sign in to the Amazon Bedrock role with the modified trust policy.\n3.  Create an Amazon SageMaker AI Notebook Instance, specifying the ARN of your Amazon Bedrock role.\n4.  Once the notebook instance status is 'InService', choose 'Open JupyterLab'.\n\nWithin the SageMaker AI notebook, you can execute Python SDK scripts to:\n*   List the foundation models available in Amazon Bedrock.\n*   Submit text prompts to a model (e.g., `amazon.titan-text-express-v1`) and generate responses using the `Converse` operation.",
    "tags": [
      "SageMaker AI",
      "notebooks",
      "examples",
      "IAM",
      "trust policy",
      "JupyterLab",
      "model inference",
      "Titan Text",
      "Converse",
      "prerequisites"
    ]
  },
  {
    "id": 21,
    "service": "Amazon Bedrock",
    "category": "Getting Started",
    "title": "Working with AWS SDKs",
    "content": "AWS provides Software Development Kits (SDKs) for various popular programming languages to streamline the process of building applications with Amazon Bedrock. Each SDK typically includes an API, code examples, and comprehensive documentation to assist developers.\n\nAmazon Bedrock supports a wide array of AWS SDKs, including:\n*   AWS SDK for C++\n*   AWS CLI\n*   AWS SDK for Go\n*   AWS SDK for Java\n*   AWS SDK for JavaScript\n*   AWS SDK for Kotlin\n*   AWS SDK for .NET\n*   AWS SDK for PHP\n*   AWS Tools for PowerShell\n*   AWS SDK for Python (Boto3)\n*   AWS SDK for Ruby\n*   AWS SDK for Rust\n*   AWS SDK for SAP ABAP\n*   AWS SDK for Swift\n\nThese SDKs facilitate interactions with Amazon Bedrock, providing the necessary tools to integrate foundation models into your applications efficiently.",
    "tags": [
      "AWS SDKs",
      "development",
      "programming languages",
      "Boto3",
      "CLI",
      "API"
    ]
  },
  {
    "id": 22,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Generate Amazon Bedrock API keys for Easy Authentication",
    "content": "You can easily make calls to the Amazon Bedrock API by generating an Amazon Bedrock API key and using it to authenticate your identity when making requests to the Amazon Bedrock API. API calls made using Amazon Bedrock API keys are logged in AWS CloudTrail, but the API keys themselves are passed as authorization headers and are not logged. These keys are limited to Amazon Bedrock and Amazon Bedrock Runtime actions and cannot be used with `InvokeModelWithBidirectionalStream`, Agents for Amazon Bedrock, or Data Automation for Amazon Bedrock API operations.",
    "tags": [
      "API keys",
      "authentication",
      "easy access",
      "CloudTrail",
      "limitations"
    ]
  },
  {
    "id": 23,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "How Amazon Bedrock API keys work",
    "content": "Amazon Bedrock API keys offer an alternative authentication flow compared to the default process of creating an identity in AWS IAM Identity Center or IAM and generating general AWS credentials. Two types of API keys can be generated specifically for Amazon Bedrock:\n\n*   Short-term key: This is a secure option that allows temporary access to Amazon Bedrock. It is valid for the shorter of 12 hours or the duration of the session generated by the IAM principal used to create it. Short-term keys inherit the permissions attached to the principal and can only be used in the AWS Region from which they were generated. This option is preferred for production environments requiring regular credential changes for greater security.\n*   Long-term key: Recommended only for exploration and development of Amazon Bedrock. You can set an expiration time for these keys. When a long-term key is generated, an underlying IAM user is created, specific IAM policies are attached to it, and the key is associated with that user. After generation, you can modify the IAM user's permissions through the IAM service. AWS strongly recommends switching to short-term credentials for applications with greater security requirements.",
    "tags": [
      "API key mechanism",
      "short-term",
      "long-term",
      "authentication flow",
      "security best practices"
    ]
  },
  {
    "id": 24,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Supported Regions and AWS software development kits (SDKs)",
    "content": "Amazon Bedrock API keys are supported in various AWS Regions, including US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo, Seoul, Osaka, Mumbai, Hyderabad, Singapore, Sydney), Canada (Central), Europe (Frankfurt, Zurich, Stockholm, Milan, Spain, Ireland, London, Paris), and South America (São Paulo).\n\nThe supported AWS SDKs for Amazon Bedrock API keys include:\n*   Python\n*   Javascript\n*   Java\n\nAs noted previously, Amazon Bedrock API keys are limited to Amazon Bedrock and Amazon Bedrock Runtime actions and cannot be used with specific API operations like `InvokeModelWithBidirectionalStream`, Agents for Amazon Bedrock, or Data Automation for Amazon Bedrock API operations.",
    "tags": [
      "supported regions",
      "SDKs",
      "Python",
      "Javascript",
      "Java",
      "limitations"
    ]
  },
  {
    "id": 25,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Generate an Amazon Bedrock API key (General)",
    "content": "You can generate an Amazon Bedrock API key using either the AWS Management Console or the AWS API. For ease of use, it is recommended to use the AWS Management Console to generate an API key with fewer steps.",
    "tags": [
      "API key generation",
      "console",
      "API"
    ]
  },
  {
    "id": 26,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Generate an Amazon Bedrock API key using the console",
    "content": "To generate an API key using the console, sign in to the AWS Management Console with an IAM principal that has Bedrock console permissions and open the Amazon Bedrock console. Navigate to the 'API keys' section in the left navigation pane.\n\n*   For a short-term API key, select the 'Short-term API keys' tab and choose 'Generate short-term API keys'. This key will expire with your console session (up to 12 hours) and can only be used in the AWS Region where it was generated.\n*   For a long-term API key, select the 'Long-term API keys' tab and choose 'Generate long-term API keys'. You must specify an expiration time for the key. By default, the `AmazonBedrockLimitedAccess` AWS-managed policy is attached to the associated IAM user, granting access to core Amazon Bedrock API operations. You can optionally add more policies in the 'Advanced permissions' section. After generation, copy and securely store the API key as it is displayed only once. Remember that long-term API keys are strongly recommended only for exploration.",
    "tags": [
      "console generation",
      "short-term key",
      "long-term key",
      "expiration",
      "IAM policies"
    ]
  },
  {
    "id": 27,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Generate a long-term Amazon Bedrock API key using the API",
    "content": "The general steps for creating a long-term Amazon Bedrock API key via the API involve IAM operations:\n1.  Create an IAM user by sending a `CreateUser` request.\n2.  Attach policies like `AmazonBedrockLimitedAccess` to the IAM user using an `AttachUserPolicy` request. You can attach other managed or custom policies. As a best security practice, it is strongly recommended to attach IAM policies to restrict the use of API keys, for instance, by time-bounding or restricting IP addresses.\n3.  Generate the long-term API key by sending a `CreateServiceSpecificCredential` request with an IAM endpoint, specifying `bedrock.amazonaws.com` as the `ServiceName`. The `ServiceApiKeyValue` in the response is your API key, and the `ServiceSpecificCredentialId` can be used for key-related API operations.\n\nCLI Example:\n```bash\naws iam create-user --user-name bedrock-api-user\naws iam attach-user-policy --user-name bedrock-api-user --policy-arn arn:aws:iam::aws:policy/AmazonBedrockLimitedAccess\naws iam create-service-specific-credential \\ --user-name bedrock-api-user \\ --service-name bedrock.amazonaws.com \\ --credential-age-days ${NUMBER-OF-DAYS}\n```\n\nPython Example:\nUsing `boto3`, you can create an IAM user, attach policies, and then create the service-specific credential to obtain the API key. Ensure your Python setup automatically recognizes your AWS credentials.",
    "tags": [
      "long-term key",
      "API generation",
      "IAM",
      "CLI",
      "Python",
      "security best practices"
    ]
  },
  {
    "id": 28,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Generate a short-term Amazon Bedrock API key using a client library",
    "content": "Short-term API keys are valid for a maximum of 12 hours or the duration of the IAM principal's session, inherit the principal's permissions, and are tied to the AWS Region of generation.\n\nPrerequisites:\n*   The IAM principal generating the key must have the necessary permissions (e.g., `AmazonBedrockLimitedAccess`).\n*   Your setup must allow Python to automatically recognize AWS credentials, following the standardized credential providers hierarchy.\n*   Install the `aws-bedrock-token-generator` client library for your preferred language.\n\nInstallation Examples:\n*   Python: `pip install aws-bedrock-token-generator`\n*   Javascript: `npm install @aws/bedrock-token-generator`\n*   Java (Maven): Add dependency `software.amazon.bedrock:aws-bedrock-token-generator:1.1.0` to `pom.xml`\n\nUsage Example (Python):\n```python\nfrom aws_bedrock_token_generator import provide_token\ntoken = provide_token()\nprint(f\"Token: {token}\")\n```\n\nFor long-running applications, this library can automatically create new short-term keys as credentials refresh.",
    "tags": [
      "short-term key",
      "client library",
      "Python",
      "Javascript",
      "Java",
      "prerequisites"
    ]
  },
  {
    "id": 29,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Set up automatic refresh of short-term Amazon Bedrock API keys",
    "content": "You can create a script using the `aws-bedrock-token-generator` package to programmatically regenerate new short-term API keys when the current one expires. This ensures continuous authentication for your applications.\n\nPython Example (fetches new token for each API call, which is inexpensive):\n```python\nfrom aws_bedrock_token_generator import provide_token\nimport requests\n\ndef get_new_token():\n    url = \"https://bedrock-runtime.us-west-2.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse\"\n    payload = {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"text\": \"Hello\"}]\n            }\n        ]\n    }\n    token = provide_token()  # Create a token provider that uses default credentials and region providers.\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {token}\"\n    }\n    response = requests.post(url, headers=headers, json=payload)\n    print(response.json())\n\nif __name__ == \"__main__\":\n    get_new_token()\n```\n\nSimilar examples are provided for Javascript and Java, demonstrating how `getTokenProvider()` or `tokenGenerator.getToken()` can be used to retrieve a valid token for API calls.",
    "tags": [
      "automatic refresh",
      "short-term key",
      "token generator",
      "Python",
      "Javascript",
      "Java"
    ]
  },
  {
    "id": 30,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Use an Amazon Bedrock API key",
    "content": "You can use your Amazon Bedrock API key in two primary ways:\n\n1.  Set it as an environment variable: The Amazon Bedrock service recognizes the environment variable `AWS_BEARER_TOKEN_BEDROCK`. You can set it in your terminal (e.g., `export AWS_BEARER_TOKEN_BEDROCK=${api-key}` for MacOS/Linux, or `setx AWS_BEARER_TOKEN_BEDROCK \"${api-key}\"` for Windows). Alternatively, set it in your code before making an API request, such as `os.environ['AWS_BEARER_TOKEN_BEDROCK'] = \"${api-key}\"` in Python.\n\n2.  Specify it in a request: Include the API key in the authorization header as `Authorization: Bearer $AWS_BEARER_TOKEN_BEDROCK` for direct HTTP requests. For supported SDKs, you can specify the value in a parameter like `api_key` when setting up the client (e.g., with the OpenAI Python SDK).\n\nImportant: Amazon Bedrock API keys are limited to Amazon Bedrock and Amazon Bedrock Runtime actions and cannot be used with `InvokeModelWithBidirectionalStream`, Agents for Amazon Bedrock, or Data Automation for Amazon Bedrock API operations.\n\nPython (boto3) Example:\n```python\nimport os\nimport boto3\n\nos.environ['AWS_BEARER_TOKEN_BEDROCK'] = \"${api-key}\" # Or set externally\nclient = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\nmodel_id = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\nmessages = [{\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}]\nresponse = client.converse(modelId=model_id, messages=messages)\nprint(response)\n```\n\ncURL Example:\n```bash\ncurl -X POST \"https://bedrock-runtime.us-east-1.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $AWS_BEARER_TOKEN_BEDROCK\" \\\n  -d '{\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [{\"text\": \"Hello\"}]\n        }\n    ]\n  }'\n```",
    "tags": [
      "API key usage",
      "environment variable",
      "authorization header",
      "Python SDK",
      "cURL",
      "limitations"
    ]
  },
  {
    "id": 31,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Modify permissions for Amazon Bedrock API keys",
    "content": "When you generate a long-term Amazon Bedrock API key, an IAM user is associated with it. To modify the permissions for this key, you must modify the permissions for the associated IAM user through the IAM service. If you generated the key in the AWS Management Console, the `AmazonBedrockLimitedAccess` policy is attached by default. If you plan to set custom permissions, it is recommended to remove this default policy first.",
    "tags": [
      "permissions",
      "IAM user",
      "long-term key",
      "modify access"
    ]
  },
  {
    "id": 32,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Example of modifying permissions for API keys",
    "content": "To replace the `AmazonBedrockLimitedAccess` policy with a more restrictive one via the console:\n1.  Sign in to the AWS Management Console with an IAM principal having Bedrock console permissions and open the Amazon Bedrock console.\n2.  Navigate to 'API keys', select the 'Long-term API keys' tab, and choose your API key. Then, select 'Manage in IAM Console'.\n3.  In the Permissions tab, remove the `AmazonBedrockLimitedAccess` policy. At this point, the API key will have no permissions.\n4.  Add a new inline policy with specific permissions. For example, to only allow inference with the US Anthropic Claude 3 Haiku inference profile in US West (Oregon), you would paste a JSON policy similar to this into the editor:\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:CallWithBearerToken\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:bedrock:us-west-2:111122223333:inference-profile/us.anthropic.claude-3-haiku-20240307-v1:0\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\",\n                \"arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\"\n            ],\n            \"Condition\": {\n                \"StringLike\": {\n                    \"bedrock:InferenceProfileArn\": \"arn:aws:bedrock:us-west-2:111122223333:inference-profile/us.anthropic.claude-3-haiku-20240307-v1:0\"\n                }\n            }\n        }\n    ]\n}\n```\n5.  Name and create the policy. This allows the user to only run inference with the specified profile in US West (Oregon).",
    "tags": [
      "permissions example",
      "IAM policy",
      "console",
      "restrict access"
    ]
  },
  {
    "id": 33,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Handle compromised Amazon Bedrock API keys",
    "content": "If an Amazon Bedrock API key becomes compromised, you should immediately revoke its permissions. There are different methods depending on the key type:\n\n*   Long-term API keys: You can revoke permissions using the `UpdateServiceSpecificCredential`, `ResetServiceSpecificCredential`, or `DeleteServiceSpecificCredential` API operations. These actions allow you to:\n    *   Change the status of the key to inactive (and reactivate later).\n    *   Reset the key to generate a new password.\n    *   Delete the key permanently.\n    Note that performing these actions via API requires authentication with AWS credentials, not the Amazon Bedrock API key itself.\n*   Both long-term and short-term API keys: You can attach IAM policies to revoke permissions for both types of keys.",
    "tags": [
      "security",
      "compromised keys",
      "revocation",
      "long-term key",
      "short-term key"
    ]
  },
  {
    "id": 34,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Change the status of a long-term Amazon Bedrock API key",
    "content": "To change the status of a long-term API key:\n\n*   Console: Sign in to the AWS Management Console, navigate to 'API keys' in Amazon Bedrock, select the desired key, choose 'Actions', and then select 'Deactivate' or 'Reactivate' as needed.\n*   Python (API): Send an `UpdateServiceSpecificCredential` request with an IAM endpoint, specifying the `service_specific_credential_id` and setting the `status` to `\"Inactive\"` or `\"Active\"`.\n```python\nimport boto3\niam_client = boto3.client(\"iam\")\niam_client.update_service_specific_credential(\n    service_specific_credential_id=${ServiceSpecificCredentialId},\n    status=\"Inactive\"\n)\n```",
    "tags": [
      "long-term key",
      "deactivate",
      "reactivate",
      "console",
      "Python"
    ]
  },
  {
    "id": 35,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Reset a long-term Amazon Bedrock API key",
    "content": "To reset a long-term API key (which generates a new password for it):\n\n*   Console: Sign in to the AWS Management Console, navigate to 'API keys' in Amazon Bedrock, select the desired key, choose 'Actions', and then select 'Reset key'.\n*   Python (API): Send a `ResetServiceSpecificCredential` request with an IAM endpoint, providing the `service_specific_credential_id`.\n```python\nimport boto3\niam_client = boto3.client(\"iam\")\niam_client.reset_service_specific_credential(\n    service_specific_credential_id=${ServiceSpecificCredentialId}\n)\n```",
    "tags": [
      "long-term key",
      "reset key",
      "console",
      "Python"
    ]
  },
  {
    "id": 36,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Delete a long-term Amazon Bedrock API key",
    "content": "To delete a long-term API key permanently:\n\n*   Console: Sign in to the AWS Management Console, navigate to 'API keys' in Amazon Bedrock, select the desired key, choose 'Actions', and then select 'Delete'.\n*   Python (API): Send a `DeleteServiceSpecificCredential` request with an IAM endpoint, providing the `service_specific_credential_id`.\n```python\nimport boto3\niam_client = boto3.client(\"iam\")\niam_client.delete_service_specific_credential(\n    service_specific_credential_id=${ServiceSpecificCredentialId}\n)\n```",
    "tags": [
      "long-term key",
      "delete key",
      "console",
      "Python"
    ]
  },
  {
    "id": 37,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Attach IAM policies to remove permissions for using an Amazon Bedrock API key",
    "content": "You can use IAM policies to explicitly remove permissions for using Amazon Bedrock API keys.\n\n*   Deny the ability to make calls with an API key: The action that allows an identity to make calls with an Amazon Bedrock API key is `bedrock:CallWithBearerToken`. To prevent an identity from using the API key, attach an IAM policy that denies this action. For long-term keys, attach it to the associated IAM user. For short-term keys, attach it to the IAM role used to generate the key.\n    Example Policy:\n    ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": {\n        \"Effect\": \"Deny\",\n        \"Action\": \"bedrock:CallWithBearerToken\",\n        \"Resource\": \"*\"\n      }\n    }\n    ```\n\n*   Invalidate an IAM role session (for short-term keys): If a short-term key is compromised, you can prevent its usage by invalidating the role session that generated it. Attach a policy to the IAM identity that generated the key, using a `DateLessThan` condition on `aws:TokenIssueTime` to specify a time after which the session becomes invalid.\n    Example Policy:\n    ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": {\n        \"Effect\": \"Deny\",\n        \"Action\": \"*\",\n        \"Resource\": \"*\",\n        \"Condition\": {\n          \"DateLessThan\": {\"aws:TokenIssueTime\": \"2014-05-07T23:47:00Z\"}\n        }\n      }\n    }\n    ```",
    "tags": [
      "IAM policies",
      "deny access",
      "revoke permissions",
      "short-term key",
      "long-term key",
      "CallWithBearerToken",
      "session invalidation"
    ]
  },
  {
    "id": 38,
    "service": "Amazon Bedrock",
    "category": "API Keys",
    "title": "Control permissions for generating and using Amazon Bedrock API keys",
    "content": "The following IAM actions control the generation and use of Amazon Bedrock API keys:\n*   `iam:CreateServiceSpecificCredentials`: Controls the generation of a service-specific key, such as a long-term Amazon Bedrock API key.\n*   `bedrock:CallWithBearerToken`: Controls the use of both short-term and long-term Amazon Bedrock API keys.\n\nNote: You cannot prevent the generation of a short-term key because it uses existing session credentials; however, you can prevent its usage by denying the `bedrock:CallWithBearerToken` action on the generating identity.\n\nSummary of Prevention Strategies:\n| Purpose                       | Long-term key                                                                  | Short-term key                                                                    |\n|-------------------------------|--------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n| Prevent generation of keys  | Attach a policy denying `iam:CreateServiceSpecificCredential` to an IAM identity | N/A                                                                               |\n| Prevent usage of a key      | Attach a policy denying `bedrock:CallWithBearerToken` to the IAM user associated with the key | Attach a policy denying `bedrock:CallWithBearerToken` to IAM identities that you don't want to use the key |\n\nExample Policy (to prevent both generation and usage for an IAM identity):\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\":\"DenyBedrockShortAndLongTermAPIKeys\",\n      \"Effect\": \"Deny\",\n      \"Action\": [\n        \"iam:CreateServiceSpecificCredential\",\n        \"bedrock:CallWithBearerToken\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n```\n\nWarning: This policy will prevent the creation of credentials for *all* AWS services that support service-specific credentials, not just Amazon Bedrock.",
    "tags": [
      "control permissions",
      "generation",
      "usage",
      "IAM actions",
      "CreateServiceSpecificCredentials",
      "CallWithBearerToken",
      "deny policy"
    ]
  },
  {
    "id": 39,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Access Foundation Models",
    "content": "Access to Amazon Bedrock foundation models is not granted by default, with the exception of OpenAI gpt-oss-120b and gpt-oss-20b models, which can be accessed directly from the model catalog in the console. For other models, you must request or modify access using the Amazon Bedrock console. It is crucial that your IAM identity has sufficient permissions to manage foundation models. Requesting access to a model does not incur charges. Once access is granted to a model, it becomes available for all users within that AWS account. If an identity already has access to a model, it is recommended to control permissions to use the model rather than permissions to request access.",
    "tags": [
      "access",
      "foundation models",
      "IAM",
      "OpenAI models",
      "console",
      "permissions"
    ]
  },
  {
    "id": 40,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Grant Permissions to Request Access to Foundation Models",
    "content": "Access to Amazon Bedrock serverless foundation models is controlled by specific IAM actions. To request access to models, an IAM identity must have a policy attached that allows the bedrock:PutFoundationModelEntitlement action. This action allows an IAM identity to request access to all Amazon Bedrock serverless models. Additionally, if the model has a product ID in AWS Marketplace, the aws-marketplace:Subscribe action is also required. Other relevant AWS Marketplace actions include aws-marketplace:Unsubscribe (to unsubscribe from models) and aws-marketplace:ViewSubscriptions (to list subscribed products). For the aws-marketplace:Subscribe action, the aws-marketplace:ProductId condition key can be used to restrict subscription to specific models. If an identity has already subscribed to a model in one AWS Region, they only need permissions for bedrock:PutFoundationModelEntitlement to request access in other Regions.",
    "tags": [
      "IAM permissions",
      "request access",
      "foundation models",
      "Bedrock actions",
      "AWS Marketplace actions",
      "PutFoundationModelEntitlement",
      "Subscribe"
    ]
  },
  {
    "id": 41,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Allow an Identity to Request Access to a Specific Model",
    "content": "To allow an IAM entity to request access to a specific foundation model, it must have at least the bedrock:PutFoundationModelEntitlement permission, with its Resource field set to `*`. If the model has a product ID, the `aws-marketplace:Subscribe` permission is also required, and you can use the `aws-marketplace:ProductId` condition key to scope the subscription to that specific model. Once an identity has requested access to a model in one AWS Region, only the `bedrock:PutFoundationModelEntitlement` action is necessary for requesting access in other Regions. For models without a product ID, such as Amazon Nova Micro, or for models already subscribed to in one Region, a policy with only `bedrock:PutFoundationModelEntitlement` (Resource: `*`) is sufficient.",
    "tags": [
      "IAM policy",
      "grant access",
      "specific model",
      "product ID",
      "PutFoundationModelEntitlement",
      "Subscribe"
    ]
  },
  {
    "id": 42,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Prevent an Identity from Requesting Access to a Model with a Product ID",
    "content": "To prevent an IAM entity from requesting access to a specific foundation model that has a product ID, an IAM policy must be attached to the user that denies the aws-marketplace:Subscribe action. This denial should be scoped using the Condition field to the specific product ID of the model. It's important to note that such a policy will grant the IAM entity access to any newly added models by default. Furthermore, if the identity has already subscribed to the model in at least one AWS Region, this policy will not prevent access in other Regions where the model is available.",
    "tags": [
      "IAM policy",
      "deny access",
      "product ID",
      "AWS Marketplace",
      "Subscribe"
    ]
  },
  {
    "id": 43,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Prevent an Identity from Requesting Access to Models in Specific Regions",
    "content": "If an IAM identity has already been granted access to a model in one AWS Region, you can control access to model subscription in other Regions. This is achieved by including the bedrock:PutFoundationModelEntitlement action in an IAM policy statement and using the global aws:RequestedRegion condition key. For example, a policy can be configured to allow access only to US Regions. However, it's important to understand that `bedrock:PutFoundationModelEntitlement` does not scope to specific models. Therefore, this method cannot control an identity's access to individual models if they lack a product ID or if the identity already has access to the model in another Region. In such cases, controlling the usage of the model by denying specific Amazon Bedrock actions is recommended instead.",
    "tags": [
      "IAM policy",
      "Regional access",
      "deny access",
      "AWS Region",
      "PutFoundationModelEntitlement",
      "RequestedRegion"
    ]
  },
  {
    "id": 44,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Prevent an Identity from Using a Model After Access Has Already Been Granted",
    "content": "Once an IAM identity has been granted access to a foundation model, you can prevent its usage by applying an IAM policy that denies all Amazon Bedrock actions (`bedrock:*`) and scopes the Resource field to the specific Amazon Resource Name (ARN) of the foundation model. This ensures that even if access was previously granted, the identity is explicitly prevented from invoking or interacting with that particular model.",
    "tags": [
      "IAM policy",
      "deny usage",
      "model ARN",
      "access control"
    ]
  },
  {
    "id": 45,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Use Product ID Condition Keys to Control Access",
    "content": "The aws-marketplace:ProductId condition key is instrumental in controlling the ability to subscribe to Amazon Bedrock serverless models that are available through AWS Marketplace and have an associated product ID. However, it's important to note that models from providers such as Amazon, DeepSeek, Mistral AI, and Meta are not sold through AWS Marketplace and therefore do not have product keys. For these models, you cannot use `aws-marketplace` actions with the `ProductId` condition key for access control. Instead, preventing their usage would involve denying general Amazon Bedrock actions and specifying the model IDs in the Resource field of your IAM policies.",
    "tags": [
      "product ID",
      "condition key",
      "AWS Marketplace",
      "model access control",
      "IAM",
      "deny access"
    ]
  },
  {
    "id": 46,
    "service": "Amazon Bedrock",
    "category": "Foundation Models",
    "title": "Add or Remove Access to Foundation Models",
    "content": "You must request access to a foundation model before you can use it, and you can remove access if it's no longer needed. This process is primarily managed through the Amazon Bedrock console. Models from Amazon, DeepSeek, Mistral AI, and Meta do not use AWS Marketplace product keys, so their usage must be controlled by denying Amazon Bedrock actions with their specific model IDs in IAM policies. Access to a model, once provided, is available to all users in the AWS account. \n\nTo add or remove access via the console:\n*   Ensure you have the necessary permissions.\n*   Sign in to the Amazon Bedrock console and navigate to Model access under Bedrock configurations.\n*   Choose Modify model access.\n*   Select or unselect the models to manage access for, reviewing the End User License Agreement (EULA) as necessary.\n*   For Anthropic models, you must submit use case details.\n*   Review the access changes and terms, then Submit.\n\nChanges may take several minutes to reflect. If access to a model is revoked, it might still be accessible via the API for a short period while changes propagate; for immediate removal, an IAM policy denying access to the model should be added to the relevant role. For AWS GovCloud (US) customers, model access must first be enabled via a standard AWS account ID linked to their GovCloud account, and then specific sign-up steps must be followed within the AWS GovCloud (US) account in `us-gov-west-1` for regional entitlement.",
    "tags": [
      "model access",
      "console",
      "EULA",
      "permissions",
      "revoke access",
      "GovCloud"
    ]
  },
  {
    "id": 47,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Foundation model (FM)",
    "content": "A Foundation model (FM) is an AI model characterized by a large number of parameters and training on a massive amount of diverse data. These models are capable of generating a variety of responses for a wide range of use cases, including generating text, images, or converting input into embeddings. To use an Amazon Bedrock foundation model, you must first request access to it.",
    "tags": [
      "Foundation Model",
      "FM",
      "AI model",
      "generative AI"
    ]
  },
  {
    "id": 48,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Get model information",
    "content": "You can find overarching information about Amazon Bedrock foundation model providers and the models they provide in the Providers and Base models sections of the Amazon Bedrock console. For programmatic access, the API allows you to retrieve information about Amazon Bedrock foundation models, including their ARN, model ID, supported modalities and features, and whether they are deprecated. This information is returned in a FoundationModelSummary object.\n\nTo get information using the API:\n*   Send a ListFoundationModels request to return information about all available foundation models in Amazon Bedrock. Note that this response may also include deprecated model IDs or those for backward compatibility not listed in standard charts.\n*   Send a GetFoundationModel request, specifying the model ID, to return information about a specific foundation model.",
    "tags": [
      "model information",
      "API",
      "ListFoundationModels",
      "GetFoundationModel",
      "console"
    ]
  },
  {
    "id": 49,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Supported foundation models",
    "content": "Amazon Bedrock supports foundation models from multiple providers, offering a wide range of industry-leading FMs. These models are packaged by providers and are ready to use. Examples of model providers include Amazon Nova, Amazon Titan, Anthropic Claude, AI21 Labs, Cohere, DeepSeek, Luma AI, Meta Llama, Mistral AI, OpenAI, Stability AI, TwelveLabs, and Writer AI Palmyra. Each model has an associated Model ID for on-demand API calls, specific AWS Regions supported, various input and output modalities, streaming support, and links to relevant documentation. You must request access to a model before you can use it.",
    "tags": [
      "supported models",
      "foundation model providers",
      "model ID",
      "Amazon Nova",
      "Amazon Titan",
      "Anthropic Claude",
      "AI21 Labs",
      "Cohere",
      "DeepSeek",
      "Luma AI",
      "Meta Llama",
      "Mistral AI",
      "OpenAI",
      "Stability AI",
      "TwelveLabs",
      "Writer AI Palmyra"
    ]
  },
  {
    "id": 50,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Model support by Region",
    "content": "Amazon Bedrock foundation models differ in their Regional support. While Amazon Bedrock itself is available in various AWS Regions, individual models may only be available in a subset of these regions. For example, Amazon Titan Text G1 - Express is available in US East (N. Virginia), US West (Oregon), and several other regions, but not in US East (Ohio). Some Meta Llama models like Llama 3.2 11B Instruct are also available across a number of regions, often with specific regional availability indicated by an asterisk. The sources provide tables detailing the Regional support for each model.",
    "tags": [
      "regional support",
      "AWS Region",
      "model availability"
    ]
  },
  {
    "id": 51,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Feature support by Region",
    "content": "Amazon Bedrock features have varying Regional support. Model inference (InvokeModel, InvokeModelWithResponseStream, Converse, ConverseStream) is available in all Regions supported by Amazon Bedrock. Other features, however, have specific regional limitations:\n*   Model evaluation in Europe (Paris) is only available for automatic evaluation jobs.\n*   Provisioned Throughput in AWS GovCloud (US-West) is only available for custom models with no-commitment.\n\nThe sources contain a table outlining feature support by Region for a comprehensive overview.",
    "tags": [
      "feature support",
      "regional availability",
      "model inference",
      "model evaluation",
      "Provisioned Throughput"
    ]
  },
  {
    "id": 52,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Model support by feature",
    "content": "Foundation models in Amazon Bedrock differ in the features they support. Certain features, such as Amazon Bedrock API keys, Amazon Bedrock Agents, and Application inference profiles, support all models in Amazon Bedrock.\n\nSupport for other features depends on model compatibility:\n*   Prompt management supports all models that support the Converse API.\n*   Amazon Bedrock Flows support depends on model support for the node types used in the flow.\n\nFor example, while Amazon Titan Text G1 - Express supports Batch inference and Guardrails, Amazon Nova Reel does not. Anthropic Claude 3 Haiku supports Batch inference, Fine-tuning, Guardrails, and Knowledge bases. The sources provide detailed tables illustrating feature support by model across various capabilities like Batch inference, Fine-tuning, Guardrails, Knowledge bases, and more.",
    "tags": [
      "feature compatibility",
      "model features",
      "API keys",
      "Agents",
      "Batch inference",
      "Fine-tuning",
      "Guardrails",
      "Knowledge Bases"
    ]
  },
  {
    "id": 53,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Model inference parameters and responses",
    "content": "When making inference calls to Amazon Bedrock models using operations like InvokeModel, InvokeModelWithResponseStream, Converse, and ConverseStream, you must include request parameters specific to the model you are using. These parameters control how the model generates its output. If you have a custom model, it will use the same inference parameters as its original foundation model. If importing a customized model, ensure its inference parameters match those specified in the documentation, otherwise, they will be ignored.\n\nCommon inference parameters include:\n*   Temperature: A float value (0 to 1) that regulates the creativity of responses; lower values lead to more deterministic outputs, while higher values result in more creative or varied responses.\n*   Top P: Controls the diversity of the output.\n*   Max Token Count: Specifies the maximum number of tokens to generate in the response.\n*   Stop Sequences: Character sequences that stop the model from generating further tokens.\n\nSome models also have model-specific inference parameters. For example, Anthropic Claude models have an additional Top-k parameter, and AI21 Labs Jurassic models include parameters like presence penalty, count penalty, frequency penalty, and special token penalty. The structure and fields of model responses also vary by model. For detailed parameter and response information, refer to the specific model's documentation within Amazon Bedrock.",
    "tags": [
      "model inference",
      "inference parameters",
      "response fields",
      "InvokeModel",
      "Converse",
      "temperature",
      "max tokens",
      "stop sequences"
    ]
  },
  {
    "id": 54,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Amazon Nova models",
    "content": "Amazon Bedrock supports several Amazon Nova models, including Amazon Nova Canvas, Amazon Nova Lite, Amazon Nova Micro, Amazon Nova Premier, Amazon Nova Pro, Amazon Nova Reel, and Amazon Nova Sonic. These models offer various input and output modalities, such as Text, Image, and Video input, with outputs like Text, Image, Speech, and Video. Nova models also have specific prompt engineering guides and their default inference parameters are detailed in the Amazon Nova User Guide. For example, Amazon Nova Lite supports text, image, and video input, and text output, with streaming capabilities. Amazon Nova Micro supports text input and text output, also with streaming. Amazon Nova Reel supports text and image input for video output.",
    "tags": [
      "Amazon Nova",
      "Nova Canvas",
      "Nova Lite",
      "Nova Micro",
      "Nova Premier",
      "Nova Pro",
      "Nova Reel",
      "Nova Sonic",
      "multimodal",
      "text to video"
    ]
  },
  {
    "id": 55,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Amazon Titan models",
    "content": "Amazon Bedrock provides access to various Amazon Titan models, including Amazon Titan Text, Amazon Titan Image Generator, Amazon Titan Text Embeddings, and Amazon Titan Multimodal Embeddings. These models support different input and output modalities. For instance, Amazon Titan Text models (Lite, Express, Premier) primarily handle text input and generate text or chat output. They support inference parameters like `maxTokenCount`, `temperature`, `topP`, and `stopSequences`. Amazon Titan Image Generator models take text and image input to produce image output. Amazon Titan Embeddings models (Text Embeddings G1, Text Embeddings V2, Multimodal Embeddings G1) are designed to convert text or images into numerical vector embeddings for tasks like similarity comparison. Embeddings models typically do not support inference parameters beyond input specification.",
    "tags": [
      "Amazon Titan",
      "Titan Text",
      "Titan Image Generator",
      "Titan Embeddings",
      "multimodal",
      "image generation",
      "text generation",
      "embeddings"
    ]
  },
  {
    "id": 56,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Anthropic Claude models",
    "content": "Amazon Bedrock offers several Anthropic Claude models, including Claude 3 Haiku, Claude 3 Opus, Claude 3.5 Sonnet, and Claude 3.7 Sonnet. These models support Text and Image input, generating Text and Chat output, with streaming supported. You can interact with Claude models using either the Text Completions API or the Messages API. The Messages API is recommended for conversational applications, allowing for multi-turn interactions, tool use, and the handling of various content types including text and images. Claude models also support the use of XML tags to structure prompts for optimal results. Inference parameters include `max_tokens`, `temperature`, `top_p`, and model-specific `top_k`.",
    "tags": [
      "Anthropic Claude",
      "Claude 3 Haiku",
      "Claude 3 Opus",
      "Claude 3.5 Sonnet",
      "Claude 3.7 Sonnet",
      "Text Completions API",
      "Messages API",
      "conversational AI",
      "tool use"
    ]
  },
  {
    "id": 57,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "AI21 Labs models",
    "content": "Amazon Bedrock integrates AI21 Labs models, such as Jamba 1.5 Large, Jamba 1.5 Mini, and Jamba-Instruct. These models primarily support Text input and generate Text and Chat output, with streaming capabilities. They can be invoked using the InvokeModel and InvokeModelWithResponseStream operations. AI21 Labs models offer specific inference parameters beyond the common ones, including presence penalty, count penalty, and frequency penalty. These penalties can be applied to various tokens like whitespaces, punctuations, and numbers to control generation. A dedicated AI21 Labs prompt engineering guide is available for crafting effective prompts.",
    "tags": [
      "AI21 Labs",
      "Jamba",
      "Jurassic-2",
      "text generation",
      "inference parameters"
    ]
  },
  {
    "id": 58,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Cohere models",
    "content": "Amazon Bedrock supports Cohere models, including Cohere Command, Cohere Command R, Cohere Command R+, Cohere Embed (English), Cohere Embed (Multilingual), and Cohere Rerank. These models generally handle Text input and can produce Text, Chat, or Embedding output. Streaming is supported for Command models but not for Embed models. Cohere Command R and R+ models are recommended for conversational applications and support tool use. Inference parameters for Command models include `temperature`, `p`, `k`, `max_tokens`, and `stop_sequences`. Embed models have parameters like `input_type`, `texts`, `images`, and `embedding_types` to generate different forms of embeddings (float, int8, uint8).",
    "tags": [
      "Cohere",
      "Command",
      "Embed",
      "Rerank",
      "text generation",
      "chat",
      "embeddings",
      "tool use"
    ]
  },
  {
    "id": 59,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "DeepSeek models",
    "content": "DeepSeek offers the DeepSeek-R1 text-to-text model on Amazon Bedrock, available for inferencing via the Invoke API (InvokeModel, InvokeModelWithResponseStream) and the Converse API (Converse, ConverseStream). This model supports text input and text output. DeepSeek models require a prompt for inference. When using the InvokeModel operation, you'll need the inference profile ID as the model ID, for example, `us.deepseek.r1-v1:0` for the US region. Inference parameters include `prompt`, `temperature`, `top_p`, `max_tokens`, and `stop` sequences. DeepSeek-R1 always has reasoning enabled, and its maximum token count for both output and reasoning is 8192. You cannot remove access to DeepSeek models, but you can prevent their usage with an IAM policy.",
    "tags": [
      "DeepSeek",
      "DeepSeek-R1",
      "text-to-text",
      "InvokeModel",
      "Converse API",
      "reasoning"
    ]
  },
  {
    "id": 60,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Luma AI models",
    "content": "Amazon Bedrock supports Luma AI models, specifically Luma Ray v2, which is a Text to Video model. This model processes prompts asynchronously using the StartAsyncInvoke, GetAsyncInvoke, and ListAsyncInvokes APIs. Users provide a text prompt to the model, and the output video (MP4 file) is stored in a specified Amazon S3 bucket. Luma Ray v2 supports various customization options including `aspect_ratio`, `loop`, `duration` (5s, 9s), and `resolution` (540p, 720p). It also supports image-to-video generation using `keyframes` for start and end frames.",
    "tags": [
      "Luma AI",
      "Luma Ray v2",
      "text to video",
      "async invoke",
      "video generation",
      "image to video"
    ]
  },
  {
    "id": 61,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Meta Llama models",
    "content": "Amazon Bedrock provides access to several Meta Llama models, including Llama 3 Instruct, Llama 3.1 Instruct, Llama 3.2 Instruct, Llama 3.3 Instruct, and Llama 4 Instruct. These models generally support Text input and generate Text and Chat output, with streaming supported for most models except Llama 4 Instruct. Llama 3.2 and later models also support image input. Specific Llama models like Llama 3.2 Instruct and Llama 3.3 Instruct utilize geofencing, meaning they can only be used within their supported AWS Regions. Common inference parameters include `prompt`, `temperature`, `top_p`, and `max_gen_len`. A dedicated Meta Llama prompt engineering guide is available.",
    "tags": [
      "Meta Llama",
      "Llama 3",
      "Llama 3.1",
      "Llama 3.2",
      "Llama 3.3",
      "Llama 4",
      "text generation",
      "chat",
      "multimodal",
      "geofencing"
    ]
  },
  {
    "id": 62,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Mistral AI models",
    "content": "Amazon Bedrock offers several Mistral AI models, including Mistral 7B Instruct, Mixtral 8x7B Instruct, Mistral Large, Mistral Small, and Pixtral Large. These models support Text input and generate Text output, with streaming generally supported. Pixtral Large (25.02) is a multimodal model combining image understanding with text processing, supporting both text and image input for text output. Mistral AI models are available under the Apache 2.0 license. They support both text completion and chat completion APIs. The chat completion API is recommended with the Converse API for conversational applications and tool use. Inference parameters include `prompt`, `max_tokens`, `stop`, `temperature`, `top_p`, and `top_k`.",
    "tags": [
      "Mistral AI",
      "Mistral 7B Instruct",
      "Mixtral 8x7B Instruct",
      "Mistral Large",
      "Mistral Small",
      "Pixtral Large",
      "text generation",
      "chat completion",
      "multimodal"
    ]
  },
  {
    "id": 63,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "OpenAI models",
    "content": "Amazon Bedrock supports OpenAI's open-weight models, specifically gpt-oss-20b (optimized for lower latency) and gpt-oss-120b (optimized for production and high-reasoning use cases). Both models have a context window of 128,000 tokens. They support text input and text output. You can interact with these models through: \n*   InvokeModel.\n*   Converse API.\n*   The OpenAI Chat Completions API.\n*   Batch inference using CreateModelInvocationJob.\n*   Guardrails application through headers in model invocation operations.\n\nFor OpenAI Chat Completions API, authentication can only be done with an Amazon Bedrock API key. Access to these OpenAI models is not granted by default, but once requested, they can be directly selected from the model catalog in the console.",
    "tags": [
      "OpenAI",
      "gpt-oss-20b",
      "gpt-oss-120b",
      "text generation",
      "chat completions",
      "batch inference",
      "guardrails"
    ]
  },
  {
    "id": 64,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Stability AI models",
    "content": "Amazon Bedrock features Stability AI Diffusion models, including Stable Diffusion 3.5 Large, Stable Image Core 1.0, and Stable Image Ultra 1.0. These models are designed for text-to-image and image-to-image inference, supporting Text and Image input to generate Image output. They can be invoked using the InvokeModel operation. Inference parameters include `text_prompts` (with optional `weight`), `height`, `width`, `cfg_scale`, `seed`, `steps`, and `style_preset`. Negative prompts can be used to guide the model away from certain concepts. Stable Image Core and Ultra models are also supported with image filters in Amazon Bedrock Guardrails.",
    "tags": [
      "Stability AI",
      "Stable Diffusion",
      "Stable Image Core",
      "Stable Image Ultra",
      "image generation",
      "text to image",
      "image to image",
      "inference parameters"
    ]
  },
  {
    "id": 65,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "TwelveLabs models",
    "content": "Amazon Bedrock supports TwelveLabs multimodal AI models specialized in video understanding and analysis, including TwelveLabs Pegasus 1.2 and TwelveLabs Marengo Embed 2.7.\n*   TwelveLabs Pegasus 1.2 (`twelvelabs.pegasus-1-2-v1:0`) is a multimodal model for comprehensive video understanding and analysis, taking Video input to generate Text output. It supports InvokeModel and InvokeModelWithResponseStream operations. Max video size is 1 hour long (under 2GB). Parameters include `inputPrompt`, `temperature`, `responseFormat`, and `mediaSource`.\n*   TwelveLabs Marengo Embed 2.7 (`twelvelabs.marengo-embed-2-7-v1:0`) generates high-quality embeddings from Video, Text, Audio, or Image inputs. It uses asynchronous inference via the StartAsyncInvoke API. Max video size is 2 hours long (under 2GB). Parameters include `inputType`, `textTruncate`, and `embeddingOption`.",
    "tags": [
      "TwelveLabs",
      "Pegasus 1.2",
      "Marengo Embed 2.7",
      "video understanding",
      "multimodal",
      "embeddings",
      "text",
      "audio",
      "image"
    ]
  },
  {
    "id": 66,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Writer AI Palmyra models",
    "content": "Amazon Bedrock offers Writer AI Palmyra models, including Palmyra X4 and Palmyra X5. These models are text-to-text, supporting Text input and generating Text output. They are available as serverless deployments and support multiple languages including English, Spanish, French, German, and Chinese. Palmyra X4 and X5 models can be invoked using the InvokeModel and InvokeModelWithResponseStream operations. Inference parameters include `temperature`, `max_tokens`, `top_p`, `presence_penalty`, and `frequency_penalty`. For example, Palmyra X4 supports a maximum of 4096 tokens, while Palmyra X5 supports 1,040,000 input tokens and 8192 output tokens.",
    "tags": [
      "Writer AI",
      "Palmyra X4",
      "Palmyra X5",
      "text generation",
      "multilingual",
      "inference parameters"
    ]
  },
  {
    "id": 67,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Customization",
    "title": "Custom model hyperparameters",
    "content": "Hyperparameters are values that can be adjusted for model customization to control the training process and, consequently, the output custom model. Different Amazon Bedrock models support specific hyperparameters for customization techniques like fine-tuning or continued pre-training.\n\nExamples of model-specific hyperparameters include:\n*   Amazon Nova Canvas model: Supports `batchSize` (8-192, default 8), `stepCount` (10-20,000, default 500), and `learningRate` (1.00E-7 to 1.00E-4, default 1.00E-5).\n*   Amazon Titan Text Premier model: Supports `epochs` (1-100) and `batchSize` (256-9,216, default 576).\n*   Amazon Titan Multimodal Embeddings G1: Supports `learningRate` (5.00E-8 to 1, default 5.00E-5).\n*   Anthropic Claude 3 models: Support `learningRate` (5.00E-6 to 0.1, default 1.00E-4).\n\nAdjusting the number of epochs, for instance, directly impacts model customization cost as each epoch processes the entire training dataset once.",
    "tags": [
      "custom models",
      "hyperparameters",
      "model customization",
      "fine-tuning",
      "training process",
      "epochs",
      "batch size",
      "learning rate"
    ]
  },
  {
    "id": 68,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Amazon Nova Understanding models",
    "content": "Amazon Nova includes various models designed for understanding and generation tasks. Specifically, the Amazon Nova Micro, Lite, and Pro models are part of the 'understanding' category, focusing on prompt understanding for different modalities. They support capabilities like prompt caching. Detailed prompting best practices for Amazon Nova understanding models are available in the dedicated prompt guide.",
    "tags": [
      "Amazon Nova",
      "Nova understanding",
      "Nova Micro",
      "Nova Lite",
      "Nova Pro",
      "prompt guide"
    ]
  },
  {
    "id": 69,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Amazon Nova Canvas model",
    "content": "The Amazon Nova Canvas model (`amazon.nova-canvas-v1:0`) is available in regions like us-east-1 and ap-northeast-1. It takes Text and Image input and generates Image output. It does not support streaming. Specific guidance for generating images with Amazon Nova Canvas is provided in its prompt guide. For customization, it supports hyperparameters such as `batchSize`, `stepCount`, and `learningRate`.",
    "tags": [
      "Amazon Nova Canvas",
      "image generation",
      "multimodal",
      "customization hyperparameters"
    ]
  },
  {
    "id": 70,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Amazon Titan text models",
    "content": "The Amazon Titan Text models on Amazon Bedrock include Titan Text G1 - Express, Titan Text G1 - Lite, and Titan Text G1 - Premier. These models are designed for text generation and chat applications, accepting Text input and producing Text or Chat output. They all support streaming. Specific prompt engineering guidelines for Titan Text models are available to optimize performance. For customization, the Amazon Titan Text Premier model supports `epochs` and `batchSize` hyperparameters.",
    "tags": [
      "Amazon Titan Text",
      "Titan Text G1 - Express",
      "Titan Text G1 - Lite",
      "Titan Text G1 - Premier",
      "text generation",
      "chat"
    ]
  },
  {
    "id": 71,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Amazon Titan Image Generator G1 models",
    "content": "The Amazon Titan Image Generator G1 models (V1 and V2) are available on Amazon Bedrock, with model IDs like `amazon.titan-image-generator-v1` and `amazon.titan-image-generator-v2:0`. These models enable image generation from text or image input, producing image output. They do not support streaming. Inference parameters for these models include `taskType` (e.g., TEXT_IMAGE), `textToImageParams` (with `text`, `negativeText`, `conditionImage`), and `imageGenerationConfig` (with `quality`, `numberOfImages`, `height`, `width`, `cfgScale`, `seed`). These models can also be used with Amazon Bedrock Guardrails image filters to block inappropriate or harmful content.",
    "tags": [
      "Amazon Titan Image Generator",
      "Titan Image Generator G1",
      "image generation",
      "text to image",
      "image to image",
      "guardrails"
    ]
  },
  {
    "id": 72,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Amazon Titan Multimodal Embeddings G1",
    "content": "The Amazon Titan Multimodal Embeddings G1 model (`amazon.titan-embed-image-v1`) supports generating embeddings from text and/or image inputs. It can produce a single embeddings vector by averaging text and image embeddings vectors if both inputs are provided. This model generates Embedding output and does not support streaming. The request body includes `inputText`, `inputImage` (base64-encoded string), and `embeddingConfig` to specify the `outputEmbeddingLength` (256, 384, or 1024). The response includes the `embedding` vector, `inputTextTokenCount`, and a `message` field for errors. For customization, it supports the `learningRate` hyperparameter.",
    "tags": [
      "Amazon Titan Multimodal Embeddings",
      "multimodal embeddings",
      "text embeddings",
      "image embeddings",
      "vector embeddings"
    ]
  },
  {
    "id": 73,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Anthropic Claude 3 models",
    "content": "Amazon Bedrock features several Anthropic Claude 3 models, including Claude 3 Haiku, Claude 3 Opus, Claude 3.5 Sonnet, and Claude 3.7 Sonnet. These models support Text and Image input for Text and Chat output, with streaming capabilities. They are compatible with the Converse API and support tool use. For customization, Anthropic Claude 3 models support the `learningRate` hyperparameter. Claude 3.7 Sonnet and Claude 4 models handle thinking blocks differently, with Claude 3.7 returning full thinking output and Claude 4 returning summarized thinking. Prompt caching is also generally available for Claude 3.7 Sonnet and Claude 3.5 Haiku.",
    "tags": [
      "Anthropic Claude 3",
      "Claude 3 Haiku",
      "Claude 3 Opus",
      "Claude 3.5 Sonnet",
      "Claude 3.7 Sonnet",
      "multimodal chat",
      "tool use",
      "customization hyperparameters",
      "prompt caching"
    ]
  },
  {
    "id": 74,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Cohere Command models",
    "content": "The Cohere Command models available on Amazon Bedrock include Cohere Command, Cohere Command R, and Cohere Command R+. These models are designed for text generation and conversational AI, accepting Text input and generating Text or Chat output. They support streaming. For conversational applications, using the Converse API with Command R and Command R+ models is recommended, as it provides a unified set of parameters and supports tool use. Inference parameters include `prompt`, `temperature`, `p`, `k`, `max_tokens`, `stop_sequences`, `chat_history`, `documents`, and `tools`. Note that Cohere Command (Text) does not support chat with the Converse API, only single user messages.",
    "tags": [
      "Cohere Command",
      "Command R",
      "Command R+",
      "text generation",
      "conversational AI",
      "tool use",
      "Converse API"
    ]
  },
  {
    "id": 75,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Meta Llama 3.1 models",
    "content": "Amazon Bedrock supports Meta Llama 3.1 Instruct models, including 8B, 70B, and 405B parameter versions. These models accept Text input and generate Text and Chat output, with streaming support for 8B and 70B models, but not for the 405B version. They utilize inference parameters such as `prompt`, `temperature`, `top_p`, and `max_gen_len`. Llama 3.1 models, similar to other Llama models, are subject to geofencing, restricting their use to specific AWS Regions.",
    "tags": [
      "Meta Llama 3.1",
      "Llama 3.1 Instruct",
      "text generation",
      "chat"
    ]
  },
  {
    "id": 76,
    "service": "Amazon Bedrock",
    "category": "Foundation Model Information",
    "title": "Meta Llama 3.2 models",
    "content": "Amazon Bedrock offers Meta Llama 3.2 Instruct models, including 1B, 3B, 11B, and 90B parameter versions. These models support both Text and Image input, generating Text and Chat output, with streaming capabilities. They are subject to geofencing, meaning they can only be used within their supported AWS Regions. Llama 3.2 models add image support to the request structure. The EU Meta Llama 3.2 1B Instruct inference profile is available, allowing requests to be routed across various European destination regions from specific source regions.",
    "tags": [
      "Meta Llama 3.2",
      "Llama 3.2 Instruct",
      "multimodal chat",
      "geofencing"
    ]
  },
  {
    "id": 77,
    "service": "Amazon Bedrock",
    "category": "Model Lifecycle",
    "title": "Model lifecycle",
    "content": "Amazon Bedrock continuously updates its foundation models with newer versions offering improved capabilities, accuracy, and safety. Models offered on Amazon Bedrock can be in one of three states: Active, Legacy, or End-of-Life (EOL). The status of models is visible in the console and tables, and also returned in the `modelLifecycle` field when making `GetFoundationModel` or `ListFoundationModels` API calls. Users are encouraged to test and migrate applications to the latest model versions.",
    "tags": [
      "model lifecycle",
      "Active",
      "Legacy",
      "End-of-Life",
      "EOL",
      "model versions"
    ]
  },
  {
    "id": 78,
    "service": "Amazon Bedrock",
    "category": "Model Lifecycle",
    "title": "Active versions",
    "content": "Active versions are the current generation of models available and fully supported on Amazon Bedrock. These models offer the latest capabilities and receive ongoing updates. Examples of active models include AI21 Labs Jamba 1.5 Large, Amazon Nova Canvas, Amazon Titan Image Generator G1 v2, Anthropic Claude 3 Haiku, Cohere Embed English, Meta Llama 3 8B Instruct, Mistral AI Mistral 7B Instruct, Stability AI Stable Image Core 1.1, and TwelveLabs Pegasus 1.2. The sources provide tables with details such as model ID, supported regions, launch date, and input/output modalities for each active model.",
    "tags": [
      "active models",
      "current versions",
      "model support"
    ]
  },
  {
    "id": 79,
    "service": "Amazon Bedrock",
    "category": "Model Lifecycle",
    "title": "Legacy versions",
    "content": "Legacy versions are older models that are still available on Amazon Bedrock but may have a target deprecation date. Users are generally advised to migrate to newer, recommended versions for better performance and continued support. For example, AI21 J2 Mid-v1, Ai21 J2 Ultra-v1, and Ai21 J2-Grande-Instruct became legacy on October 4, 2024 (in us-east-1) with a recommended replacement of Jamba-Instruct v1. Provisioned capacity for fine-tuned models based on legacy base models will be supported for 12 months after the EOL date of the base model, provided capacity was provisioned before EOL.",
    "tags": [
      "legacy models",
      "deprecated models",
      "model migration"
    ]
  },
  {
    "id": 80,
    "service": "Amazon Bedrock",
    "category": "Model Lifecycle",
    "title": "EOL versions",
    "content": "End-of-Life (EOL) versions are models that have passed their End-of-Life date and are no longer actively supported by Amazon Bedrock. It is not possible to fine-tune a base model after its EOL date. Examples of models that have reached EOL include Stable Diffusion XL 0.8 (EOL April 30, 2024, replaced by Stable Diffusion XL 1.x), Cohere Command and Cohere Command Light (EOL August 12, 2025, replaced by Cohere Command R and R+), Claude v1.3 (EOL February 28, 2024, replaced by Claude v2.1), Titan Embeddings - Text v1.1 (EOL February 15, 2024, replaced by Titan Embeddings - Text v1.2), and Meta Llama 2 13b-chat-v1 (EOL October 30, 2024, replaced by Meta Llama3 and Llama3.1).",
    "tags": [
      "EOL models",
      "end-of-life",
      "unsupported models",
      "model deprecation"
    ]
  },
  {
    "id": 81,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Amazon Bedrock Marketplace Overview",
    "content": "Amazon Bedrock Marketplace is a fully managed service that allows users to discover, test, and use over 100 popular, emerging, and specialized foundation models (FMs). These models complement the selection of industry-leading models already available in Amazon Bedrock. Users can find models in a unified catalog, subscribe to them, and then deploy them to an endpoint managed by SageMaker AI. Accessing these models through Amazon Bedrock’s APIs enables their native integration with Bedrock tools such as Agents, Knowledge Bases, and Guardrails. Models can be accessed using the InvokeModel and Converse operations, as well as through the Amazon Bedrock console.",
    "tags": [
      "Marketplace",
      "Foundation Models",
      "Model Discovery",
      "AI Models"
    ]
  },
  {
    "id": 82,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Set up Amazon Bedrock Marketplace",
    "content": "To set up Amazon Bedrock Marketplace, users typically require specific permissions. The Amazon Bedrock Full Access policy can provide the necessary permissions for SageMaker AI. If a custom policy is used instead of the managed policy, the IAM role must explicitly include permissions for various actions, including `bedrock:*`, `kms:DescribeKey`, `iam:ListRoles`, `ec2:DescribeVpcs`, `ec2:DescribeSubnets`, `ec2:DescribeSecurityGroups`, `sagemaker:*` for SageMaker endpoint operations (conditioned by `aws:CalledViaLast: bedrock.amazonaws.com`), `sagemaker:DescribeHubContent`, `sagemaker:ListHubContents`, `license-manager:ListReceivedLicenses`, and `iam:PassRole` to both SageMaker and Bedrock services. For accessing the Marketplace models through the AWS Management Console, additional S3 permissions are required, such as `s3:GetObject` and `s3:ListBucket`. If an AWS Key Management Service (KMS) key is used for endpoint encryption, the IAM policy and the KMS key policy must grant the necessary encryption permissions.",
    "tags": [
      "Setup",
      "Permissions",
      "IAM",
      "SageMaker",
      "KMS"
    ]
  },
  {
    "id": 83,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Controlling Access to Amazon Bedrock Marketplace Models",
    "content": "Access to Amazon Bedrock Marketplace models can be controlled using IAM policies to either deny or selectively allow access to specific models. To deny access to particular models, a `Deny` policy can be applied, targeting `sagemaker:*` and `bedrock:*` actions on SageMaker resources (endpoints, endpoint configurations, and models), with a `StringLike` condition on `aws:ResourceTag/sagemaker-studio:hub-content-arn` that matches the identifier of the model to be denied. This explicitly blocks the specified model. Conversely, to allow access to only specific models, an `Allow` policy is used. This policy should include specific SageMaker actions like `CreateEndpoint`, `UpdateEndpoint`, and `AddTags` on relevant SageMaker resources. Critical conditions for such a policy include `StringEquals` for `aws:CalledViaLast: bedrock.amazonaws.com` and `aws:ResourceTag/sagemaker-sdk:bedrock: compatible`, along with a `StringLike` condition on `aws:ResourceTag/sagemaker-studio:hub-content-arn` matching the model ID to be allowed. This approach ensures that only designated models are accessible, denying all others.",
    "tags": [
      "Access Control",
      "IAM Policies",
      "Security",
      "Model Permissions"
    ]
  },
  {
    "id": 84,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "End-to-end Workflow",
    "content": "An end-to-end workflow in Amazon Bedrock Marketplace allows users to seamlessly manage and interact with models from discovery to invocation. This workflow typically involves several stages, often demonstrated with Python code examples utilizing AWS SDKs (Boto3) for SageMaker, Bedrock, and Bedrock Runtime. Key capabilities within this workflow include:\n*   Model discovery and description: Listing all available models and retrieving detailed information about a specific model.\n*   Model deployment: Creating, describing, updating, and deleting endpoints for Marketplace models, including the necessary EULA acceptance for gated models and waiting for endpoint status changes.\n*   Model invocation: Sending prompts and generating responses using the `InvokeModel`, `InvokeModelWithResponseStream`, `Converse`, and `ConverseStream` operations with the deployed endpoints.\n*   Endpoint management: Listing all deployed endpoints, listing endpoints associated with a particular model, and performing operations such as deregistering and re-registering endpoints.\nThis comprehensive workflow supports using deployed endpoints with various Bedrock developer tools like Guardrails, Model evaluation, Agents, Knowledge Bases, and Prompt management.",
    "tags": [
      "Workflow",
      "API Integration",
      "Deployment",
      "Invocation",
      "Endpoint Management"
    ]
  },
  {
    "id": 85,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Discover a Model",
    "content": "Models in Amazon Bedrock Marketplace can be discovered through the model catalog in the AWS Management Console. To do this, users sign in with appropriate IAM permissions, navigate to the Amazon Bedrock service, and then select 'Model Catalog' from the navigation pane. Within the Model Catalog, users can filter by 'Model Collection = Bedrock Marketplace' to specifically view models offered through the marketplace, or they can use the search bar to find models by name. It is important to note that proprietary models found in the Amazon Bedrock Marketplace require a subscription before they can be accessed.",
    "tags": [
      "Model Discovery",
      "Model Catalog",
      "Console",
      "Marketplace Models"
    ]
  },
  {
    "id": 86,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Subscribe to a Model",
    "content": "To utilize a model from Amazon Bedrock Marketplace, users must subscribe to it. This step is mandatory for proprietary models, while publicly available models (e.g., HuggingFace models) do not require a subscription. The subscription process involves reviewing and accepting the model provider's prices and End User License Agreements (EULAs). To subscribe via the AWS Management Console, users navigate to the Model Catalog, locate the desired Marketplace model, and then select 'View Subscription Options' to review offers, costs, and legal terms before confirming their subscription. The subscription typically completes within 10-15 minutes. It's crucial to understand that subscription itself does not incur charges; pricing is applied only when the model is actively used through various Bedrock actions.",
    "tags": [
      "Subscription",
      "EULA",
      "Pricing",
      "Model Access",
      "Console"
    ]
  },
  {
    "id": 87,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Deploy a Model",
    "content": "Deploying an Amazon Bedrock Marketplace model involves setting up an endpoint managed by SageMaker AI. During deployment, users define key configurations such as the endpoint name, the number of instances, and the instance type. Advanced options like Virtual Private Cloud (VPC) setup, custom service access roles, encryption settings (using an AWS KMS key), and tags can also be configured. A SageMaker AI service role is either automatically created or can be selected by the user to perform actions on their behalf. The deployment process, which can take 10-15 minutes, can be initiated via the AWS Management Console or through API calls like `create-marketplace-model-endpoint` in the AWS CLI. For the endpoint to be fully operational and usable with Amazon Bedrock, it must achieve an `InService` `endpointStatus` and a `REGISTERED` `status`. It's important to avoid modifying critical endpoint settings directly within SageMaker AI, such as `EnableNetworkIsolation` or the model definition, as this can render the endpoint incompatible with Amazon Bedrock; in such cases, deregistering and reregistering the endpoint may be necessary.",
    "tags": [
      "Deployment",
      "Endpoint",
      "SageMaker",
      "Instance Type",
      "VPC",
      "Encryption",
      "API"
    ]
  },
  {
    "id": 88,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Bring Your Own Endpoint",
    "content": "Amazon Bedrock Marketplace allows users to register an existing SageMaker AI endpoint that hosts an Amazon Bedrock Marketplace model. This feature enables greater flexibility for users who have already set up their model hosting environment within SageMaker AI. During the registration process, Amazon Bedrock performs compatibility checks to ensure that the endpoint adheres to specific Marketplace requirements. Critical requirements include having network isolation enabled on the endpoint and ensuring that the model artifacts remain unchanged from the base model provided by the original provider. This ensures the integrity and security of the models when integrated with Bedrock services.",
    "tags": [
      "BYOE",
      "SageMaker Endpoint",
      "Custom Endpoint",
      "Network Isolation",
      "Model Compatibility"
    ]
  },
  {
    "id": 89,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Call the Endpoint",
    "content": "After an Amazon Bedrock Marketplace model has been successfully deployed to an endpoint, users can begin to interact with it by calling the endpoint. The primary API operations used for this purpose are `Converse` and `InvokeModel`. In the AWS Management Console, users can access their deployed model through tools like the 'Playground' by selecting their model's endpoint. For programmatic access, AWS CLI commands can be used, specifying the endpoint's ARN as the `model-id` for both `bedrock-runtime converse` and `bedrock-runtime invoke-model` operations. All models support the `InvokeModel` operation, while some models also support the `Converse` operation. This allows for seamless integration with other Bedrock developer tools, including Guardrails, Model evaluation, Agents, Knowledge Bases, and Prompt management.",
    "tags": [
      "Invocation",
      "API Calls",
      "Converse",
      "InvokeModel",
      "CLI",
      "Model Interaction"
    ]
  },
  {
    "id": 90,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Manage Your Endpoints",
    "content": "Users can view and manage their Amazon Bedrock Marketplace model endpoints to ensure optimal performance and resource utilization. Management tasks include editing the number of instances or changing instance types, modifying associated tags, and deleting endpoints. Additionally, endpoints of Amazon Bedrock Marketplace models initially created in SageMaker AI can be registered and deregistered within Bedrock. Management operations can be performed through the AWS Management Console by navigating to 'Marketplace deployments' under 'Foundation models', where options like 'Register', 'Edit', or 'Delete' are available. For programmatic management, AWS CLI commands such as `delete-marketplace-model-endpoint` and `update-marketplace-model-endpoint` are provided to modify or remove endpoints.",
    "tags": [
      "Endpoint Management",
      "Scaling",
      "Tagging",
      "Deletion",
      "CLI",
      "Console"
    ]
  },
  {
    "id": 91,
    "service": "Amazon Bedrock",
    "category": "Amazon Bedrock Marketplace",
    "title": "Model Compatibility",
    "content": "Regarding model compatibility within Amazon Bedrock Marketplace:\n*   `InvokeModel` operation: All models available through the marketplace are compatible with and can utilize the `InvokeModel` operation.\n*   `Converse` operation: Some models support the `Converse` operation, which is designed for conversational applications. However, embedding and image generation models are not supported by the Converse API. Examples of models compatible with the Converse API include Arcee Lite, Arcee Nova, Arcee SuperNova, Arcee Virtuoso Small, and Writer Palmyra X5. Conversely, Cohere Command (Text) and AI21 Labs Jurassic-2 (Text) do not support chat functionality with the Converse API and can only process single user messages.\n*   Streaming: Support for streaming capabilities varies by model.\n*   Guardrails: For Amazon Bedrock Marketplace models, the `ApplyGuardrail` API must be used to integrate Amazon Bedrock Guardrails for content filtering and safety policies.",
    "tags": [
      "Model Compatibility",
      "InvokeModel",
      "Converse API",
      "Streaming",
      "Guardrails"
    ]
  },
  {
    "id": 92,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Inference: Generate Responses",
    "content": "Inference is the process of generating an output from an input provided to a model. Amazon Bedrock offers a suite of foundation models capable of generating various output modalities, including text and images. Modality support varies by foundation model. You can run model inference directly through the AWS Management Console playgrounds, or by using the Converse, ConverseStream, InvokeModel, or InvokeModelWithResponseStream APIs. For batch processing of multiple prompts, the CreateModelInvocationJob request can be used. Several Amazon Bedrock features also incorporate model inference as part of their workflow. After experimenting with different models, prompts, and inference parameters, you can configure your application to make API calls with your desired specifications.",
    "tags": [
      "inference",
      "generate responses",
      "foundation models",
      "API",
      "console"
    ]
  },
  {
    "id": 93,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "How Inference Works",
    "content": "When you submit an input, or prompt, to a model, the model predicts and returns a probable sequence of tokens as its output. To run inference, you provide the prompt, select a foundation model or inference profile, and can specify stop sequences. Stop sequences are characters that, if generated by the model, will cause it to cease further token generation after that sequence. The chosen model or inference profile also determines the throughput level, which governs the volume and rate of input and output tokens that can be processed.",
    "tags": [
      "inference process",
      "prompts",
      "tokens",
      "stop sequences",
      "throughput"
    ]
  },
  {
    "id": 94,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Invoking Models in Different AWS Regions",
    "content": "Model inference using foundation models is supported in all Regions and with all models supported by Amazon Bedrock. Additionally, Amazon Bedrock supports cross-Region inference (CRIS), which involves cross-Region (system-defined) inference profiles. These profiles route model invocation requests to multiple AWS Regions, allowing for seamless management of unplanned traffic bursts and distribution of traffic across different regions. This capability helps in utilizing compute resources efficiently across geographies.",
    "tags": [
      "AWS Regions",
      "cross-Region inference",
      "CRIS",
      "model invocation",
      "traffic management"
    ]
  },
  {
    "id": 95,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Influence Response Generation with Inference Parameters",
    "content": "Inference parameters are values that can be adjusted during model inference to influence a response. These parameters can affect the randomness and diversity of responses, as well as limit the length of a response or the occurrence of specified sequences. Common inference parameters available across Amazon Bedrock LLMs include Temperature and Top P. Some models, like Anthropic Claude, have an additional Top-k inference parameter, while AI21 Labs Jurassic models come with parameters such as presence penalty, count penalty, frequency penalty, and special token penalty.",
    "tags": [
      "inference parameters",
      "response generation",
      "randomness",
      "diversity",
      "length",
      "temperature",
      "topP",
      "topK",
      "penalties"
    ]
  },
  {
    "id": 96,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Randomness and Diversity",
    "content": "The Temperature parameter (a float value between 0.0 and 1.0, or 0.0 and 2.0 for AI21 Labs Jamba models) controls the randomness and creativity of LLM responses. A lower temperature value results in more deterministic responses, while a higher temperature encourages more variation and creative or different responses for the same prompt. The Top P parameter (a float value between 0.0 and 1.0) controls diversity by defining the percentage of most-likely candidates the model considers for the next token. Lowering Top P causes the model to ignore less probable options, thereby decreasing the diversity of responses. For AI21 Labs Jamba models, `n` controls the number of chat responses to generate, influencing diversity.",
    "tags": [
      "randomness",
      "diversity",
      "temperature",
      "topP",
      "inference parameters"
    ]
  },
  {
    "id": 97,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Length",
    "content": "The length of a model's response can be influenced and limited by inference parameters. For Amazon Titan Text models, the `maxTokenCount` parameter is used to specify the maximum number of tokens allowed in the generated response. Similarly, for Anthropic Claude Text Completions API, `max_tokens_to_sample` dictates the maximum number of tokens to generate. DeepSeek models use `max_tokens` with a range of 1 to 32,768 tokens. For Meta Llama models, `max_gen_len` serves this purpose. Mistral AI models also use `max_tokens` to specify the maximum number of tokens in the generated response, truncating the response if exceeded.",
    "tags": [
      "response length",
      "inference parameters",
      "max tokens",
      "maxTokenCount",
      "max_tokens_to_sample"
    ]
  },
  {
    "id": 98,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Supported Regions and Models",
    "content": "Model inference using foundation models is supported in all AWS Regions and with all models supported by Amazon Bedrock. To find detailed information about specific foundation models, including their IDs, capabilities, and Regional availability, you can refer to the 'Supported foundation models in Amazon Bedrock' documentation. This resource also provides information on model support by AWS Region and feature. Additionally, separate documentation outlines supported Regions and models for other Amazon Bedrock resources like inference profiles, prompt management, fine-tuning, and Guardrails.",
    "tags": [
      "supported regions",
      "supported models",
      "foundation models",
      "regional availability",
      "model IDs"
    ]
  },
  {
    "id": 99,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Prerequisites",
    "content": "To run model inference, the role used must have permissions to perform model invocation API actions. If your role has the `AmazonBedrockFullAccess` AWS managed policy, you can skip this step. Otherwise, you need to attach a policy that allows specific actions, including: \n*   `bedrock:InvokeModel` \n*   `bedrock:InvokeModelWithResponseStream` \n*   `bedrock:GetInferenceProfile` \n*   `bedrock:ListInferenceProfiles` \n*   `bedrock:RenderPrompt` \n*   `bedrock:GetCustomModel` \n*   `bedrock:ListCustomModels` \n*   `bedrock:GetImportedModel` \n*   `bedrock:ListImportedModels` \n*   `bedrock:GetProvisionedModelThroughput` \n*   `bedrock:ListProvisionedModelThroughputs` \n*   `bedrock:GetGuardrail` \n*   `bedrock:ListGuardrails` \n*   `bedrock:ApplyGuardrail` \n\nThese permissions grant access to use the InvokeModel, InvokeModelWithResponseStream, Converse, and ConverseStream actions with all supported resources in Amazon Bedrock. For scenarios involving inference profiles, additional specific policies might be required, for example, to invoke a model only through a particular inference profile.",
    "tags": [
      "permissions",
      "IAM",
      "InvokeModel",
      "InvokeModelWithResponseStream",
      "Converse",
      "ConverseStream",
      "API actions"
    ]
  },
  {
    "id": 100,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Generate Responses in the Console Using Playgrounds",
    "content": "Amazon Bedrock provides user-friendly graphical interfaces called playgrounds in the AWS Management Console for experimenting with model inference. These playgrounds are available for text, image, and chat. Using a playground is equivalent to making an InvokeModel, InvokeModelWithResponseStream, Converse, or ConverseStream request via API. To get started, you must first request access to the desired models. In the chat/text playground, you can select a mode (e.g., chat), choose a model, and then submit prompts. Prompts can include text, images, or videos, with subsequent prompts in chat mode incorporating previous interactions for context. You can adjust configurations like inference parameters, system prompts (in chat mode), and apply Guardrails. Streaming responses are typically enabled by default but can be toggled. The console also allows comparing responses from different models and exporting responses as JSON files. Amazon Bedrock does not store any input data (text, images, or documents) provided in the playgrounds; it is only used for response generation. Model metrics such as latency, input token count, and output token count are available.",
    "tags": [
      "console",
      "playgrounds",
      "text playground",
      "image playground",
      "chat playground",
      "prompts",
      "inference parameters",
      "system prompts",
      "Guardrails",
      "streaming",
      "model comparison"
    ]
  },
  {
    "id": 101,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Enhance Model Responses with Model Reasoning",
    "content": "Model reasoning allows models to output their internal thought processes, which can enhance responses. For models like DeepSeek-R1, reasoning is always enabled by default and cannot be toggled off. This model's token count for responses includes both output and reasoning tokens, with a maximum of 8192 tokens. Similarly, Anthropic Claude's extended thinking feature enables Claude to generate 'thinking content blocks' that detail its internal reasoning before formulating a final response. The API response will include these thinking blocks followed by text content blocks, and this can also be streamed via server-sent events (SSE). When using tools with thinking, the thinking blocks must be passed back to the API to maintain reasoning continuity. The API response may also include 'redacted_thinking' blocks for encrypted reasoning content.",
    "tags": [
      "model reasoning",
      "extended thinking",
      "DeepSeek-R1",
      "Anthropic Claude",
      "thinking content blocks",
      "API response"
    ]
  },
  {
    "id": 102,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Optimize Model Inference for Latency",
    "content": "The Latency Optimized Inference feature in Amazon Bedrock is currently in preview release. This optional feature is designed to reduce inference response latency and input token costs. It achieves this by leveraging prompt caching, which allows the model to skip recomputation of previously processed inputs by storing portions of context in a cache. This shared compute saving contributes to lower response latencies. Latency Optimized Inference is available in specific regions for models like Amazon Nova Pro, Anthropic Claude 3.5 Haiku, Meta Llama 3.1 405B Instruct, and Meta Llama 3.1 70B Instruct.",
    "tags": [
      "latency optimization",
      "prompt caching",
      "inference latency",
      "cost reduction",
      "preview feature"
    ]
  },
  {
    "id": 103,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Generate Responses Using the API",
    "content": "Amazon Bedrock offers several API operations for conducting model inference programmatically:\n*   InvokeModel: Submits a single prompt to generate a non-streaming response. The request body is model-specific.\n*   InvokeModelWithResponseStream: Submits a single prompt to generate streaming responses.\n*   Converse: Submits a prompt and generates responses with a unified structure across all models, supporting system prompts and conversation history. Model-specific fields can be included in `additionalModelRequestFields`.\n*   ConverseStream: Generates streaming responses for conversational applications, also with a unified structure and support for conversation history.\n*   StartAsyncInvoke: Submits a prompt to generate a response asynchronously, primarily used for video generation.\n*   OpenAI Chat Completions API: Allows using the OpenAI Chat Completions API with supported Amazon Bedrock models.\n\nWhen using these APIs, you need to determine the Model ID (or ARN) of the foundation model, inference profile, or prompt management resource, and construct a request body containing model-specific inference parameters and configurations.",
    "tags": [
      "API",
      "InvokeModel",
      "InvokeModelWithResponseStream",
      "Converse",
      "ConverseStream",
      "StartAsyncInvoke",
      "OpenAI Chat Completions API",
      "model ID",
      "request body",
      "streaming"
    ]
  },
  {
    "id": 104,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Submit a Single Prompt",
    "content": "You can run inference on a single prompt using the InvokeModel and InvokeModelWithResponseStream API operations. Amazon Bedrock models vary in their support for text, image, or video inputs and their ability to produce text, image, or embeddings outputs. Some models also support streaming responses. To generate video outputs, you must use the `StartAsyncInvoke` API instead of InvokeModel. It's important to note that you cannot include documents directly in the request body for InvokeModel; this functionality is available through the Chat/text playground in the console or by sending a Converse request. The `modelId` and `body` fields are required parameters for these API calls, with the request body being model-specific. Code examples for various models, including Amazon Titan Text Premier, Stability AI Stable Diffusion XL, and Amazon Titan Text Embeddings V2, demonstrate how to submit prompts and retrieve responses. Multimodal prompts, such as generating text from an image (e.g., Anthropic Claude 3 Haiku) or a video (e.g., Amazon Nova Lite), are also supported.",
    "tags": [
      "single prompt",
      "InvokeModel",
      "InvokeModelWithResponseStream",
      "multimodal input",
      "text output",
      "image output",
      "embeddings output",
      "streaming"
    ]
  },
  {
    "id": 105,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Use the OpenAI Chat Completions API",
    "content": "You can run model inference using the OpenAI Create chat completion API with supported Amazon Bedrock models. This can be done by making an HTTP request with an Amazon Bedrock Runtime endpoint or by using an OpenAI SDK request with the same endpoint. This API is compatible with all OpenAI models available in Amazon Bedrock and the AWS Regions that support them.\n\nPrerequisites include:\n*   Authentication: For the OpenAI SDK, authentication is only supported with an Amazon Bedrock API key. For HTTP requests, either AWS credentials or an Amazon Bedrock API key can be used.\n*   Endpoint: You need the Amazon Bedrock Runtime endpoint, formatted as `https://${bedrock-runtime-endpoint}/openai/v1` for the OpenAI SDK, or `https://${bedrock-runtime-endpoint}/openai/v1/chat/completions` for direct HTTP requests.\n*   Model Access: You must request access to an OpenAI model that supports this feature.\n*   SDK Installation: If using the OpenAI SDK, it must be installed.\n\nWhen constructing the request body, you map the OpenAI fields to Bedrock's `Converse` API fields for consistent use, such as `max_completion_tokens` to `maxTokens`, and `temperature` to `temperature`. Guardrails can also be included in chat completions by specifying guardrail information in headers (e.g., `X-Amzn-Bedrock-GuardrailIdentifier`) and additional body parameters (e.g., `amazon-bedrock-guardrailConfig`).",
    "tags": [
      "OpenAI Chat Completions API",
      "chat completion",
      "OpenAI SDK",
      "HTTP request",
      "authentication",
      "API key",
      "endpoint",
      "guardrails"
    ]
  },
  {
    "id": 106,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Carry out a Conversation with Converse",
    "content": "The Amazon Bedrock Converse API (including `Converse` and `ConverseStream` operations) is designed to create conversational applications, such as chat bots that maintain conversation history and adopt specific personas. It offers a consistent API that works across all Amazon Bedrock models supporting messages, tool use, and Guardrails, simplifying development compared to using base inference operations like `InvokeModel`.\n\nKey fields in a Converse request include:\n*   `modelId`: Required, specifies the model to use.\n*   `messages`: Defines content and role (user/assistant) of prompts, allowing conversation context to be maintained by including all prior messages. Supports text, images (up to 20), and documents (up to 5) as content.\n*   `system`: Specifies system prompts for model instructions or persona.\n*   `inferenceConfig`: Sets common inference parameters like `maxTokens`, `stopSequences`, `temperature`, and `topP`.\n*   `toolConfig`: Used to include a tool for the model to generate responses.\n*   `guardrailConfig`: Applies a created guardrail to the conversation.\n\nThe response includes an `output` message, a `stopReason`, `usage` metrics (input/output/total tokens), and `metrics` (latency). For streaming, events like `messageStart`, `contentBlockDelta`, and `messageStop` are emitted. Models like AI21 Labs Jurassic-2 (Text) and Cohere Command (Text) have limited chat support and cannot maintain conversation history with Converse API.",
    "tags": [
      "Converse API",
      "ConverseStream",
      "conversational applications",
      "chatbots",
      "conversation history",
      "system prompts",
      "inference parameters",
      "tool use",
      "Guardrails",
      "messages"
    ]
  },
  {
    "id": 107,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Use a Tool to Complete a Model Response",
    "content": "You can provide Amazon Bedrock models with access to tools to help them generate responses to user messages. For example, a chat application might use a tool to query information like popular songs on a radio station. The tool's implementation can be an API, database, Lambda function, or other software, which you define.\n\nTo use tools, the Converse API (Converse or ConverseStream) is recommended due to its consistent API across models that support tool use. The process involves several steps:\n1.  Send the message and tool definition: The user's message and the tool's schema (e.g., `top_song` with a `sign` parameter) are sent to the model.\n2.  Get the tool request from the model: The model determines if the tool is needed and returns a response containing a `toolUse` field, specifying the tool's name and input parameters, with a `stopReason` of `tool_use`.\n3.  Make the tool request for the model: Your code calls the actual tool with the parameters provided by the model. The result from the tool is then encapsulated in a `toolResult` content block within a new user message and sent back to the model.\n4.  Get the model response: The model processes the tool's result and generates a final response to the original user message, with a `stopReason` of `end_turn`.",
    "tags": [
      "tool use",
      "Converse API",
      "tools",
      "model response",
      "function calling",
      "tool definition",
      "tool request",
      "tool result"
    ]
  },
  {
    "id": 108,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Call a Tool with the Converse API",
    "content": "When using the Converse API for tool use, after you send a message along with your tool definitions, the model processes this information. The model uses the provided tool definition schema to determine if a tool is necessary to answer the user's message. For instance, if a user asks 'What's the most popular song on WZPZ?', the model might match this query with a `top_song` tool definition. The model's response, in this case, will contain a `toolUse` content block, detailing the `toolUseId`, the `name` of the tool, and the `input` parameters the tool requires (e.g., `{\"sign\": \"WZPZ\"}`), and the `stopReason` will be `tool_use`. This indicates that the model is requesting your application to execute the tool with the specified inputs.",
    "tags": [
      "Converse API",
      "tool invocation",
      "tool definition",
      "toolUse",
      "stopReason: tool_use"
    ]
  },
  {
    "id": 109,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Converse API Tool Use Examples",
    "content": "Python code examples demonstrate how to use tools with the Converse API for both synchronous and asynchronous operations. These examples illustrate a fictional scenario where a model uses a `top_song` tool to find the most popular song on a radio station.\n\nFor synchronous tool use with `Converse`, the `generate_text` function handles the initial message, receives the `tool_use` request, calls the external tool (e.g., `get_top_song`), and then sends the `toolResult` back to the model for the final response.\n\nFor asynchronous tool use with `ConverseStream`, the `stream_messages` function processes the stream of events. It captures `messageStart`, `contentBlockStart` (for `toolUse` details), `contentBlockDelta` (for partial `toolUse` inputs or text), and `contentBlockStop` events to reconstruct the tool request. Upon receiving `stop_reason: tool_use`, the application then executes the tool and submits the `toolResult` in a subsequent message to continue the streamed conversation. These examples use models like Cohere Command R and Anthropic Claude 3 Haiku.",
    "tags": [
      "Converse API",
      "tool use examples",
      "Python SDK",
      "synchronous",
      "asynchronous",
      "streaming",
      "Cohere Command R",
      "Anthropic Claude 3 Haiku"
    ]
  },
  {
    "id": 110,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Use a Computer Use Tool to Complete a Model Response",
    "content": "The computer use tool is a beta capability offered with Anthropic Claude 3.7 Sonnet and Claude 3.5 Sonnet v2 models in Amazon Bedrock. This feature allows Claude to automate tasks through basic GUI actions, such as navigating the AWS console. To enable this, the tool definition in the `additionalModelRequestFields` of a `converse` call specifies `type: \"computer_20241022\"`, `name: \"computer\"`, and display dimensions (e.g., `display_height_px`, `display_width_px`). The model can then respond with `tool_use` blocks that include actions like `\"action\": \"screenshot\"` for GUI interaction.",
    "tags": [
      "computer use tool",
      "Anthropic Claude",
      "beta feature",
      "GUI automation",
      "tool_use"
    ]
  },
  {
    "id": 111,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Example Code (for Computer Use Tool)",
    "content": "An example Python code snippet demonstrates calling the `bedrock.converse` API with a computer use tool. The code constructs a message that includes text ('Go to the bedrock console') and an image (e.g., of the AWS console). The `additionalModelRequestFields` parameter defines the `computer` tool with specific properties: `type: \"computer_20241022\"`, `name: \"computer\"`, and `display_height_px`, `display_width_px`, and `display_number`. This configuration allows the Anthropic Claude 3.5 Sonnet v2 model to interpret the request and potentially interact with a simulated GUI environment.",
    "tags": [
      "example code",
      "computer use tool",
      "Python SDK",
      "Converse API",
      "Anthropic Claude 3.5 Sonnet v2",
      "multimodal input"
    ]
  },
  {
    "id": 112,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Example Response (for Computer Use Tool)",
    "content": "The example response from using the computer use tool illustrates how the model communicates its intent to perform a GUI action. After processing the input (text and an image of the AWS console), the assistant's `content` includes a text response (e.g., 'I can see from the screenshot that we're already in the AWS Console. To go to the Amazon Bedrock console specifically, I'll click on the Amazon Bedrock service from the \"Recently Visited\" section.'). This is followed by a `tool_use` content block with `type: \"tool_use\"`, `id`, `name: \"computer\"`, and `input: {\"action\": \"screenshot\"}`. The `stop_reason` is `\"tool_use\"`, indicating that the model requires the application to perform a tool action. The response also provides `usage` details, including `input_tokens` and `output_tokens`.",
    "tags": [
      "example response",
      "computer use tool",
      "tool_use",
      "Anthropic Claude",
      "GUI action"
    ]
  },
  {
    "id": 113,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Prompt Caching",
    "content": "Prompt caching is an optional feature in Amazon Bedrock designed to reduce inference response latency and input token costs for supported models. It is particularly beneficial for workloads with long and repeated contexts that are frequently reused across multiple queries, such as chatbots processing uploaded documents. Prompt caching can be utilized with the Converse and ConverseStream APIs, the InvokeModel and InvokeModelWithResponseStream APIs, and in the Amazon Bedrock console playgrounds. It also supports cross-Region inference, which may lead to increased cache writes during high demand due to traffic distribution across regions.",
    "tags": [
      "prompt caching",
      "latency reduction",
      "cost saving",
      "inference optimization",
      "Converse API",
      "InvokeModel API",
      "playgrounds"
    ]
  },
  {
    "id": 114,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "How Prompt Caching Works",
    "content": "Prompt caching works by storing portions of your context in a cache, allowing the model to skip recomputation of those inputs. This process enables Amazon Bedrock to share in the compute savings and ultimately lowers response latencies. For example, in a chatbot where users frequently ask questions about the same uploaded document, caching the document prevents reprocessing it for every new query, saving time and resources. When prompt caching is active, the response from the Converse API includes `CacheReadInputTokens` and `CacheWriteInputTokens` values, which indicate the number of tokens read from and written to the cache, respectively. These cached token costs are generally lower than full model inference.",
    "tags": [
      "prompt caching mechanism",
      "cache recomputation",
      "latency reduction",
      "cost efficiency",
      "token usage"
    ]
  },
  {
    "id": 115,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Supported Models, Regions, and Limits (for Prompt Caching)",
    "content": "Amazon Bedrock prompt caching is generally available for several models, including Claude 3.7 Sonnet, Claude 3.5 Haiku, Amazon Nova Micro, Amazon Nova Lite, Amazon Nova Pro, and Amazon Nova Premier. While Claude 3.5 Sonnet v2 was accessible during a preview, new customers are no longer granted access to prompt caching for this specific model.\n\nKey limits and supported fields for prompt caching vary by model:\n*   Minimum number of tokens per cache checkpoint typically ranges from 1K to 2K.\n*   Maximum number of cache checkpoints per request is generally 4.\n*   Fields that accept prompt cache checkpoints include `system`, `messages`, and `tools`.\n\nFor example, Anthropic Claude 3.5 Sonnet v2 (preview) supports `system`, `messages`, and `tools` fields with a minimum of 1,024 tokens and a maximum of 4 checkpoints. Amazon Nova Micro and Lite models are generally available, supporting `system` and `messages` fields with 1K tokens and 4 checkpoints.",
    "tags": [
      "prompt caching support",
      "supported models",
      "AWS Regions",
      "cache limits",
      "Claude 3.7 Sonnet",
      "Claude 3.5 Haiku",
      "Amazon Nova"
    ]
  },
  {
    "id": 116,
    "service": "Amazon Bedrock",
    "category": "Model Inference",
    "title": "Getting Started (for Prompt Caching)",
    "content": "Getting started with prompt caching involves different approaches depending on the method of interaction with Amazon Bedrock models:\n\n*   Converse API: Offers advanced and flexible options for implementing prompt caching in multi-turn conversations. You can place cache checkpoints in the `messages`, `system`, or `tools` fields of your request. For example, a system prompt or a tool definition can include a `cachePoint` field with `\"type\": \"default\"`. The model response will then include `CacheReadInputTokens` and `CacheWriteInputTokens` metrics.\n\n*   InvokeModel API: Prompt caching is enabled by default for `InvokeModel` calls. You can set cache checkpoints at any point within your request body, similar to the Converse API. Examples are provided for structuring requests for Anthropic Claude 3.5 Sonnet v2 and Amazon Nova models, detailing how to embed `cache_control` or `cachePoint` within the content.\n\n*   Playground: In the Amazon Bedrock console's chat playground, you can turn on the prompt caching option in the `Configurations` menu. Amazon Bedrock will then automatically create cache checkpoints once your combined input and model responses reach the model's minimum required token count. These checkpoints can be viewed, and caching metrics (tokens read/written) are displayed in the playground responses.",
    "tags": [
      "prompt caching tutorial",
      "getting started",
      "Converse API",
      "InvokeModel API",
      "Playground",
      "cache checkpoints",
      "system prompts",
      "tools",
      "caching metrics"
    ]
  },
  {
    "id": 117,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Batch Inference: Process Multiple Prompts",
    "content": "Batch inference in Amazon Bedrock allows you to process multiple prompts asynchronously, which is highly efficient for large datasets. You can submit a single request to process a large number of inputs, with the responses being generated and stored in an Amazon S3 bucket. This feature significantly improves the performance of model inference on extensive collections of data. However, it is important to note that batch inference is not supported for provisioned models.",
    "tags": [
      "batch inference",
      "asynchronous",
      "s3",
      "large datasets"
    ]
  },
  {
    "id": 118,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Regions and Models for Batch Inference",
    "content": "For Amazon Bedrock batch inference, model inference using foundation models is generally supported in all Regions where Bedrock is available. Specific regional support for different models that support batch inference is detailed in the documentation, including information on foundation models, inference profiles, and custom models. Users should refer to the 'Supported foundation models in Amazon Bedrock' and 'Amazon Bedrock endpoints and quotas' for the most current lists of compatible models and regions.",
    "tags": [
      "supported regions",
      "models",
      "compatibility",
      "availability"
    ]
  },
  {
    "id": 119,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Prerequisites for Batch Inference Jobs",
    "content": "To perform batch inference in Amazon Bedrock, several prerequisites must be met. You need to prepare your dataset and upload it to an Amazon S3 bucket, and also create a separate S3 bucket for your output data. Additionally, you must set up batch inference-related permissions for the relevant IAM identities, including both the identity that creates and manages jobs and the service role Bedrock assumes. Optionally, a VPC can be configured for enhanced data protection.",
    "tags": [
      "prerequisites",
      "s3",
      "iam permissions",
      "setup"
    ]
  },
  {
    "id": 120,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Setting Up Input Data for Batch Inference",
    "content": "Input data for batch inference jobs must be placed in an Amazon S3 location and formatted as JSONL files. Each line in a JSONL file should contain a JSON object with a recordId and a modelInput field, where modelInput matches the request body for the chosen model's InvokeModel request. If content is defined as an S3 location, the specified S3 path must contain both your content and JSONL files. Ensure your inputs adhere to batch inference quotas for record counts and file sizes.",
    "tags": [
      "data setup",
      "jsonl",
      "s3 input",
      "modelinput",
      "quotas"
    ]
  },
  {
    "id": 121,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Required Permissions for Batch Inference",
    "content": "For batch inference, permissions must be configured for two key IAM identities: the identity managing the jobs and the service role assumed by Amazon Bedrock. The managing identity needs permissions like `CreateModelInvocationJob`, `GetModelInvocationJob`, and `ListModelInvocationJobs`. The service role requires permissions to perform actions on your behalf, including accessing input and output S3 buckets. While `AmazonBedrockFullAccess` provides broad access, it's a security best practice to grant only the necessary, scoped-down actions.",
    "tags": [
      "iam",
      "permissions",
      "service role",
      "access control"
    ]
  },
  {
    "id": 122,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Optional VPC Setup for Batch Inference",
    "content": "For enhanced data security during batch inference, you can optionally set up a Virtual Private Cloud (VPC) with Amazon VPC. This protects your data by ensuring it is not accessible over the internet, instead establishing a private connection via a VPC interface endpoint with AWS PrivateLink. To implement this, you must configure the VPC, potentially set up an S3 VPC endpoint, and attach specific VPC permissions to your batch inference service role. When creating the job via API, you specify the VPC subnets and security groups, prompting Bedrock to create elastic network interfaces (ENIs).",
    "tags": [
      "vpc",
      "security",
      "data protection",
      "privatelink",
      "network"
    ]
  },
  {
    "id": 123,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Creating a Batch Inference Job",
    "content": "To create a batch inference job in Amazon Bedrock, you first prepare your S3 input data according to the specified format. In the console, you provide a job name, select a model, and specify input and output S3 locations. You also configure a service access role for Bedrock, choosing an existing role or allowing automatic creation. For API users, a `CreateModelInvocationJob` request is sent with parameters like `jobName`, `roleArn`, `modelId`, `inputDataConfig`, and `outputDataConfig`, with optional `vpcConfig` and `tags`.",
    "tags": [
      "create job",
      "job creation",
      "s3",
      "console",
      "api"
    ]
  },
  {
    "id": 124,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Batch Inference Jobs",
    "content": "You can monitor the progress and status of your batch inference jobs in Amazon Bedrock through the console or API. The console displays the job's overall status on its details page. Via API, you can use `GetModelInvocationJob` for specific job details or `ListModelInvocationJobs` for multiple jobs, filtering by status. Furthermore, job status and record counts (total, processed, successful, error, input/output tokens) are summarized in the `manifest.json.out` file located in your output Amazon S3 bucket, providing detailed progress insights.",
    "tags": [
      "monitor jobs",
      "job status",
      "s3 output",
      "api",
      "cloudwatch"
    ]
  },
  {
    "id": 125,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Stopping a Batch Inference Job",
    "content": "You can stop an ongoing batch inference job in Amazon Bedrock either through the AWS Management Console or via the API. In the console, you navigate to the Batch inference section, select the job, and choose 'Stop job'. Using the API, you send a `StopModelInvocationJob` request with the job's identifier. Upon successful stoppage, an HTTP 200 response is received. It is important to remember that you will be charged for tokens that have already been processed up to the point of stopping the job.",
    "tags": [
      "stop job",
      "cancel job",
      "resource management",
      "cost control"
    ]
  },
  {
    "id": 126,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Viewing Batch Inference Job Results",
    "content": "Once a batch inference job in Amazon Bedrock is 'Completed', its results can be retrieved from the specified Amazon S3 bucket. The S3 bucket will contain an output JSONL file for each input JSONL file, with each line showing the `recordId`, `modelInput`, and either `modelOutput` (matching the InvokeModel response format) or an `error` object if inference failed. Additionally, a `manifest.json.out` file provides a summary of the entire job, including total, processed, success, and error record counts, along with input and output token counts.",
    "tags": [
      "job results",
      "output data",
      "s3 output",
      "jsonl",
      "manifest"
    ]
  },
  {
    "id": 127,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Batch Inference Code Example (Python)",
    "content": "A Python code example for batch inference in Amazon Bedrock typically involves creating a JSONL file with input records and uploading it to an S3 input bucket, and also setting up an S3 output bucket. The code then uses the Boto3 client to call `create_model_invocation_job`, providing the service role ARN, model ID, job name, and S3 input/output configurations. Subsequent calls like `get_model_invocation_job` can retrieve the job status, `list_model_invocation_jobs` can list jobs with filters, and `stop_model_invocation_job` can terminate an active job. This illustrates the full lifecycle of a batch inference task programmatically.",
    "tags": [
      "code example",
      "python",
      "boto3",
      "automation",
      "s3"
    ]
  },
  {
    "id": 128,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Inference Profiles: Model Invocation Resources",
    "content": "Inference profiles are Amazon Bedrock resources that define a model and one or more Regions for routing model invocation requests. They help track usage metrics, monitor costs with tags, and enable cross-Region inference for increased throughput and performance. There are two types: Cross Region (system-defined) inference profiles (predefined by Amazon Bedrock for multiple Regions) and Application inference profiles (user-created for tracking costs and usage). These profiles can be used with features like model inference, knowledge bases, model evaluation, prompt management, and flows. Pricing is calculated based on the model's price in the Region from which the profile is called.",
    "tags": [
      "inference profiles",
      "model invocation",
      "cross-region",
      "cost tracking",
      "performance"
    ]
  },
  {
    "id": 129,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Regions and Models for Inference Profiles",
    "content": "Application inference profiles are supported in a wide array of AWS Regions, including US East (N. Virginia), US East (Ohio), US West (Oregon), AWS GovCloud (US-East), and several Asia Pacific (Tokyo, Seoul, Mumbai, Singapore, Sydney), Canada (Central), and Europe (Frankfurt, Ireland, London, Paris, Zurich, Stockholm, Milan, Spain) regions, as well as South America (São Paulo). These profiles can be created from all foundation models and other inference profiles supported within Amazon Bedrock. Generally, model inference with foundation models is available across all Regions and models supported by Amazon Bedrock.",
    "tags": [
      "regions",
      "models",
      "support",
      "availability",
      "foundation models"
    ]
  },
  {
    "id": 130,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Cross-Region Inference Profiles",
    "content": "Cross-Region (system-defined) inference profiles are designed to manage unplanned traffic bursts by utilizing compute resources across different AWS Regions. This capability distributes model invocation requests across multiple regions to enhance throughput and performance. Requests are routed from a Source Region to any designated Destination Regions defined within the profile. Examples include profiles for Amazon Nova, Anthropic Claude 3 Haiku, and Meta Llama models across various US, EU, and APAC regions. These system-defined profiles are immutable, though new ones may be introduced to include additional regions.",
    "tags": [
      "cross-region inference",
      "system-defined",
      "traffic management",
      "throughput",
      "model routing"
    ]
  },
  {
    "id": 131,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Application Inference Profiles: Supported Regions and Models",
    "content": "Application inference profiles are supported across numerous AWS Regions, including US East (N. Virginia), US East (Ohio), US West (Oregon), AWS GovCloud (US-East), Asia Pacific (Tokyo, Seoul, Mumbai, Singapore, Sydney), Canada (Central), Europe (Frankfurt, Ireland, London, Paris, Zurich, Stockholm, Milan, Spain), and South America (São Paulo). These profiles can be generated from any foundation models and system-defined inference profiles available in Amazon Bedrock. This broad support enables users to effectively track costs and monitor usage for their models in single or multiple specified Regions.",
    "tags": [
      "application profiles",
      "regions",
      "model support",
      "cost tracking",
      "usage monitoring"
    ]
  },
  {
    "id": 132,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Inference Profile Prerequisites",
    "content": "To utilize an inference profile, your IAM role must possess the necessary permissions for inference profile API actions. This typically involves an IAM policy allowing actions like `bedrock:InvokeModel*` and `bedrock:CreateInferenceProfile` across various Amazon Bedrock resources. Permissions can be granularly restricted to specific actions or inference profiles. It is also possible to enforce that a foundation model is invoked only through a particular inference profile using the `bedrock:InferenceProfileArn` condition key. Additionally, access to the underlying model within the profile must be explicitly granted in the target Region.",
    "tags": [
      "prerequisites",
      "iam",
      "permissions",
      "access control",
      "security"
    ]
  },
  {
    "id": 133,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Creating an Application Inference Profile",
    "content": "An application inference profile can be created to monitor model usage and costs, whether for a single Region or across multiple regions. Currently, this process is exclusively performed via the Amazon Bedrock API by sending a `CreateInferenceProfile` request. Essential fields include `inferenceProfileName` for identification and `modelSource` to specify the foundation model or cross-Region inference profile it will track. Optional parameters like `description`, tags for cost allocation, and `clientRequestToken` for idempotency can also be included. The API response provides an `inferenceProfileArn` for subsequent management and reference.",
    "tags": [
      "create",
      "api",
      "application profile",
      "cost management",
      "usage tracking"
    ]
  },
  {
    "id": 134,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Modifying Application Inference Profile Tags",
    "content": "Once an application inference profile is established, its associated tags can be modified using the Amazon Bedrock API. This is achieved by submitting a `TagResource` or `UntagResource` request to an Amazon Bedrock control plane endpoint. The request requires specifying the ARN of the application inference profile in the `resourceArn` field. This functionality is crucial for effectively organizing, managing, and tracking costs related to your inference resources within your AWS environment, allowing for detailed cost allocation and resource categorization.",
    "tags": [
      "modify tags",
      "api",
      "cost allocation",
      "resource management",
      "tagging"
    ]
  },
  {
    "id": 135,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Viewing Inference Profile Information",
    "content": "Information regarding cross-Region inference profiles and application inference profiles can be accessed through both the AWS Management Console and the Amazon Bedrock API. In the console, details and associated Regions for cross-Region inference profiles are visible under the 'Cross-Region inference' section. However, application inference profiles cannot be viewed directly in the console. Via the API, a `GetInferenceProfile` request retrieves specific profile information using its ARN or ID, while `ListInferenceProfiles` provides a list of multiple profiles, supporting pagination with `maxResults` and `nextToken` parameters.",
    "tags": [
      "view information",
      "console",
      "api",
      "monitoring",
      "details"
    ]
  },
  {
    "id": 136,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Using Inference Profiles in Model Invocation",
    "content": "An inference profile can be used in place of a foundation model for model invocation to route requests across multiple Regions or to track costs and usage. In the AWS Management Console, you select the inference profile from a feature's model selection, such as in the Chat/Text playground. Programmatically, you specify the ARN or ID of the inference profile in the `modelId` field for `InvokeModel`, `Converse`, `CreateEvaluationJob`, `CreatePrompt`, and `CreateFlow` operations. For knowledge bases, it is used in the `modelArn` field of `RetrieveAndGenerate` and `CreateDataSource` requests.",
    "tags": [
      "model invocation",
      "inference",
      "api",
      "console",
      "flows"
    ]
  },
  {
    "id": 137,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Deleting an Application Inference Profile",
    "content": "Application inference profiles can only be removed using the Amazon Bedrock API. To delete a profile, a `DeleteInferenceProfile` request must be sent to an Amazon Bedrock control plane endpoint. This request requires specifying the Amazon Resource Name (ARN) or ID of the inference profile in the `inferenceProflieIdentifier` field. This action permanently removes the user-created profile and ceases the tracking of model usage and associated costs.",
    "tags": [
      "delete",
      "api",
      "application profile",
      "resource management",
      "cleanup"
    ]
  },
  {
    "id": 138,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Prompt engineering concepts",
    "content": "Prompt engineering is the practice of optimizing textual input to a Large Language Model (LLM) to obtain desired responses. It is essentially the art of communicating with an LLM. This process involves crafting and optimizing input prompts by selecting appropriate words, phrases, sentences, punctuation, and separator characters. High-quality prompts condition the LLM to generate desired or better responses for a wide variety of applications, including classification, question answering, code generation, and creative writing. The best approach depends on the specific task and data.",
    "tags": [
      "prompt engineering",
      "llm",
      "optimization",
      "input",
      "response"
    ]
  },
  {
    "id": 139,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "What is a prompt?",
    "content": "A prompt is a specific set of inputs provided by a user that guides Large Language Models (LLMs) on Amazon Bedrock to generate an appropriate response or output for a given task or instruction. For instance, a text prompt can be a single line for the model to respond to, or it can detail instructions or a task to perform. Prompts can include context, examples of desired outputs, or text for the model to use in its response. They are used for tasks like classification, question answering, code generation, and creative writing.",
    "tags": [
      "prompt",
      "input",
      "llm",
      "response",
      "task"
    ]
  },
  {
    "id": 140,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Components of a prompt",
    "content": "A single prompt typically includes several key components to effectively guide Large Language Models (LLMs). These components consist of the task or instruction the LLM needs to perform, the context relevant to the task (such as a domain description), demonstration examples to show desired output formats, and the specific input text the LLM should use in its response. Depending on the use case and data availability, a prompt should combine one or more of these elements for optimal results.",
    "tags": [
      "prompt",
      "components",
      "instruction",
      "context",
      "examples"
    ]
  },
  {
    "id": 141,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Few-shot prompting vs. zero-shot prompting",
    "content": "Few-shot prompting is a technique where a prompt includes a few examples of input-output pairs to help Large Language Models (LLMs) calibrate their responses to meet specific expectations, also known as in-context learning. In contrast, zero-shot prompting involves providing no example input-output pairs within the prompt text. For instance, a zero-shot sentiment classification prompt asks the model to classify sentiment without any prior examples, while a few-shot version includes one or more such examples. Including examples can significantly improve LLMs' responses, especially for complex tasks.",
    "tags": [
      "few-shot",
      "zero-shot",
      "prompting",
      "examples",
      "in-context learning"
    ]
  },
  {
    "id": 142,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Prompt template",
    "content": "A prompt template is a structured \"recipe\" that specifies the formatting of a prompt, allowing for exchangeable content, or variables, to be filled in for different use cases. These templates can include instructions, few-shot examples, and specific context or questions relevant to a particular task like classification, summarization, or question answering. Variables are marked by double curly braces (e.g., `{{variable}}`) and are filled in when the prompt is called at runtime. This feature in Amazon Bedrock's Prompt management helps users save time by reusing prompts across workflows.",
    "tags": [
      "prompt template",
      "variables",
      "formatting",
      "reusable",
      "prompt management"
    ]
  },
  {
    "id": 143,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Maintain recall over inference requests",
    "content": "When accessing Amazon Bedrock models via API calls, models do not automatically recall prior prompts or previous requests unless that past interaction is explicitly included within the current prompt. To maintain a conversational style or make follow-up requests, the entire history of previous prompts and responses must be submitted with each new request, effectively providing the necessary context. For Anthropic Claude models, this typically means wrapping prompts in a conversational format like `\\n\\nHuman: {{Main Content}}\\n\\nAssistant:` when using the API.",
    "tags": [
      "recall",
      "inference",
      "api calls",
      "conversation",
      "context"
    ]
  },
  {
    "id": 144,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "What is prompt engineering?",
    "content": "Prompt engineering is the practice of crafting and optimizing input prompts for Large Language Models (LLMs). It involves carefully selecting words, phrases, sentences, punctuation, and separator characters to effectively use LLMs for various applications. Essentially, it's the art of communicating with an LLM, where high-quality prompts condition the model to generate desired or better responses. The most effective approach for prompt engineering is dependent on both the specific task and the data involved.",
    "tags": [
      "prompt engineering",
      "llm",
      "optimization",
      "communication",
      "input"
    ]
  },
  {
    "id": 145,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Intelligent prompt routing",
    "content": "Intelligent prompt routing in Amazon Bedrock offers a single serverless endpoint designed to efficiently route requests among different foundational models within the same model family. This feature dynamically predicts the response quality of each model for an incoming request and then directs the request to the model expected to provide the best response, thereby optimizing for both quality and cost. It supports both default and configurable prompt routers. This routing is currently optimized for English prompts and cannot adjust decisions based on application-specific performance data.",
    "tags": [
      "prompt routing",
      "serverless",
      "foundational models",
      "optimization",
      "cost"
    ]
  },
  {
    "id": 146,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Supported Regions and models",
    "content": "Intelligent prompt routing in Amazon Bedrock is supported in specific Regions and models. Currently, for default prompt routers, customers can choose from select models in the Anthropic and Meta families during the preview phase. While the specific supported models and regions are listed in the documentation, it is important to note that this feature is optimized for English prompts and has certain limitations.",
    "tags": [
      "prompt routing",
      "regions",
      "models",
      "anthropic",
      "meta"
    ]
  },
  {
    "id": 147,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Benefits",
    "content": "The primary benefit of intelligent prompt routing is its ability to optimize for both response quality and cost. By dynamically predicting the response quality of various foundational models for each incoming request, it routes the request to the model that offers the best outcome. This ensures efficient resource utilization and can lead to more consistent and high-quality outputs while managing operational expenses effectively.",
    "tags": [
      "prompt routing",
      "benefits",
      "quality",
      "cost",
      "optimization"
    ]
  },
  {
    "id": 148,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Default and configured prompt routers",
    "content": "Amazon Bedrock offers two types of prompt routers for intelligent prompt routing: default prompt routers and configured prompt routers. Default prompt routers are pre-configured systems provided by Amazon Bedrock, designed to work out-of-the-box with specific foundational models and offer a ready-to-use solution without needing any setup. In contrast, configured prompt routers allow users greater flexibility to choose additional models and define their own routing criteria, such as response quality differences, to tailor the routing decisions more specifically to their application's needs.",
    "tags": [
      "prompt routers",
      "default",
      "configured",
      "routing criteria",
      "customization"
    ]
  },
  {
    "id": 149,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Considerations and limitations",
    "content": "Intelligent prompt routing in Amazon Bedrock comes with specific considerations and limitations. Primarily, it is only optimized for English prompts. The system cannot adjust routing decisions or responses based on application-specific performance data, which means its effectiveness for unique or highly specialized use cases might vary. This is due to its reliance on the initial training data, which might not always provide the most optimal routing for every niche application.",
    "tags": [
      "prompt routing",
      "limitations",
      "english",
      "optimization",
      "data"
    ]
  },
  {
    "id": 150,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Prompt router criteria and fallback model",
    "content": "In intelligent prompt routing, routing criteria are used to define the conditions under which requests are routed to specific models. For example, the criteria can involve a `responseQualityDifference` threshold. If the chosen model does not meet these criteria, a designated fallback model is then used to process the request. This mechanism ensures that even if the primary routing conditions are not met, a response can still be generated, maintaining application stability.",
    "tags": [
      "prompt routing",
      "criteria",
      "fallback",
      "response quality",
      "model selection"
    ]
  },
  {
    "id": 151,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "How intelligent prompt routing works",
    "content": "Intelligent prompt routing operates through a multi-step process. First, model selection involves choosing the family of models for the application. Next, for each incoming request, the system performs request analysis to understand the prompt's content and context. This leads to response quality prediction for each specified model, taking into account routing criteria. Based on this prediction, Amazon Bedrock dynamically selects and forwards the request to the most suitable model. Finally, the response handling phase retrieves and returns the output from the chosen model to the user, including information about the model used.",
    "tags": [
      "prompt routing",
      "workflow",
      "model selection",
      "analysis",
      "prediction"
    ]
  },
  {
    "id": 152,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "How to use intelligent prompt routing",
    "content": "To use intelligent prompt routing, you can leverage the Amazon Bedrock console, the AWS Command Line Interface (CLI), or the AWS SDK. In the console, you navigate to the Prompt Routers hub to choose a model family or configure your own router. Via the CLI, you can use commands like `create-prompt-router` to define models and routing criteria. After configuration, you send your prompts to the intelligent prompt router, which then dynamically routes the request to the best-suited model. Regularly reviewing performance is recommended for optimal usage.",
    "tags": [
      "prompt routing",
      "usage",
      "console",
      "cli",
      "sdk"
    ]
  },
  {
    "id": 153,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Design a prompt",
    "content": "Designing a prompt effectively is crucial for building successful applications with Amazon Bedrock models. This involves creating prompts that are consistent, clear, and concise, as LLMs may generate undesirable responses if instructions or formatting are ambiguous. Key aspects of good design include providing simple instructions, placing questions at the end, using separator characters, and employing output indicators. Additionally, controlling the model's response can be achieved by adjusting inference parameters, which influence aspects like randomness and length.",
    "tags": [
      "prompt design",
      "best practices",
      "clarity",
      "conciseness",
      "inference parameters"
    ]
  },
  {
    "id": 154,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Provide simple, clear, and complete instructions",
    "content": "For optimal performance, Large Language Models (LLMs) on Amazon Bedrock respond best to simple, clear, and complete instructions. By meticulously describing the expected outcome of a task and actively reducing ambiguity in the prompt, users can ensure the model accurately interprets the given instructions. This clarity leads to outputs that precisely match user expectations, as demonstrated by examples where explicit choices lead to better classification results compared to vague prompts.",
    "tags": [
      "instructions",
      "clarity",
      "simplicity",
      "ambiguity",
      "prompt design"
    ]
  },
  {
    "id": 155,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Place the question or instruction at the end of the prompt for best results",
    "content": "For optimal results, it is recommended to place the question or instruction at the end of the prompt. This strategic positioning aids the Large Language Model (LLM) in determining precisely which information it needs to find or act upon within the provided context. In tasks like classification, having the answer choices appear at the end also contributes to better performance and clarity for the model. This practice helps the model stay focused on the specific task, improving accuracy and relevance of the generated response.",
    "tags": [
      "question",
      "instruction",
      "placement",
      "prompt design",
      "best practices"
    ]
  },
  {
    "id": 156,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Use separator characters for API calls",
    "content": "The use of separator characters, such as newline characters (`\\n`), is crucial for effectively formatting API calls and can significantly impact the performance of Large Language Models (LLMs). For Anthropic Claude models, it is necessary to include specific formatting like `\\n\\nHuman: {{Query Content}}\\n\\nAssistant:` to obtain desired responses. With Titan models, adding `\\n` at the end of a prompt can enhance model performance, and for classification tasks, separating answer options by `\\n` is also beneficial. Proper adherence to these formatting guidelines improves the model's interpretation and output.",
    "tags": [
      "separators",
      "api calls",
      "formatting",
      "newline",
      "prompt design"
    ]
  },
  {
    "id": 157,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Use output indicators",
    "content": "Incorporating output indicators into prompts is a valuable strategy to guide Large Language Models (LLMs) to produce responses that adhere to specific constraints. These indicators involve adding details about the desired format, length, or other characteristics of the output. For example, specifying a short phrase or a particular format can help the model generate a more precise and targeted response, as seen in examples where clear indicators lead to exact answers. This helps ensure the model's output is directly applicable to the user's needs.",
    "tags": [
      "output indicators",
      "constraints",
      "formatting",
      "length",
      "prompt design"
    ]
  },
  {
    "id": 158,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Best practices for good generalization",
    "content": "While not explicitly detailed under a dedicated \"best practices for good generalization\" section, several prompt design principles contribute to a model's ability to generalize effectively. These include providing simple, clear, and complete instructions to reduce ambiguity, strategically placing the question or instruction at the end of the prompt for better focus, and using output indicators to guide the desired response format. Additionally, techniques like few-shot prompting, which provide examples to the model, and optimizing prompts for text models by breaking down complex tasks into steps or refining instructions, all enhance the model's ability to generate relevant and accurate responses across various inputs.",
    "tags": [
      "generalization",
      "best practices",
      "prompt design",
      "few-shot",
      "optimization"
    ]
  },
  {
    "id": 159,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Optimize prompts for text models on Amazon Bedrock—when the basics aren't good enough",
    "content": "When basic prompting techniques are insufficient, more advanced strategies are needed to optimize prompts for text models on Amazon Bedrock. For complex tasks, it is effective to instruct the model to \"Think step-by-step to come up with the right answer,\" allowing it to break down the problem and improve accuracy. Few-shot prompting, which involves providing input-response examples, can also significantly enhance model responses, especially for challenging tasks. Furthermore, task instruction refinement through prompt modifiers like domain/input specification, task specification, label description, output specification, or even LLM encouragement, can fine-tune the model's behavior for better results.",
    "tags": [
      "prompt optimization",
      "advanced prompting",
      "few-shot",
      "complex tasks",
      "refinement"
    ]
  },
  {
    "id": 160,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Control the model response with inference parameters",
    "content": "You can control the model response by adjusting various inference parameters available for Large Language Models (LLMs) on Amazon Bedrock. Temperature (a value between 0 and 1) regulates the creativity and randomness of responses, with lower values yielding more deterministic results. Maximum generation length (or maximum new tokens) limits the number of tokens the LLM generates. Top-p controls the diversity of token choices by considering only the most probable options. Additionally, an End token or end sequence can be specified to stop generation upon encountering certain characters. Some models may also have specific parameters like Anthropic Claude's Top-k or AI21 Labs Jurassic's presence, count, and frequency penalties.",
    "tags": [
      "inference parameters",
      "model control",
      "temperature",
      "max tokens",
      "top-p"
    ]
  },
  {
    "id": 161,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Prompt templates and examples for Amazon Bedrock text models",
    "content": "Amazon Bedrock offers a collection of prompt templates and examples specifically designed for its text models to facilitate the creation of effective prompts. These resources cover a range of common tasks including text classification, summarization, and question-answering, both with and without context. By utilizing these predefined templates, users can more easily structure their prompts and fill in data-specific information to guide LLMs towards desired outputs, saving time and improving consistency.",
    "tags": [
      "prompt templates",
      "examples",
      "text models",
      "use cases",
      "bedrock"
    ]
  },
  {
    "id": 162,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Text classification",
    "content": "Text classification is a common task where the prompt presents a question along with several possible answer choices, and the model's objective is to respond with the correct category. For instance, this can involve a straightforward multiple-choice question or sentiment analysis, where the model classifies the sentiment of a text as positive, negative, or neutral from a given list of options. Including answer choices directly in the prompt is recommended, as it helps Large Language Models (LLMs) on Amazon Bedrock produce more accurate responses.",
    "tags": [
      "text classification",
      "sentiment analysis",
      "categories",
      "prompting",
      "llm tasks"
    ]
  },
  {
    "id": 163,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Question-answer, without context",
    "content": "In a question-answer, without context task, the Large Language Model (LLM) is required to answer a question solely based on its internal knowledge, without any explicit context or external document provided within the prompt. The prompt typically consists only of the question itself, possibly with model encouragement or constraints. This task assesses the model's ability to retrieve and synthesize information from its training data to generate a relevant response.",
    "tags": [
      "question-answer",
      "no context",
      "internal knowledge",
      "prompting",
      "llm tasks"
    ]
  },
  {
    "id": 164,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Question-answer, with context",
    "content": "In a question-answer, with context task, the user provides an input text followed by a specific question. The Large Language Model (LLM) is then instructed to answer the question solely based on the information contained within that provided text. Placing the question at the end of the prompt after the context is often beneficial for LLMs on Amazon Bedrock to better focus on the task. Model encouragement and wrapping input text in XML tags (for Anthropic Claude) can further enhance the accuracy of the generated responses.",
    "tags": [
      "question-answer",
      "with context",
      "information extraction",
      "prompting",
      "llm tasks"
    ]
  },
  {
    "id": 165,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Summarization",
    "content": "Summarization is a task where the input provided to the Large Language Model (LLM) is a passage of text, and the model's objective is to generate a shorter, concise response that accurately captures the main points of the original input. This can be applied to various text types, such as restaurant reviews, where the user specifies the desired length of the summary. Specifying detailed task requirements, including the length of the summary, can significantly improve the quality of the generated output.",
    "tags": [
      "summarization",
      "text generation",
      "main points",
      "concise",
      "llm tasks"
    ]
  },
  {
    "id": 166,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Text generation",
    "content": "Text generation is a versatile task where, given a prompt, the Large Language Model (LLM) is expected to respond with a passage of original text that adheres to the provided description. This capability extends to various forms, including creative writing such as stories, poems, or movie scripts, as well as more structured outputs like formatted emails. Specifying detailed task requirements and desired output characteristics, such as tone or inclusion of punctuation, can effectively guide the model to produce tailored and high-quality generated content.",
    "tags": [
      "text generation",
      "creative writing",
      "original content",
      "prompting",
      "llm tasks"
    ]
  },
  {
    "id": 167,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Code generation",
    "content": "Code generation is a task where the Large Language Model (LLM) is required to produce programming code based on user specifications provided in the prompt. This can range from generating Python code to quickly approximate a square root to more complex tasks like text-to-SQL conversions. Prompts for code generation typically specify the programming language and the task or purpose of the function to be created.",
    "tags": [
      "code generation",
      "programming",
      "python",
      "sql",
      "llm tasks"
    ]
  },
  {
    "id": 168,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Mathematics",
    "content": "The Mathematics task involves providing the Large Language Model (LLM) with an input that describes a problem requiring mathematical reasoning at some level. This reasoning can be numerical, logical, geometric, or otherwise, challenging the model to apply its understanding of mathematical principles. For complex problems, instructing the model to \"Think step-by-step\" can significantly improve its ability to arrive at the correct answer by breaking down the problem into smaller, manageable parts.",
    "tags": [
      "mathematics",
      "reasoning",
      "numerical",
      "logical",
      "problem-solving"
    ]
  },
  {
    "id": 169,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Reasoning/logical thinking",
    "content": "Reasoning or logical thinking is a task where the Large Language Model (LLM) is required to make a series of logical deductions and often explain its answers. This is particularly useful for complex reasoning tasks or problems that benefit from a structured thought process. Prompting the model with instructions like \"Think Step by Step and walk me through your thinking\" can lead to a detailed step-by-step analysis of how an answer was derived, improving both accuracy and transparency. This capability is crucial for verifying the model's output and fact-checking.",
    "tags": [
      "reasoning",
      "logical thinking",
      "deductions",
      "problem-solving",
      "explanation"
    ]
  },
  {
    "id": 170,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Entity extraction",
    "content": "Entity extraction is a task focused on identifying and extracting specific entities from a given text or input based on a provided input question. For instance, a prompt might instruct the model to extract \"name\" and \"location\" from a query. The extracted entities are often formatted and placed within XML tags, such as `<name></name>`, to facilitate further processing or structured data handling. This process helps in structuring unstructured text data into meaningful information.",
    "tags": [
      "entity extraction",
      "information retrieval",
      "entities",
      "xml tags",
      "llm tasks"
    ]
  },
  {
    "id": 171,
    "service": "Amazon Bedrock",
    "category": "Prompt engineering concepts",
    "title": "Chain-of-thought reasoning",
    "content": "Chain-of-thought reasoning (CoT) is a technique where the Large Language Model (LLM) provides a step-by-step analysis of how it arrived at a particular answer. This process is crucial for enhancing model reasoning, improving accuracy, and allowing users to fact-check and validate the model's output. By breaking down complex tasks into smaller, simpler steps and showing its internal thinking, the model can often achieve better performance, particularly for multi-step analysis or mathematical problems.",
    "tags": [
      "chain-of-thought",
      "reasoning",
      "step-by-step",
      "accuracy",
      "validation"
    ]
  },
  {
    "id": 172,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Prompt management: Store reusable prompts",
    "content": "Amazon Bedrock's Prompt management feature allows you to create and save your own prompts for reuse across different workflows, saving significant time. When creating a prompt, you can specify a model for inference and adjust inference parameters. It supports incorporating variables into prompts for dynamic adjustments based on use cases. This feature enables testing different prompt variants, comparing outputs, and saving versions, which can then be integrated into applications through Amazon Bedrock Flows or direct model invocation.",
    "tags": [
      "prompt management",
      "reusable prompts",
      "variables",
      "model invocation"
    ]
  },
  {
    "id": 173,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Key definitions",
    "content": "Prompt management uses several key definitions. A Prompt is the input provided to a model to guide its response. A Variable is a placeholder within a prompt, represented by double curly braces (e.g., {{variable}}), allowing for flexible content insertion at runtime. A Prompt variant refers to an alternative configuration of a prompt, including its message or the model and inference settings. The Prompt builder is a visual interface in the Amazon Bedrock console designed for creating, editing, and testing prompts and their variations.",
    "tags": [
      "prompt",
      "variable",
      "prompt variant",
      "prompt builder"
    ]
  },
  {
    "id": 174,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Supported Regions and models",
    "content": "Prompt management is supported in several AWS Regions, including US East (N. Virginia), US West (Oregon), Asia Pacific (Mumbai, Sydney, Tokyo, Seoul, Singapore), Canada (Central), Europe (Frankfurt, Ireland, London, Paris, Stockholm), and South America (São Paulo). It is compatible with any text model that supports the Converse API. However, the InvokeModel and InvokeModelWithResponseStream operations for Prompt management only work with prompts configured for Anthropic Claude or Meta Llama models.",
    "tags": [
      "regions",
      "supported models",
      "converse api",
      "invokemodel"
    ]
  },
  {
    "id": 175,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Prerequisites",
    "content": "To use Prompt management, your IAM role must have the necessary permissions. This involves either attaching the AmazonBedrockFullAccess AWS managed policy or creating a more restricted identity-based policy that grants specific Prompt management API actions, such as `CreatePrompt`, `UpdatePrompt`, `GetPrompt`, and `InvokeModel`. If you plan to encrypt your prompts with a customer managed key, you must also configure appropriate KMS key policies and attach a policy to the Prompt management role that allows key generation and decryption.",
    "tags": [
      "iam permissions",
      "kms key",
      "access control",
      "security"
    ]
  },
  {
    "id": 176,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Create a prompt",
    "content": "To create a prompt using Prompt management, you can either use the AWS Management Console or the Agents for Amazon Bedrock build-time API. In the console, you provide a name, an optional description, and can choose to encrypt it with a customer managed key. You then use the Prompt pane to construct the message, including variables in double curly braces (e.g., {{variable}}). You can also select a Generative AI resource (model, inference profile, or agent) and configure inference parameters like `maxTokens`, `temperature`, and `topP`.",
    "tags": [
      "create prompt",
      "prompt builder",
      "variables",
      "inference parameters"
    ]
  },
  {
    "id": 177,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "View information about prompts",
    "content": "You can view information about prompts through the AWS Management Console or by sending a `GetPrompt` or `ListPrompts` request via the Agents for Amazon Bedrock build-time API. In the console, navigating to Prompt management and selecting a prompt displays its details, including an Overview of general information, the Prompt draft with the latest configurations, and a list of Prompt versions. The API allows you to specify a `promptIdentifier` (ARN or ID) and optionally a `promptVersion` (DRAFT or version number) to retrieve specific prompt information.",
    "tags": [
      "view prompt",
      "prompt details",
      "getprompt",
      "listprompts"
    ]
  },
  {
    "id": 178,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Modify a prompt",
    "content": "To modify a prompt in Prompt management, you can use the AWS Management Console or the Agents for Amazon Bedrock build-time API with an `UpdatePrompt` request. In the console, you can edit the prompt's Name or Description in the Overview section. For content and configuration changes, you access the Prompt builder via 'Edit in prompt builder'. When using the `UpdatePrompt` API, you must include all fields you wish to maintain, as well as those you intend to change.",
    "tags": [
      "modify prompt",
      "updateprompt",
      "edit prompt",
      "prompt configuration"
    ]
  },
  {
    "id": 179,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Test a prompt",
    "content": "You can test a prompt from Prompt management in several ways. In the console, you can use the Prompt builder to enter Test values for variables and run the prompt. Prompts can also be tested within an Amazon Bedrock Flow by including a prompt node. For API testing, you can directly run inference using `InvokeModel`, `InvokeModelWithResponseStream`, `Converse`, or `ConverseStream` operations, specifying the prompt's ARN as the `modelId`. When using `Converse` or `ConverseStream` with a Prompt management prompt, certain fields like `additionalModelRequestFields` or `toolConfig` are restricted.",
    "tags": [
      "test prompt",
      "prompt builder",
      "invoke model",
      "converse api"
    ]
  },
  {
    "id": 180,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Optimize a prompt",
    "content": "Prompt optimization in Amazon Bedrock is a tool designed to rewrite prompts to yield more suitable inference results for specific use cases. When you submit a prompt for optimization, Amazon Bedrock analyzes its components and, if successful, generates a revised, optimized prompt. This feature helps improve the quality of model responses and can be particularly beneficial for non-English prompts, though best results are noted for English. You can then copy and use the text of the optimized prompt in your workflows.",
    "tags": [
      "prompt optimization",
      "rewrite prompt",
      "inference results",
      "llm tuning"
    ]
  },
  {
    "id": 181,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Supported Regions and models (optimization)",
    "content": "Prompt optimization is supported in various AWS Regions, including US East (N. Virginia), US West (Oregon), Asia Pacific (Mumbai, Sydney), Canada (Central), and Europe (Frankfurt, Ireland, London, Paris, São Paulo). It supports a wide range of foundation models from different providers. These include Amazon Nova models (Lite, Micro, Premier, Pro), Amazon Titan Text G1 - Premier, various Anthropic Claude 3 and 3.5 models (Haiku, Opus, Sonnet), DeepSeek DeepSeek-R1, several Meta Llama 3 and 4 Instruct models, and Mistral AI models (Mistral Large).",
    "tags": [
      "regions",
      "supported models",
      "prompt optimization",
      "claude",
      "llama"
    ]
  },
  {
    "id": 182,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Submit a prompt for optimization",
    "content": "To submit a prompt for optimization, you can use either the AWS Management Console or the Agents for Amazon Bedrock runtime API. In the console, after writing a prompt in a playground or Prompt management, you select the wand icon ( ) or Optimize button. Amazon Bedrock then analyzes and optimizes the prompt, displaying the original and optimized versions side-by-side for comparison. Via API, you send an `OptimizePrompt` request, providing the prompt input and specifying the `targetModelId`. The API response streams events, including an `analyzePromptEvent` and an `optimizedPromptEvent` containing the rewritten prompt.",
    "tags": [
      "optimize prompt",
      "playground",
      "api",
      "prompt analysis"
    ]
  },
  {
    "id": 183,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Deploy to your application using versions",
    "content": "When working with Prompt management, saving your prompt initially creates a draft version. To deploy a prompt to production, you create a version of it, which acts as a snapshot of your prompt's configurations at a specific point in time. This versioning allows for easy switching between different prompt configurations in your application and ensures consistent behavior. You can iterate on the draft version by modifying and saving it, then create new versions when satisfied, enabling controlled updates to your generative AI applications.",
    "tags": [
      "prompt versions",
      "deployment",
      "draft version",
      "application integration"
    ]
  },
  {
    "id": 184,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Create a version",
    "content": "To create a version of your prompt in Prompt management, you can use the AWS Management Console or send a `CreatePromptVersion` request via the Agents for Amazon Bedrock build-time API. In the console, within the prompt builder or the Prompt versions section of a selected prompt, you can choose 'Create version' to take a snapshot of your current draft version. API calls require specifying the `promptIdentifier` (ARN or ID) of the prompt. Versions are created incrementally, starting from 1, and the response provides the ID and ARN for the newly created version.",
    "tags": [
      "create version",
      "prompt versioning",
      "api call",
      "console"
    ]
  },
  {
    "id": 185,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "View information about versions",
    "content": "You can view information about versions of your prompt using the AWS Management Console or the Agents for Amazon Bedrock build-time API. In the console, after selecting a prompt, you can navigate to the Prompt versions section and choose a specific version to see its details, message, and configurations. Through the API, a `GetPrompt` request can be used by specifying the `promptIdentifier` (ARN or ID) and the `promptVersion` number to retrieve specific version details. The `ListPrompts` API can also be used to view information about all available prompt versions.",
    "tags": [
      "view versions",
      "prompt details",
      "getprompt",
      "listprompts"
    ]
  },
  {
    "id": 186,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Compare versions",
    "content": "The Amazon Bedrock console provides a tool to compare versions of a prompt created in Prompt management. This feature allows for a side-by-side comparison of the JSON objects defining each prompt version. Differences are clearly highlighted: fields existing in one but not the other are marked with a plus (+) symbol and green, while fields not existing in one but present in the other are marked with a minus (-) symbol and red. You can also fill in Test variables and run the prompts to compare their output model responses.",
    "tags": [
      "compare versions",
      "json comparison",
      "console tool",
      "prompt variants"
    ]
  },
  {
    "id": 187,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Delete a version",
    "content": "To delete a version of your prompt in Prompt management, you can utilize either the AWS Management Console or the Agents for Amazon Bedrock build-time API. In the console, navigate to the Prompt versions section of your selected prompt, choose the specific version, and then select 'Delete'. For API users, the `DeletePrompt` request allows you to specify both the `promptIdentifier` (ARN or ID) and the `promptVersion` number to remove a particular version. Confirming the deletion is usually required, especially in the console, to prevent accidental removal.",
    "tags": [
      "delete version",
      "prompt versioning",
      "remove version",
      "deleteprompt"
    ]
  },
  {
    "id": 188,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Delete a prompt",
    "content": "To delete a prompt entirely from Prompt management, you can use the AWS Management Console or the Agents for Amazon Bedrock build-time API. In the console, from the Prompt management page, simply select the prompt you wish to remove and choose 'Delete'. A warning will appear, requiring you to type 'confirm' to proceed with the deletion. Programmatically, the `DeletePrompt` API call requires specifying the `promptIdentifier` (ARN or ID) of the prompt. Deleting a prompt will remove all its associated draft and versioned configurations.",
    "tags": [
      "delete prompt",
      "remove prompt",
      "api call",
      "console"
    ]
  },
  {
    "id": 189,
    "service": "Amazon Bedrock",
    "category": "Prompt management",
    "title": "Run code samples",
    "content": "Amazon Bedrock provides code samples to demonstrate the use of Prompt management with the AWS SDK for Python (Boto3). These samples illustrate how to create a prompt with variables using `create_prompt`, list prompts with `list_prompts`, and get detailed information about a prompt using `get_prompt`. Further examples cover creating a version of a prompt with `create_prompt_version`, viewing versions, and even integrating a prompt into a flow. Finally, code snippets are provided for deleting specific prompt versions and the entire prompt resource.",
    "tags": [
      "code samples",
      "python sdk",
      "boto3",
      "api examples"
    ]
  },
  {
    "id": 190,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Guardrails: Detect and filter harmful content",
    "content": "Amazon Bedrock Guardrails provides configurable safeguards for generative AI applications, aligning with use cases and responsible AI policies. It helps filter harmful user inputs and toxic model responses, block undesirable topics, and redact sensitive information like PII. Guardrails can be applied across multiple foundation models (FMs), agents, and knowledge bases, offering consistent safety and privacy controls. Key safeguards include Content filters, Denied topics, Word filters, Sensitive information filters, and Contextual grounding checks. Guardrails evaluate prompts and FM completions against defined policies, intervening if harmful content is detected.",
    "tags": [
      "guardrails",
      "safety",
      "content moderation",
      "ai policies"
    ]
  },
  {
    "id": 191,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "How charges are calculated for using Amazon Bedrock Guardrails",
    "content": "Charges for Amazon Bedrock Guardrails are incurred only for the policies configured within the guardrail. The price varies per policy type. If a guardrail blocks an input prompt, only the guardrail evaluation is charged, not the foundation model inference. If a guardrail blocks a model response, charges apply for both the input prompt evaluation, the model response evaluation, and the foundation model inference that occurred before the block. When a guardrail does not block content, both guardrail evaluations (input and output) and foundation model inference are charged.",
    "tags": [
      "pricing",
      "costs",
      "charges",
      "billing"
    ]
  },
  {
    "id": 192,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Regions and models",
    "content": "Amazon Bedrock Guardrails is supported in a wide range of AWS Regions, including US East (N. Virginia), US West (Oregon), Europe (Frankfurt), and Asia Pacific (Tokyo). It supports various foundation models from providers like AI21 Labs (Jamba 1.5 Large/Mini), Amazon Nova (Lite), Anthropic (Claude 3 Haiku/Sonnet/Opus), Cohere (Command R/R+), Meta (Llama 3/3.1/3.2/3.3), Mistral AI (Mistral 7B/Large/Small, Mixtral 8x7B), OpenAI (gpt-oss-120b/20b), TwelveLabs (Pegasus v1.2), and Writer (Palmyra X4/X5). Notably, model reasoning with Anthropic Claude 3.7 Sonnet or DeepSeek-R1 is not supported by Guardrails.",
    "tags": [
      "regions",
      "models",
      "compatibility",
      "support"
    ]
  },
  {
    "id": 193,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Safeguard tiers for guardrails policies",
    "content": "Amazon Bedrock Guardrails offers safeguard tiers for specific policies, providing distinct performance characteristics and language support to meet diverse application needs. These tiers allow users to control when to adopt new capabilities or maintain consistency with existing guardrail setups. Currently, content filters (text) and prompt attacks, as well as denied topics, support safeguard tiers. Choosing an appropriate tier is crucial for optimizing guardrail effectiveness based on specific use cases.",
    "tags": [
      "safeguard tiers",
      "policies",
      "performance",
      "language support"
    ]
  },
  {
    "id": 194,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Available safeguard tiers",
    "content": "Amazon Bedrock Guardrails provides two main safeguard tiers: Standard tier and Classic tier. The Standard tier offers more robust performance and comprehensive language support, with enhanced protection against prompt attacks. Guardrails utilizing the Standard tier also leverage cross-Region inference for improved reliability. In contrast, the Classic tier provides established guardrails functionality, supporting English, French, and Spanish languages, and serves as a foundational option.",
    "tags": [
      "standard tier",
      "classic tier",
      "guardrails tiers"
    ]
  },
  {
    "id": 195,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Key differences between safeguard tiers",
    "content": "The Standard tier for Amazon Bedrock Guardrails offers more robust performance for content filters and prompt attacks compared to the Classic tier. Denied topics in the Standard tier can have definitions up to 1,000 characters, while the Classic tier is limited to 200 characters. The Standard tier provides extensive language support and cross-Region inference, whereas the Classic tier supports only English, French, and Spanish, and does not support cross-Region inference. These differences help in choosing the most suitable tier for specific application requirements.",
    "tags": [
      "tier comparison",
      "standard vs classic",
      "features"
    ]
  },
  {
    "id": 196,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Choosing a safeguard tier for your use case",
    "content": "The choice of safeguard tier for Amazon Bedrock Guardrails depends on specific application requirements. The Standard tier is recommended for applications handling multiple languages or those requiring higher accuracy and performance for content filters, prompt attacks, and denied topics. Conversely, the Classic tier may be suitable for applications primarily using English, French, or Spanish content, or for those needing time before migrating from an existing guardrails implementation. Evaluating these factors helps in selecting the optimal tier for a given use case.",
    "tags": [
      "use cases",
      "tier selection",
      "application requirements"
    ]
  },
  {
    "id": 197,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Migrating your guardrail to Standard tier",
    "content": "To migrate an existing guardrail to the Standard tier in Amazon Bedrock Guardrails, you must first modify your guardrail configuration to enable Standard tier and cross-Region inference. It is highly recommended to roll out the updated guardrail using a phased approach, starting with noncritical workloads. This systematic migration helps ensure stability and allows for testing before broader deployment, minimizing potential disruptions.",
    "tags": [
      "migration",
      "standard tier",
      "guardrail update"
    ]
  },
  {
    "id": 198,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Regions for safeguard tiers",
    "content": "Safeguard tiers for Amazon Bedrock Guardrails are supported in several AWS Regions where Guardrails is available, including US East (N. Virginia), US West (Oregon), US East (Ohio), and Europe (Paris). Other supported regions include Europe (Ireland, Spain, Milan, Stockholm, Frankfurt), Asia Pacific (Tokyo, Sydney, Singapore, Mumbai, Seoul), and AWS GovCloud (US-West). This broad regional support allows for flexible deployment of guardrails with chosen safeguard tiers.",
    "tags": [
      "regions",
      "safeguard tiers",
      "availability"
    ]
  },
  {
    "id": 199,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported languages",
    "content": "Amazon Bedrock Guardrails supports a variety of languages, though effectiveness can vary. It's strongly recommended to test all intended languages for your guardrails, as unsupported languages will render guardrails ineffective. Language support is categorized into 'Optimized and supported,' where underlying models are tuned and tested for the language, and 'Supported,' where models are tested but not tuned. This distinction helps in understanding the expected performance across different linguistic contexts.",
    "tags": [
      "languages",
      "multilingual",
      "support level"
    ]
  },
  {
    "id": 200,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Content filters and prompt attacks language support",
    "content": "Language support for text-based content filters and prompt attacks within Amazon Bedrock Guardrails varies by safeguard tier. The Standard tier offers extensive language support, with Arabic and English being 'Optimized and supported,' and many other languages like Afrikaans, Albanian, Armenian, and Bulgarian listed as 'Supported'. In contrast, the Classic tier primarily supports English, French, and Spanish, all of which are 'Optimized and supported'. This distinction highlights the enhanced linguistic capabilities of the Standard tier.",
    "tags": [
      "content filters",
      "prompt attacks",
      "language support"
    ]
  },
  {
    "id": 201,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Denied topics language support",
    "content": "Language support for denied topics in Amazon Bedrock Guardrails also varies based on the chosen safeguard tier. The Standard tier provides extensive language support, with Arabic, English, and Spanish designated as 'Optimized and supported'. Many other languages, such as Afrikaans and Albanian, are listed as 'Supported' in the Standard tier. The Classic tier offers 'Optimized and supported' coverage for English, French, and Spanish.",
    "tags": [
      "denied topics",
      "language support"
    ]
  },
  {
    "id": 202,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Word filters language support",
    "content": "The provided sources do not explicitly detail the specific language support levels for Word filters within Amazon Bedrock Guardrails. Generally, Guardrails supports a variety of languages, and it's strongly recommended to test all intended languages for your use case to ensure effectiveness. While profanity filters are based on conventional definitions, the precise linguistic scope for custom word filters is not separately specified.",
    "tags": [
      "word filters",
      "language support"
    ]
  },
  {
    "id": 203,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Sensitive information filters language support",
    "content": "The provided sources do not explicitly detail the specific language support levels for Sensitive information filters within Amazon Bedrock Guardrails. Amazon Bedrock Guardrails is designed to support a variety of languages, but it is strongly advised to test the intended languages for your specific use case. These filters detect PII in standard formats and custom regex entities, but their linguistic coverage is not specifically itemized.",
    "tags": [
      "sensitive info filters",
      "pii",
      "language support"
    ]
  },
  {
    "id": 204,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Contextual grounding checks language support",
    "content": "Language support for Contextual grounding checks in Amazon Bedrock Guardrails is specified as 'Optimized and supported' for English . These checks are designed to detect and filter hallucinations in model responses by evaluating if content is factually accurate and relevant to a provided source and user query. While Automated Reasoning checks, a related feature, are generally available in certain US and EU regions and support English (US) only, the specific language for contextual grounding is clearly English. It's important to note the reliance on English for optimal performance .",
    "tags": [
      "contextual grounding",
      "hallucinations",
      "language support"
    ]
  },
  {
    "id": 205,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Prerequisites for Amazon Bedrock Guardrails",
    "content": "Before using Amazon Bedrock Guardrails, you must request access to the foundation models you intend to use and ensure your IAM role has the necessary permissions. It is recommended to prepare your content filters by determining the desired strength for categories like Hate or Violence. Additionally, define specific denied topics precisely, compile word lists for custom filtering (up to 10,000 items, 50 KB), and identify sensitive information (PII or regex patterns) to block or mask. Finally, develop clear messages for blocked content to enhance user experience.",
    "tags": [
      "prerequisites",
      "setup",
      "permissions",
      "filters",
      "topics"
    ]
  },
  {
    "id": 206,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Permissions to Create and Manage Guardrails",
    "content": "To create and manage Amazon Bedrock Guardrails, an IAM policy must include specific actions. These actions, such as bedrock:CreateGuardrail, CreateGuardrailVersion, DeleteGuardrail, GetGuardrail, ListGuardrails, and UpdateGuardrail, grant the necessary control. The policy should be attached to the relevant IAM role, allowing it to perform guardrail-related operations across all resources.",
    "tags": [
      "permissions",
      "iam",
      "create",
      "manage",
      "guardrails"
    ]
  },
  {
    "id": 207,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Permissions for Invoking Guardrails to Filter Content",
    "content": "To invoke a guardrail during model inference, the IAM policy needs permissions for bedrock:InvokeModel and bedrock:InvokeModelWithResponseStream on foundation models. Additionally, the policy must allow bedrock:ApplyGuardrail for the specific guardrail's ARN. These permissions ensure that the guardrail can evaluate and filter content in both prompts and model responses.",
    "tags": [
      "permissions",
      "invoke",
      "filter",
      "inference",
      "applyguardrail"
    ]
  },
  {
    "id": 208,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Permissions for Automated Reasoning Policies",
    "content": "For Automated Reasoning policies within Amazon Bedrock Guardrails, a specific IAM policy action is required. This policy must grant bedrock:InvokeAutomatedReasoningPolicy on the ARN of the particular Automated Reasoning policy and version. This permission enables the system to mathematically verify natural language content against defined policies.",
    "tags": [
      "permissions",
      "automated reasoning",
      "invoke",
      "policies"
    ]
  },
  {
    "id": 209,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Optional Permissions for Encrypting a Guardrail",
    "content": "For enhanced security, guardrails can be encrypted using customer managed AWS KMS keys. Permissions are required for both guardrail creators and users. Creators need `kms:Decrypt`, `kms:GenerateDataKey`, `kms:DescribeKey`, and `kms:CreateGrant` on the KMS key. Users only need `kms:Decrypt`. These permissions are typically configured through a resource-based key policy on the KMS key and identity-based policies for the IAM roles.",
    "tags": [
      "permissions",
      "encryption",
      "kms",
      "security",
      "iam"
    ]
  },
  {
    "id": 210,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Enforcing Specific Guardrails During Inference",
    "content": "You can enforce the use of a specific guardrail for model inference by including the bedrock:GuardrailIdentifier condition key in your IAM policy. This prevents API requests that do not specify the required guardrail, whether it's a numeric version or the DRAFT. The policy typically uses an `Allow` statement for the specified guardrail ARN and a `Deny` statement with `StringNotEquals` for all other guardrail identifiers. Limitations include no cross-account access for resource-based policies and the ability to bypass enforcement via input tags, though the guardrail still applies to the response.",
    "tags": [
      "permissions",
      "enforce",
      "inference",
      "guardrailidentifier",
      "iam"
    ]
  },
  {
    "id": 211,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Permissions for Using Cross-Region Inference",
    "content": "Cross-Region inference for Amazon Bedrock Guardrails requires specific IAM permissions. To create and manage guardrails utilizing this feature, the policy must grant `bedrock:CreateGuardrail`, `UpdateGuardrail`, `DeleteGuardrail`, `GetGuardrail`, and `ListGuardrails` on both guardrail and guardrail-profile ARNs. For invoking guardrails with cross-Region inference, `bedrock:ApplyGuardrail` must be allowed on the source Region guardrail and the destination Region guardrail profiles.",
    "tags": [
      "permissions",
      "cross-region",
      "inference",
      "iam",
      "guardrail-profile"
    ]
  },
  {
    "id": 212,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Configure Content Filters for Guardrails",
    "content": "Content filters in Amazon Bedrock Guardrails detect and filter harmful text or image content, including categories like Hate, Insults, Sexual, Violence, Misconduct, and Prompt Attack. You can configure filter strengths (NONE, LOW, MEDIUM, HIGH) independently for prompts and model responses. Prompt attack filters specifically require input tags for `InvokeModel` and `InvokeModelWithResponseStream` operations. Guardrails can be created through the console or API, allowing for the specification of details such as blocked messaging, cross-Region inference, KMS keys, and safeguard tiers.",
    "tags": [
      "content filters",
      "harmful content",
      "prompt attack",
      "filtering",
      "image filtering"
    ]
  },
  {
    "id": 213,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Add Denied Topics to Guardrails",
    "content": "Amazon Bedrock Guardrails allows you to define and block denied topics that are undesirable for your generative AI application, such as investment advice. Each topic requires a Name, a concise Definition (up to 200 characters), and optionally up to five Sample phrases. Definitions should be clear and avoid instructions or negative phrasing. When a denied topic is detected in natural language prompts or responses, the guardrail intervenes. You can add up to 30 denied topics and specify input/output actions and safeguard tiers.",
    "tags": [
      "denied topics",
      "topics",
      "blocking",
      "filtering",
      "policy"
    ]
  },
  {
    "id": 214,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Add Word Filters to Guardrails",
    "content": "Word filters in Amazon Bedrock Guardrails block specific words and phrases (exact matches) in user prompts and model responses. This feature includes a Profanity filter based on conventional definitions, and a Custom word filter where you can add up to 10,000 items, each up to three words long. Custom word lists can be manually entered, uploaded as .txt or .csv files, or from an S3 object via the console. You can configure specific input and output actions (BLOCK or NONE) for these filters.",
    "tags": [
      "word filters",
      "profanity",
      "custom words",
      "blocking",
      "phrases"
    ]
  },
  {
    "id": 215,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Add Sensitive Information Filters to Guardrails",
    "content": "Sensitive information filters in Amazon Bedrock Guardrails detect and handle Personally Identifiable Information (PII) in standard formats and custom regex patterns. Guardrails offer two modes of action: Block, which completely blocks content containing sensitive data, and Mask, which anonymizes the information by replacing it with identifier tags like {NAME}. Predefined PII types include ADDRESS, EMAIL, and PHONE. These filters can be configured for both input prompts and model responses through the console or API.",
    "tags": [
      "sensitive information",
      "pii",
      "regex",
      "masking",
      "blocking"
    ]
  },
  {
    "id": 216,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Add Contextual Grounding Checks to Guardrails",
    "content": "Contextual grounding checks in Amazon Bedrock Guardrails detect and filter hallucinations in model responses by comparing them against a provided grounding source and user query. This is crucial for RAG, summarization, and conversational agents. The system generates grounding and relevance confidence scores, allowing you to set configurable thresholds (0-0.99) to block ungrounded or irrelevant content. For Invoke APIs, specific XML input tags mark the grounding source and query, while Converse and ApplyGuardrail APIs use qualifiers within content blocks. Checks are performed on the model output, not the prompt.",
    "tags": [
      "grounding checks",
      "hallucinations",
      "rag",
      "relevance",
      "confidence scores"
    ]
  },
  {
    "id": 217,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Options for Handling Harmful Content",
    "content": "Amazon Bedrock Guardrails provides flexible options for handling detected harmful content in prompts and responses. You can choose to Block the content, replacing it with a configured message. For sensitive information filters, the Mask option anonymizes the content with identifier tags (e.g., {NAME}). Alternatively, the Detect mode takes no direct action but returns detailed detection information in the trace response, which is useful for evaluating guardrail performance without immediate intervention. Filter strengths can be fine-tuned to balance blocking and desired output.",
    "tags": [
      "harmful content",
      "block",
      "mask",
      "detect",
      "filter strength"
    ]
  },
  {
    "id": 218,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Add Automated Reasoning Checks in Guardrails",
    "content": "Automated Reasoning checks in Amazon Bedrock Guardrails mathematically verify natural language content against formal logic policies, ensuring high accuracy and compliance with complex rules. These checks complement other guardrail features but require content filters for prompt injection protection. They operate on source documents to extract variables and rules, which are then tested for validity. Automated Reasoning is currently supported in US and EU regions for English (US) content and does not support streaming APIs. Policies are added to guardrails using the `automatedReasoningConfig` field in `CreateGuardrail` or `UpdateGuardrail` API operations.",
    "tags": [
      "automated reasoning",
      "formal logic",
      "compliance",
      "accuracy",
      "policy"
    ]
  },
  {
    "id": 219,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Set Up Cross-Region Guardrail Inference",
    "content": "Cross-Region inference for Amazon Bedrock Guardrails automatically routes guardrail policy evaluation requests to the optimal AWS Region within your geographic boundary. This feature maximizes available compute resources and model availability, maintaining guardrail performance during high demand without additional cost. It is configured through a guardrail profile, a system-defined resource specified during guardrail creation or modification via the console or API. Specific IAM permissions are required to utilize cross-Region guardrail inference.",
    "tags": [
      "cross-region",
      "inference",
      "guardrail profile",
      "high availability",
      "performance"
    ]
  },
  {
    "id": 220,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Supported Regions for Cross-Region Guardrail Inference",
    "content": "Cross-Region inference in Amazon Bedrock Guardrails leverages guardrail profiles that support defined sets of source and destination Regions. Examples include US Guardrail v1:0, US-GOV Guardrail v1:0, EU Guardrail v1:0, and APAC Guardrail v1:0. The source Region is where the guardrail inference request originates, and the destination Regions are where Amazon Bedrock can route that request. Each profile has specific combinations of source and destination Regions to optimize traffic distribution for guardrail policy evaluations.",
    "tags": [
      "cross-region",
      "regions",
      "supported regions",
      "guardrail profile"
    ]
  },
  {
    "id": 221,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Test Your Guardrail",
    "content": "After creation, you can test your guardrail's working draft (DRAFT) or other versions to ensure configurations meet requirements. Testing can be done in the console's test window or Text playground. Via API, use `InvokeModel`, `InvokeModelWithResponseStream`, or `Converse` operations, specifying the guardrail ID and version in request headers. Enabling a trace (`X-Amzn-Bedrock-Trace: ENABLED`) provides detailed information in the response about what content was blocked and why, including assessments from various policies. For streaming responses, the trace appears in the final chunk.",
    "tags": [
      "testing",
      "draft",
      "versions",
      "trace",
      "api"
    ]
  },
  {
    "id": 222,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "View Information About Your Guardrails",
    "content": "You can view information about your guardrails through the AWS Management Console or the Amazon Bedrock API. In the console, the Guardrail overview displays general configurations, and the Working draft section shows details of the latest draft. Using the API, you can send a `GetGuardrail` request for a specific guardrail or version, or a `ListGuardrails` request to view information about multiple guardrails or all versions of a specific guardrail. Responses detail policy configurations, creation/update times, ARN, ID, name, status, and version.",
    "tags": [
      "view",
      "information",
      "console",
      "api",
      "status"
    ]
  },
  {
    "id": 223,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Modify Your Guardrail",
    "content": "Amazon Bedrock Guardrails can be edited after creation to iterate on filters and configurations. In the console, you can modify guardrail details, manage tags, and edit specific policy types within the 'Working Draft' section. For programmatic updates, the `UpdateGuardrail` API request allows you to modify existing guardrails. When using the API, you must include both the fields you wish to update and those you want to keep the same in the request body.",
    "tags": [
      "modify",
      "update",
      "edit",
      "console",
      "api"
    ]
  },
  {
    "id": 224,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Delete Your Guardrail",
    "content": "When a guardrail is no longer needed, it can be deleted. It is crucial to disassociate the guardrail from all dependent resources or applications before deletion to prevent errors. In the AWS Management Console, navigate to the Guardrails section, select the guardrail, and choose 'Delete,' confirming the action by typing 'delete.' Programmatically, you can use the `DeleteGuardrail` API request, specifying the guardrail's ID or ARN.",
    "tags": [
      "delete",
      "remove",
      "clean-up",
      "disassociate"
    ]
  },
  {
    "id": 225,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Create a Version of a Guardrail",
    "content": "To deploy a guardrail to an application, you create a version, which is a snapshot of the current working draft. This allows for controlled deployment and tracking of changes. In the console, navigate to the 'Versions' section of your guardrail and select 'Create,' optionally providing a description. Programmatically, use the `CreateGuardrailVersion` API request, specifying the guardrail's ID or ARN and an optional description. IAM policies apply to all versions of a guardrail, as versions themselves are not standalone resources.",
    "tags": [
      "deploy",
      "versions",
      "create version",
      "snapshot",
      "guardrail versions"
    ]
  },
  {
    "id": 226,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "View Information About Guardrail Versions",
    "content": "Information about guardrail versions can be viewed through the AWS Management Console or the Amazon Bedrock API. In the console, you can navigate to the 'Versions' section of a specific guardrail. Using the API, the `GetGuardrail` request can retrieve details for a specific version, while the `ListGuardrails` request, when specifying the `guardrailIdentifier`, can list all versions of a given guardrail. The API responses provide comprehensive details, including the specific policy configurations for that version.",
    "tags": [
      "versions",
      "view",
      "information",
      "console",
      "api"
    ]
  },
  {
    "id": 227,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Delete a Guardrail Version",
    "content": "Individual versions of a guardrail can be deleted when they are no longer needed. Similar to deleting the entire guardrail, it's essential to disassociate the specific version from any dependent resources or applications beforehand. In the AWS Management Console, navigate to the 'Versions' section of your guardrail, select the version you wish to delete, and choose 'Delete.' The API supports this via the `DeleteGuardrailVersion` request, which requires the guardrail's ID and the specific version number.",
    "tags": [
      "delete",
      "versions",
      "remove",
      "clean-up"
    ]
  },
  {
    "id": 228,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Guardrails Use with Inference Operations",
    "content": "Guardrails can be applied to prompts and responses during model inference using `InvokeModel`, `InvokeModelWithResponseStream`, `Converse`, and `ConverseStream` APIs. For `InvokeModel` operations, guardrail ID and version are passed in headers, while `Converse` uses a `guardrailConfig` field. Input tagging (using XML tags with a dynamic `tagSuffix`) allows selective evaluation of user input, critical for prompt attack filters. Streaming response behavior can be synchronous or asynchronous via `streamProcessingMode`, though asynchronous mode does not support masking. Guardrails can also assess system prompts in Converse API if `guardContent` is specified.",
    "tags": [
      "inference",
      "use cases",
      "input tagging",
      "streaming",
      "api"
    ]
  },
  {
    "id": 229,
    "service": "Amazon Bedrock",
    "category": "Guardrails",
    "title": "Use the ApplyGuardrail API in Your Application",
    "content": "The ApplyGuardrail API assesses any text using pre-configured Amazon Bedrock Guardrails without invoking foundation models, offering decoupled content validation and flexible deployment. It enables independent evaluation of user inputs (`source: INPUT`) and model outputs (`source: OUTPUT`) against denied topics, content filters, PII detectors, and word block lists. The response indicates if the guardrail `INTERVENED` (masking content or providing a canned message) or took `NONE` action, with detailed `assessments`. An `outputScope` of `FULL` or `trace` enabled `full` option for Invoke/Converse APIs can provide non-detected entries for debugging.",
    "tags": [
      "applyguardrail",
      "use cases",
      "content validation",
      "decoupled",
      "api"
    ]
  },
  {
    "id": 230,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Overview of Amazon Bedrock Evaluations",
    "content": "Amazon Bedrock evaluations enable assessing the performance and effectiveness of Amazon Bedrock models and knowledge bases, as well as external models and Retrieval Augmented Generation (RAG) sources. This includes computing performance metrics like semantic robustness for models and correctness for knowledge bases in retrieving information and generating responses. Evaluations can be automatic, leveraging Large Language Models (LLMs), or human-based, involving workers who rate and provide input. The service helps determine the best model for a use case by evaluating outputs with built-in or custom prompt datasets.",
    "tags": [
      "model evaluation",
      "rag",
      "performance",
      "llm",
      "human-based"
    ]
  },
  {
    "id": 231,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Supported Regions and Models for Evaluation",
    "content": "Model evaluation in Amazon Bedrock is supported across several AWS Regions, including US East (N. Virginia), US West (Oregon), Europe (Frankfurt), and Asia Pacific (Tokyo). It supports a wide range of foundation models from various providers like AI21 Labs, Amazon Nova, Amazon Titan, Anthropic Claude, Cohere, Meta Llama, and Mistral AI. This support extends to foundation models, Amazon Bedrock Marketplace models, customized foundation models, imported foundation models, prompt routers, and models with Provisioned Throughput. Specific regional support for features and models may vary, and users should refer to the documentation for details.",
    "tags": [
      "regions",
      "models",
      "foundation models",
      "custom models",
      "provisioned throughput"
    ]
  },
  {
    "id": 232,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Automatic Model Evaluation Jobs Overview",
    "content": "Automatic model evaluation jobs in Amazon Bedrock allow for quick assessment of a model's task performance. Users can utilize either custom prompt datasets, tailored to specific use cases, or available built-in datasets. These jobs produce computed scores and metrics to evaluate a model's effectiveness. The process involves defining a task type, such as text generation or classification, and specifying input/output data locations in Amazon S3.",
    "tags": [
      "automatic evaluation",
      "model performance",
      "datasets",
      "metrics"
    ]
  },
  {
    "id": 233,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Prerequisites for Automatic Model Evaluation",
    "content": "To create your first automatic model evaluation job, several prerequisites must be met, including having access to at least one Amazon Bedrock foundation model. IAM permissions are required for creating and managing these jobs, whether through the console, AWS CLI, or SDK. A service role is necessary for Amazon Bedrock to perform actions on your behalf. All input and output data must reside in an Amazon S3 bucket within the same AWS Region, and Cross Origin Resource Sharing (CORS) permissions must be enabled on S3 buckets for console-based jobs.",
    "tags": [
      "prerequisites",
      "iam",
      "s3",
      "cors",
      "service role"
    ]
  },
  {
    "id": 234,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Model Evaluation Task Types",
    "content": "In a model evaluation job, an evaluation task type defines the specific task a model should perform based on prompt information. Only one task type can be chosen per evaluation job. Available task types for automatic evaluations include General text generation, Text summarization, Question and answer, and Text classification. Each task type is associated with specific metrics and can leverage built-in datasets for evaluation.",
    "tags": [
      "task types",
      "text generation",
      "summarization",
      "qa",
      "classification"
    ]
  },
  {
    "id": 235,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Prompt Datasets for Model Evaluation",
    "content": "Prompt datasets are crucial for model evaluation in Amazon Bedrock, providing the inputs used to assess model performance. For automatic model evaluation jobs, users can supply custom prompt datasets tailored to their specific use cases or utilize available built-in datasets. These datasets contain prompts that guide the model to perform a task, allowing for the calculation of metrics like accuracy and robustness. For human-based evaluations, custom prompt datasets are also used, providing the prompts and often the 'ground truth' responses for human workers to review.",
    "tags": [
      "prompt datasets",
      "custom datasets",
      "built-in datasets",
      "evaluation data"
    ]
  },
  {
    "id": 236,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Creating an Automatic Evaluation Job",
    "content": "To create an automatic model evaluation job in Amazon Bedrock, you initiate a `CreateEvaluationJob` request. This process requires specifying a job name, the models to evaluate, an S3 bucket for input data (containing prompt datasets), and an S3 bucket for output reports and metrics. You also define the evaluation task type and associated metrics. An IAM service role with necessary permissions for Bedrock and S3 access must be provided for the job to run.",
    "tags": [
      "create job",
      "automatic evaluation",
      "api",
      "s3",
      "iam"
    ]
  },
  {
    "id": 237,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Listing Automatic Evaluation Jobs",
    "content": "To view information about your automatic model evaluation jobs, you can send a `ListEvaluationJobs` request. This operation allows you to retrieve a list of existing jobs. You can specify optional parameters such as `maxResults` to control the number of jobs returned and `nextToken` to paginate through results if there are more jobs than specified. This helps users track the status and details of their evaluation runs.",
    "tags": [
      "list job",
      "automatic evaluation",
      "api",
      "monitoring"
    ]
  },
  {
    "id": 238,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Stopping an Automatic Evaluation Job",
    "content": "You can stop an ongoing automatic model evaluation job in Amazon Bedrock. This is achieved by sending a `StopEvaluationJob` request, specifying the unique identifier of the job you wish to terminate. Stopping a job can be useful if the evaluation is no longer needed, if there's an issue with the job, or to manage resource consumption.",
    "tags": [
      "stop job",
      "automatic evaluation",
      "api",
      "job management"
    ]
  },
  {
    "id": 239,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Deleting an Automatic Evaluation Job",
    "content": "To remove an automatic model evaluation job from Amazon Bedrock, you can use the `DeleteEvaluationJob` operation. By providing the job's identifier, you can permanently delete the job and its associated records. This helps in cleaning up resources and managing your list of past evaluations.",
    "tags": [
      "delete job",
      "automatic evaluation",
      "api",
      "cleanup"
    ]
  },
  {
    "id": 240,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Overview of Human-Based Evaluation Jobs",
    "content": "Human-based model evaluation jobs leverage a team of human workers to assess and rate model performance. These evaluations provide qualitative feedback and preferences that complement automated metrics. They are crucial for tasks where subjective judgment, nuance, or complex understanding is required. Human workers provide input and ratings based on specific metrics or custom prompt datasets.",
    "tags": [
      "human evaluation",
      "human-based",
      "model performance",
      "qualitative feedback"
    ]
  },
  {
    "id": 241,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Creating Human-Based Evaluation Jobs",
    "content": "To create a human-based model evaluation job, you would use the console to define the job and configure human workers. This typically involves specifying the models to evaluate, the evaluation criteria, and selecting or creating a work team for the human reviews. You provide custom prompt datasets which human workers will use to rate model responses. The job workflow guides you through setting up these components to ensure effective human feedback.",
    "tags": [
      "create job",
      "human evaluation",
      "work team",
      "custom datasets"
    ]
  },
  {
    "id": 242,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Custom Prompt Datasets for Human Evaluations",
    "content": "For human-based model evaluation jobs, custom prompt datasets are used to collect human feedback. These datasets contain the specific prompts or user queries that human workers will interact with and evaluate. Unlike built-in datasets, custom datasets allow for highly tailored evaluations focusing on application-specific scenarios. They may also include 'ground truth' or expected responses for workers to compare against, enriching the evaluation process.",
    "tags": [
      "custom datasets",
      "human evaluation",
      "prompt engineering",
      "ground truth"
    ]
  },
  {
    "id": 243,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Creating a Human Model Evaluation Job",
    "content": "To initiate a human-based model evaluation job, you define the job in the Amazon Bedrock console or via API, similar to automatic jobs. This process includes selecting the models, configuring evaluation task types, and crucially, setting up the human work team. You'll specify input and output S3 locations for datasets and results, and provide custom prompt datasets tailored for human assessment. The platform then orchestrates the human review process to gather qualitative feedback.",
    "tags": [
      "create job",
      "human evaluation",
      "work team",
      "s3"
    ]
  },
  {
    "id": 244,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Listing Human Model Evaluation Jobs",
    "content": "To view information about your human-based model evaluation jobs, you can use API operations to list them. This allows you to track the status, progress, and details of all human evaluations initiated within your account. Like automatic jobs, `ListEvaluationJobs` can be used with parameters like `maxResults` and `nextToken` to manage the retrieved information effectively.",
    "tags": [
      "list job",
      "human evaluation",
      "api",
      "monitoring"
    ]
  },
  {
    "id": 245,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Stopping a Human Model Evaluation Job",
    "content": "An ongoing human-based model evaluation job can be stopped in Amazon Bedrock. This action halts the human review process for the specified job, preventing further ratings or input from workers. Stopping a human evaluation job might be necessary for various reasons, such as correcting setup errors or if the evaluation's objectives change.",
    "tags": [
      "stop job",
      "human evaluation",
      "job management"
    ]
  },
  {
    "id": 246,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Deleting a Human Model Evaluation Job",
    "content": "To remove a human-based model evaluation job from Amazon Bedrock, you can use the `DeleteEvaluationJob` operation. This action permanently deletes the job and its associated configurations and results. Deleting human evaluation jobs helps maintain a clear overview of active projects and manages stored evaluation data.",
    "tags": [
      "delete job",
      "human evaluation",
      "cleanup"
    ]
  },
  {
    "id": 247,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Managing Work Teams for Human Evaluations",
    "content": "While the provided sources do not explicitly detail how to manage a work team, human-based model evaluation jobs utilize human workers to rate and provide input. When creating such a job, users are guided to set up human workers. This implies an underlying mechanism for defining, assigning, and overseeing these work teams, likely involving configuration within the console during job creation.",
    "tags": [
      "work team",
      "human evaluation",
      "management",
      "workers"
    ]
  },
  {
    "id": 248,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "LLM as a Judge Evaluation Jobs",
    "content": "LLM as a judge model evaluation jobs use other Large Language Models to compute performance metrics for evaluating models. This approach automates the assessment process by leveraging the capabilities of powerful LLMs to act as evaluators. These jobs are designed to provide computed scores and metrics, similar to automatic evaluations, but with the nuanced judgment of an LLM. They are particularly useful for scaling evaluations that require more sophisticated assessment than simple rule-based systems but without human intervention.",
    "tags": [
      "llm as judge",
      "model evaluation",
      "automation",
      "llm metrics"
    ]
  },
  {
    "id": 249,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Supported Models for LLM as a Judge",
    "content": "For LLM as a judge model evaluation jobs, specific foundation models are designated as evaluators. These models, chosen for their analytical capabilities, are used to assess the performance of other models. The sources imply that Amazon Bedrock supports certain LLMs for this role, enabling a scaled and automated evaluation process. The exact list of supported judge models would be available within the service documentation.",
    "tags": [
      "llm as judge",
      "supported models",
      "evaluator models"
    ]
  },
  {
    "id": 250,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Prompt Datasets for LLM as a Judge",
    "content": "Prompt datasets for LLM as a judge model evaluation jobs are essential inputs for the evaluation process. These datasets contain the prompts or user queries that the models under evaluation will respond to. The judge LLM then evaluates these responses against the given prompts, and potentially against 'ground truth' answers or specified criteria, to determine performance metrics. The quality and structure of these datasets significantly influence the accuracy and utility of the LLM-based evaluation.",
    "tags": [
      "prompt datasets",
      "llm as judge",
      "evaluation data",
      "ground truth"
    ]
  },
  {
    "id": 251,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Evaluation Metrics for LLM as a Judge",
    "content": "LLM as a judge model evaluation jobs produce various evaluation metrics to quantify model performance. These metrics are computed by the judge LLM, providing insights into aspects such as accuracy, relevance, coherence, or fluency of the model responses. The specific metrics would be tailored to the evaluation task and the capabilities of the judge LLM. These metrics help users compare and select the most suitable model for their applications.",
    "tags": [
      "evaluation metrics",
      "llm as judge",
      "performance metrics",
      "accuracy"
    ]
  },
  {
    "id": 252,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Creating an LLM as a Judge Evaluation Job",
    "content": "To create an LLM as a judge model evaluation job, you submit a request that specifies the models to be evaluated and the chosen judge LLM. This process involves defining the input prompt datasets and the desired evaluation metrics. An IAM service role with appropriate permissions is required, along with S3 bucket configurations for input and output data. This job orchestrates the evaluation by the judge LLM to generate automated performance reports.",
    "tags": [
      "create job",
      "llm as judge",
      "api",
      "s3"
    ]
  },
  {
    "id": 253,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Listing LLM as a Judge Evaluation Jobs",
    "content": "You can view and monitor your LLM as a judge model evaluation jobs by listing them through the API. This allows you to check the status, progress, and details of all such evaluations. The `ListEvaluationJobs` operation provides a summary of jobs, which can be filtered or paginated using parameters like `maxResults` and `nextToken`.",
    "tags": [
      "list job",
      "llm as judge",
      "monitoring",
      "api"
    ]
  },
  {
    "id": 254,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Stopping an LLM as a Judge Evaluation Job",
    "content": "An ongoing LLM as a judge model evaluation job can be stopped in Amazon Bedrock using the appropriate API operation. This action terminates the evaluation process by the judge LLM, preventing further computation of metrics. Stopping a job is useful for managing costs or if the evaluation needs to be reconfigured due to issues.",
    "tags": [
      "stop job",
      "llm as judge",
      "job management"
    ]
  },
  {
    "id": 255,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Overview of RAG Evaluation Jobs",
    "content": "RAG evaluation jobs in Amazon Bedrock are designed to assess the performance of Retrieval Augmented Generation (RAG) sources, including Amazon Bedrock Knowledge Bases. These jobs compute performance metrics to determine if a RAG system can retrieve highly relevant information and generate useful, appropriate responses. Users provide datasets with prompts and 'ground truth' retrieved texts and responses to check alignment with expectations. This helps in optimizing the retrieval and generation components of RAG applications.",
    "tags": [
      "rag evaluation",
      "knowledge bases",
      "retrieval",
      "generation",
      "performance"
    ]
  },
  {
    "id": 256,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Supported Models for RAG Evaluation",
    "content": "For RAG evaluation jobs, Amazon Bedrock supports specific models for assessing the quality of retrieval and generation components. These models are used in the evaluation process to determine the effectiveness of the RAG system in providing relevant information and producing appropriate responses. The exact supported models for RAG evaluation would be listed in the service's model compatibility documentation.",
    "tags": [
      "rag evaluation",
      "supported models",
      "knowledge bases"
    ]
  },
  {
    "id": 257,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Prompt Datasets for RAG Evaluation",
    "content": "Prompt datasets are critical for RAG evaluation jobs, containing user queries used to assess how a knowledge base retrieves information and generates responses. These datasets must also include 'ground truth' or expected retrieved texts and responses for the queries. This enables the evaluation to verify if the RAG source's output aligns with expected outcomes, providing a benchmark for performance.",
    "tags": [
      "prompt datasets",
      "rag evaluation",
      "ground truth",
      "retrieval"
    ]
  },
  {
    "id": 258,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Evaluation Metrics for RAG Evaluation",
    "content": "RAG evaluation jobs provide various evaluation metrics to quantify the performance of Retrieval Augmented Generation systems. These metrics reveal if a RAG source successfully retrieves highly relevant information and generates useful, appropriate responses. By analyzing these metrics, users can assess the quality of their knowledge bases and make informed decisions to improve their RAG applications.",
    "tags": [
      "evaluation metrics",
      "rag evaluation",
      "performance",
      "relevance"
    ]
  },
  {
    "id": 259,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Creating a RAG Evaluation Job",
    "content": "To create a RAG evaluation job, you initiate a request specifying the RAG source to evaluate, the models involved, and the input prompt datasets. These datasets must include 'ground truth' for accurate assessment. You also define the desired evaluation metrics for the job. An IAM service role and properly configured S3 buckets for input and output are essential prerequisites for running the evaluation.",
    "tags": [
      "create job",
      "rag evaluation",
      "api",
      "s3",
      "ground truth"
    ]
  },
  {
    "id": 260,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Listing RAG Evaluation Jobs",
    "content": "To monitor and retrieve information about your RAG evaluation jobs, you can use the `ListEvaluationJobs` operation. This API call allows you to view a list of all your RAG evaluation jobs, including their current status and details. You can utilize optional parameters such as `maxResults` and `nextToken` for efficient listing and pagination through extensive job histories.",
    "tags": [
      "list job",
      "rag evaluation",
      "monitoring",
      "api"
    ]
  },
  {
    "id": 261,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Stopping a RAG Evaluation Job",
    "content": "An ongoing RAG evaluation job can be stopped in Amazon Bedrock by using the `StopEvaluationJob` API operation. This action will halt the current evaluation process for the specified RAG source. Stopping a RAG evaluation job may be necessary to manage computational resources, address errors in configuration, or adjust the evaluation scope.",
    "tags": [
      "stop job",
      "rag evaluation",
      "job management"
    ]
  },
  {
    "id": 262,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Deleting a RAG Evaluation Job",
    "content": "To remove a RAG evaluation job and its associated data from Amazon Bedrock, you can use the `DeleteEvaluationJob` operation. This action permanently deletes the job, helping you to clean up completed or unnecessary evaluations. Deleting jobs contributes to maintaining an organized overview of your RAG system's performance assessments.",
    "tags": [
      "delete job",
      "rag evaluation",
      "cleanup"
    ]
  },
  {
    "id": 263,
    "service": "Amazon Bedrock",
    "category": "RAG Evaluation",
    "title": "Knowledge Base Evaluation Reports and Metrics",
    "content": "After running a knowledge base evaluation job, Amazon Bedrock generates reports and metrics that detail the performance of the Retrieval Augmented Generation (RAG) system. These outputs provide insights into how effectively the knowledge base retrieves information and generates responses. The metrics help users understand the strengths and weaknesses of their RAG configurations, allowing for informed improvements.",
    "tags": [
      "reports",
      "metrics",
      "knowledge base",
      "rag",
      "performance"
    ]
  },
  {
    "id": 264,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "CORS Requirements for Evaluations",
    "content": "Cross Origin Resource Sharing (CORS) permissions are a critical requirement for console-based model evaluation jobs in Amazon Bedrock. Specifically, CORS must be enabled on any Amazon S3 buckets that are designated for input or output data within the model evaluation job. This ensures that the web console can securely access the necessary S3 resources to run and manage the evaluation processes.",
    "tags": [
      "cors",
      "s3",
      "security",
      "prerequisites"
    ]
  },
  {
    "id": 265,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Model Evaluation Reports and Metrics",
    "content": "After a model evaluation job completes, Amazon Bedrock provides reports and metrics to assess model performance. For automated model evaluation jobs, users can review computed metrics to understand how well a model performed on specific tasks. For human model evaluation jobs, the reports summarize human ratings and feedback, offering qualitative insights. Understanding the Amazon S3 output from these jobs is crucial for interpreting the comprehensive evaluation results.",
    "tags": [
      "reports",
      "metrics",
      "model evaluation",
      "automated",
      "human"
    ]
  },
  {
    "id": 266,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Reviewing Automated Model Evaluation Metrics",
    "content": "To review the performance of an automated model evaluation job, users examine the metrics provided by Amazon Bedrock. These computed metrics quantify aspects like accuracy, robustness, and relevance, depending on the chosen task type. The evaluation reports detail these scores, allowing for a data-driven comparison of different models or configurations. This review process is essential for identifying the most effective models for specific generative AI applications.",
    "tags": [
      "review metrics",
      "automated evaluation",
      "performance",
      "reports"
    ]
  },
  {
    "id": 267,
    "service": "Amazon Bedrock",
    "category": "Model Evaluation",
    "title": "Reviewing Human Model Evaluation Jobs",
    "content": "Reviewing a human model evaluation job involves examining the feedback and ratings provided by human workers. This process yields qualitative insights into model performance, covering aspects that are difficult for automated systems to assess, such as nuance, creativity, or subjective quality. The reports from human evaluations summarize these inputs, offering a comprehensive understanding of user perception and model alignment with desired human-like qualities.",
    "tags": [
      "review job",
      "human evaluation",
      "qualitative feedback",
      "reports"
    ]
  },
  {
    "id": 268,
    "service": "Amazon Bedrock",
    "category": "Data Management",
    "title": "Understanding S3 Output for Evaluations",
    "content": "Understanding the Amazon S3 output from a model evaluation job is crucial for interpreting the results. Evaluation jobs write their output, including detailed metrics and reports, to specified S3 buckets. This output typically includes JSONL files with individual prompt results, aggregated metric summaries, and other artifacts generated during the evaluation process. Knowing the structure and content of these S3 files enables thorough analysis of model and RAG system performance.",
    "tags": [
      "s3 output",
      "data management",
      "evaluation results",
      "reports"
    ]
  },
  {
    "id": 269,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Data Management and Encryption for Evaluations",
    "content": "Data management and encryption are integral to Amazon Bedrock evaluation jobs to ensure security and privacy. All data used and generated by evaluation jobs, including prompt datasets and output reports, must be stored in Amazon S3 buckets. By default, output data is encrypted using an AWS managed key, but users can opt for a custom KMS key for enhanced security. Proper IAM policy requirements and key policy requirements are necessary to control access and protect sensitive evaluation data.",
    "tags": [
      "data management",
      "encryption",
      "kms",
      "s3",
      "security"
    ]
  },
  {
    "id": 270,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "KMS Key Policy Requirements for Evaluations",
    "content": "For Amazon Bedrock evaluation jobs using customer-managed KMS keys, specific key policy requirements must be met. These policies define who or what can use the KMS key to encrypt and decrypt data associated with the evaluation job. The key policy must grant necessary permissions to the Amazon Bedrock service role to interact with the KMS key, ensuring secure access to sensitive evaluation data stored in S3.",
    "tags": [
      "key policy",
      "kms",
      "encryption",
      "security",
      "iam"
    ]
  },
  {
    "id": 271,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "IAM Policy Requirements for Evaluations",
    "content": "For Amazon Bedrock evaluation jobs, stringent IAM policy requirements are necessary for both the IAM identity creating the job and the service role Amazon Bedrock assumes. The IAM identity needs permissions for actions like `CreateEvaluationJob`, `GetEvaluationJob`, and S3 access. The service role requires permissions to access S3 buckets for input/output data and to potentially use KMS keys for encryption. These policies ensure secure and controlled operation of evaluation jobs.",
    "tags": [
      "iam policy",
      "permissions",
      "service role",
      "security"
    ]
  },
  {
    "id": 272,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Data Encryption for Knowledge Base Evaluations",
    "content": "Data encryption is a vital security feature for knowledge base evaluation jobs in Amazon Bedrock. All sensitive data, including input prompt datasets and output evaluation reports, is encrypted when stored in Amazon S3. Users have the option to use either AWS managed keys or their own customer-managed KMS keys for this encryption. This ensures the confidentiality and integrity of evaluation data throughout its lifecycle.",
    "tags": [
      "data encryption",
      "knowledge base",
      "kms",
      "s3",
      "security"
    ]
  },
  {
    "id": 273,
    "service": "Amazon Bedrock",
    "category": "Monitoring",
    "title": "Management Events for Evaluations",
    "content": "The mention of 'Management events' in the context of evaluation jobs suggests that Amazon Bedrock records API calls and related activities as events. These events are likely logged through AWS CloudTrail, providing a record of actions taken on evaluation jobs for security and auditing purposes. This allows users to monitor who performed what actions, when, and from where, ensuring accountability and compliance for their evaluation workflows.",
    "tags": [
      "management events",
      "cloudtrail",
      "logging",
      "auditing"
    ]
  },
  {
    "id": 274,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Model customization: Train models for specific use cases",
    "content": "Model customization in Amazon Bedrock allows you to adapt foundation models to specific tasks and domains. This involves using training data to adjust the model's parameters, creating a custom model for improved performance. Key techniques include fine-tuning, which uses labeled data, and continued pre-training, which utilizes unlabeled data. This process enables private customization of models with your own data, enhancing their relevance for unique use cases. After customization, Provisioned Throughput must be purchased to use the custom model.",
    "tags": [
      "model customization",
      "fine-tuning",
      "pre-training",
      "custom models",
      "training data"
    ]
  },
  {
    "id": 275,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Guidelines for model customization",
    "content": "Guidelines for model customization involve setting hyperparameters that control the training process, such as the learning rate or epoch count. You configure these settings when submitting a fine-tuning job via the Amazon Bedrock console or the `CreateModelCustomizationJob` API operation. The number of epochs, which dictates how many times the entire training dataset is processed, directly impacts customization costs. Specific guidelines for different model families detail appropriate ranges for these hyperparameters.",
    "tags": [
      "model customization",
      "guidelines",
      "hyperparameters",
      "training",
      "cost"
    ]
  },
  {
    "id": 276,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Model customization access and security",
    "content": "Model customization access and security in Amazon Bedrock require careful configuration of IAM service roles with necessary permissions. It is crucial to set up these roles to control who can create and manage customization jobs. Optional security measures include granting specific permissions for cross-region distillation jobs, encrypting customization jobs and artifacts, and protecting your customization jobs using a Virtual Private Cloud (VPC). These measures ensure data privacy and secure operations during the customization process .",
    "tags": [
      "model customization",
      "security",
      "access",
      "IAM",
      "VPC",
      "encryption"
    ]
  },
  {
    "id": 277,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Create an IAM service role for model customization",
    "content": "To perform model customization in Amazon Bedrock, you must create an IAM service role. This role grants Amazon Bedrock the necessary permissions to execute customization jobs on your behalf, such as accessing training data and managing model artifacts. When creating this role, you define policies that specify which actions Amazon Bedrock is allowed to perform, ensuring adherence to the principle of least privilege. This setup is fundamental for securely managing access to your customization workflows.",
    "tags": [
      "IAM",
      "service role",
      "permissions",
      "model customization",
      "security"
    ]
  },
  {
    "id": 278,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "(Optional) Permissions to create a Distillation job with a cross-region inference profile",
    "content": "Creating a Distillation job that utilizes a cross-region inference profile requires specific IAM permissions. This optional configuration enables the distillation process to leverage compute resources across different AWS Regions, aiding in managing traffic bursts and optimizing resource utilization. The associated IAM policy must grant Amazon Bedrock the authority to create such distillation jobs and interact with the designated cross-region inference profiles, ensuring secure and distributed operations. Cross Region (system-defined) inference profiles are predefined in Amazon Bedrock.",
    "tags": [
      "distillation",
      "cross-region",
      "inference profile",
      "IAM permissions",
      "optional"
    ]
  },
  {
    "id": 279,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "(Optional) Encrypt model customization jobs and artifacts",
    "content": "For enhanced security, you can optionally encrypt model customization jobs and their artifacts in Amazon Bedrock. This involves using AWS Key Management Service (KMS) keys to protect your sensitive training data and custom model outputs at rest. Configuring encryption requires specific IAM policies that grant Amazon Bedrock the necessary permissions to use your KMS key for data encryption and decryption during the customization process, safeguarding your intellectual property.",
    "tags": [
      "encryption",
      "KMS",
      "model customization",
      "artifacts",
      "security"
    ]
  },
  {
    "id": 280,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "(Optional) Protect your model customization jobs using a VPC",
    "content": "To isolate your model customization jobs from the public internet, you can optionally protect them using an Amazon Virtual Private Cloud (VPC). This configuration ensures that your training data and customization processes remain within a private, secure network environment. Implementing a VPC involves setting up specific VPC subnets and security groups, and granting the IAM service role for customization the necessary permissions to access these VPC resources and manage Elastic Network Interfaces (ENIs).",
    "tags": [
      "VPC",
      "security",
      "model customization",
      "network isolation",
      "private connection"
    ]
  },
  {
    "id": 281,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Customize a model with distillation",
    "content": "Model distillation is a technique used in Amazon Bedrock to customize models, particularly to create smaller, more efficient student models from larger, higher-performing teacher models. This process transfers knowledge from the teacher to the student, allowing the student model to achieve comparable performance with reduced computational requirements. Implementing distillation involves selecting appropriate teacher and student models, preparing specialized training datasets, and submitting a distillation job.",
    "tags": [
      "model distillation",
      "customization",
      "teacher model",
      "student model",
      "training"
    ]
  },
  {
    "id": 282,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "How Amazon Bedrock Model Distillation works",
    "content": "Amazon Bedrock Model Distillation operates by transferring knowledge from a large, complex teacher model to a smaller, more efficient student model. The student model learns to replicate the teacher's outputs, effectively mimicking its performance. This process enables the creation of highly optimized models that are faster and less resource-intensive, while retaining much of the original model's accuracy. It involves iterative training where the student learns from the teacher's responses to diverse inputs .",
    "tags": [
      "model distillation",
      "knowledge transfer",
      "teacher model",
      "student model",
      "optimization"
    ]
  },
  {
    "id": 283,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Access and security for Model Distillation",
    "content": "Access and security for Model Distillation in Amazon Bedrock are managed through similar mechanisms as general model customization. This includes configuring appropriate IAM service roles to grant necessary permissions for creating and managing distillation jobs, accessing training datasets, and deploying resulting student models. Additionally, employing encryption for data at rest and in transit, and optionally using an Amazon VPC for network isolation, helps safeguard the sensitive information involved in the distillation process .",
    "tags": [
      "model distillation",
      "security",
      "access",
      "IAM",
      "encryption",
      "VPC"
    ]
  },
  {
    "id": 284,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Choose teacher and student models for distillation",
    "content": "A critical step in Model Distillation within Amazon Bedrock is choosing appropriate teacher and student models. The teacher model is typically a high-performing, larger foundation model from which knowledge is transferred. The student model is a smaller, more efficient model that learns from the teacher's outputs. This selection is crucial for achieving the desired balance between performance and efficiency for the specific use case after distillation .",
    "tags": [
      "model distillation",
      "teacher model",
      "student model",
      "model selection",
      "efficiency"
    ]
  },
  {
    "id": 285,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Prepare your training datasets for distillation",
    "content": "Preparing training datasets for distillation is a vital prerequisite for successful model customization in Amazon Bedrock. These datasets are specifically designed to enable the student model to learn effectively from the teacher model's outputs. The quality and diversity of the training data directly influence the student model's ability to retain performance while becoming more efficient. This step ensures the student accurately mimics the teacher's reasoning and generation capabilities.",
    "tags": [
      "training data",
      "distillation",
      "datasets",
      "data preparation",
      "student model"
    ]
  },
  {
    "id": 286,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Submit a model distillation job",
    "content": "After selecting teacher and student models and preparing the necessary training datasets, the next step is to submit a model distillation job in Amazon Bedrock. This initiates the process of transferring knowledge from the larger teacher model to the smaller student model. The submission involves specifying the models and the prepared data, which Amazon Bedrock then uses to conduct the distillation, ultimately yielding an optimized custom model .",
    "tags": [
      "model distillation",
      "job submission",
      "training",
      "custom model"
    ]
  },
  {
    "id": 287,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Clone a distillation job",
    "content": "Amazon Bedrock allows you to clone a distillation job, which enables the reuse of an existing job's configurations for new distillation tasks. This feature is particularly useful for iterative experimentation or for setting up similar distillation processes with minor modifications. By cloning a job, you can quickly create a new job definition without having to reconfigure all parameters from scratch, streamlining your model customization workflow .",
    "tags": [
      "distillation job",
      "clone",
      "job management",
      "reusability",
      "model customization"
    ]
  },
  {
    "id": 288,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Customize a model with fine-tuning or continued pre-training",
    "content": "Amazon Bedrock offers two primary methods for model customization: fine-tuning and continued pre-training. Fine-tuning adapts a base model to specific tasks by providing it with labeled training data, consisting of input-output pairs. In contrast, continued pre-training adjusts model parameters using unlabeled data, which is useful for enhancing a model's understanding of a new domain or style. Both techniques aim to improve model performance for targeted use cases.",
    "tags": [
      "model customization",
      "fine-tuning",
      "continued pre-training",
      "training data",
      "model adaptation"
    ]
  },
  {
    "id": 289,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Supported models and Regions for fine-tuning and continued pre-training",
    "content": "Fine-tuning and continued pre-training in Amazon Bedrock are supported by a specific set of foundation models and are available in various AWS Regions. For example, Amazon Titan Text G1 models support both fine-tuning and continued pre-training. Amazon Nova Pro, Lite, and Micro models also support fine-tuning, as do some Anthropic Claude 3 and Meta Llama models. Users should consult the documentation to verify model and region compatibility, as this ensures successful customization jobs for their applications .",
    "tags": [
      "fine-tuning",
      "pre-training",
      "supported models",
      "AWS Regions",
      "compatibility"
    ]
  },
  {
    "id": 290,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Prepare your training datasets for fine-tuning and continued pre-training",
    "content": "To effectively customize models through fine-tuning or continued pre-training in Amazon Bedrock, meticulously preparing your training datasets is essential. For fine-tuning, datasets require labeled examples (input-output pairs) specific to the target task. For continued pre-training, unlabeled data relevant to the new domain or style is needed. The quality, size, and format of these datasets directly impact the customized model's performance and its ability to adapt to new requirements.",
    "tags": [
      "training datasets",
      "fine-tuning",
      "pre-training",
      "data preparation",
      "model customization"
    ]
  },
  {
    "id": 291,
    "service": "Amazon Bedrock",
    "category": "Model customization",
    "title": "Submit a model fine-tuning or continued pre-training job",
    "content": "Once training datasets are prepared and hyperparameters are configured, you can submit a model fine-tuning or continued pre-training job in Amazon Bedrock. This initiates the customization process, adapting the chosen foundation model to your specific use case. The job submission specifies the model, training data location, and desired configuration, allowing Amazon Bedrock to manage the underlying infrastructure and execute the training, resulting in a specialized custom model .",
    "tags": [
      "fine-tuning",
      "pre-training",
      "job submission",
      "training job",
      "model customization"
    ]
  },
  {
    "id": 292,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitor your model customization job",
    "content": "The provided sources indicate a section titled \"Monitor your model customization job\" as part of the overall model customization process. However, the detailed information outlining the specific steps or tools available for actively monitoring the progress and status of these jobs is not present in the provided excerpts. Users would typically look here for insights into job status, resource utilization, and potential issues during training. This information is crucial for overseeing the customization workflow effectively.",
    "tags": [
      "model customization",
      "monitor",
      "jobs",
      "status"
    ]
  },
  {
    "id": 293,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Analyze model customization job results",
    "content": "The provided sources include a section heading for \"Analyze model customization job results\". This section would typically describe how to evaluate the performance and quality of a newly customized model after a training job completes. It might cover metrics, comparison tools, or other methods for assessing whether the customization achieved the desired improvements. However, the specific details on how to perform this analysis are not present in the provided excerpts.",
    "tags": [
      "model customization",
      "analyze",
      "results",
      "evaluation"
    ]
  },
  {
    "id": 294,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Stop a model customization job",
    "content": "The sources include a section titled \"Stop a model customization job\" within the context of model customization management. This feature allows users to terminate an ongoing customization job prematurely, which can be useful for cost control or if issues are detected during training. Despite the presence of this heading, the provided excerpts do not offer specific instructions or API commands for stopping a model customization job.",
    "tags": [
      "model customization",
      "stop",
      "jobs",
      "management"
    ]
  },
  {
    "id": 295,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "View details about a custom model",
    "content": "The provided sources mention a section for \"View details about a custom model\". This functionality would typically enable users to retrieve comprehensive information about their privately customized models, such as their ARN, creation date, and associated training data. This detail is essential for managing and integrating custom models into applications. However, the specific methods or console navigation steps to view these details are not provided in the given excerpts.",
    "tags": [
      "custom model",
      "view",
      "details",
      "management"
    ]
  },
  {
    "id": 296,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Set up inference for a custom model",
    "content": "To set up inference for a custom model on Amazon Bedrock, you must first purchase Provisioned Throughput for it. This step is required to increase the amount and rate of tokens processed during model inference. Upon purchasing Provisioned Throughput, a provisioned model is created, and its ARN (Amazon Resource Name) is then used as the model ID for subsequent inference requests. This allows the customized model to be used in generative AI applications effectively.",
    "tags": [
      "custom model",
      "inference",
      "setup",
      "provisioned throughput"
    ]
  },
  {
    "id": 297,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Purchase Provisioned Throughput for a custom model",
    "content": "Purchasing Provisioned Throughput for a custom model is a necessary step to enable its use for inference on Amazon Bedrock. You specify the custom model's name or ARN as the `modelId` in the `CreateProvisionedModelThroughput` request. This action creates a provisioned model, returning a `provisionedModelArn` that serves as the unique model ID for invoking the model. This ensures dedicated capacity and consistent performance for your customized AI models.",
    "tags": [
      "custom model",
      "provisioned throughput",
      "purchase",
      "inference"
    ]
  },
  {
    "id": 298,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Deploy a custom model for on-demand inference",
    "content": "The provided sources list a section titled \"Deploy a custom model for on-demand inference\". While this heading suggests the capability to deploy custom models without purchasing dedicated throughput upfront, similar to how base models can be invoked on-demand, the specific details or process for enabling on-demand inference for a custom model are not elaborated upon in the provided excerpts. This functionality would typically allow for flexible usage without long-term capacity commitments.",
    "tags": [
      "custom model",
      "deploy",
      "on-demand",
      "inference"
    ]
  },
  {
    "id": 299,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Share a model for another account to use",
    "content": "Amazon Bedrock enables you to share a custom model with other AWS accounts. This feature is essential for collaborative development or distributing a specialized model across different organizational units. The process involves specific steps, supported regions, and fulfilling certain prerequisites to establish the sharing mechanism. It also includes functionalities to manage the lifecycle of shared access, such as viewing, updating, and revoking permissions.",
    "tags": [
      "model sharing",
      "custom model",
      "cross-account",
      "collaboration"
    ]
  },
  {
    "id": 300,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Regions and models (for sharing)",
    "content": "The provided sources include a section titled \"Supported Regions and models\" within the context of sharing a model for another account to use. However, the detailed information specifically listing the AWS Regions where model sharing is available or the particular models that support this feature is not provided in the given excerpts. General tables show regional support for other features like model copy.",
    "tags": [
      "model sharing",
      "regions",
      "supported models",
      "availability"
    ]
  },
  {
    "id": 301,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Prerequisites (for sharing)",
    "content": "The sources mention a \"Prerequisites\" section that applies to sharing a model for another account to use. This section would typically outline the necessary conditions, permissions, or configurations required before you can successfully share a custom model. While general prerequisites for Amazon Bedrock involve IAM roles and model access, the specific requirements tailored for model sharing are not detailed in the provided excerpts.",
    "tags": [
      "model sharing",
      "prerequisites",
      "setup",
      "permissions"
    ]
  },
  {
    "id": 302,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Share a model with another account",
    "content": "The provided sources include a section heading for \"Share a model with another account\". This capability allows custom models to be made accessible to other AWS accounts, facilitating collaboration and broader usage. While the intent of this feature is clear, the detailed, step-by-step instructions or the specific API operations required to initiate the process of sharing a model are not explicitly described in the provided excerpts.",
    "tags": [
      "model sharing",
      "cross-account",
      "process",
      "custom models"
    ]
  },
  {
    "id": 303,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "View information about shared models",
    "content": "The provided sources list a section titled \"View information about shared models\". This suggests a capability to inspect details about custom models that have been shared with your account or that you have shared with others. Such information would include the model's ARN, its owner, and access permissions. However, the specific methods or interfaces (console, API) to access and view this information are not detailed within the given excerpts.",
    "tags": [
      "shared models",
      "view",
      "information",
      "access"
    ]
  },
  {
    "id": 304,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Update access to a shared model",
    "content": "The provided sources mention a section for \"Update access to a shared model\". This capability implies that users can modify the permissions or configurations associated with a custom model that has been shared with other accounts. This allows for dynamic adjustments to access rights based on evolving requirements. However, the specific procedures or API calls necessary to update access for a shared model are not detailed in the provided excerpts.",
    "tags": [
      "shared models",
      "access",
      "update",
      "permissions"
    ]
  },
  {
    "id": 305,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Revoke access to a shared model",
    "content": "The sources include a section titled \"Revoke access to a shared model\". This feature allows the owner of a custom model to withdraw permissions previously granted to another AWS account, effectively stopping their ability to use the shared model. This is an important security and management function. Nevertheless, the detailed instructions or API operations for revoking access are not provided within the given excerpts.",
    "tags": [
      "shared models",
      "access",
      "revoke",
      "security"
    ]
  },
  {
    "id": 306,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Copy a model to use in a Region",
    "content": "The \"Model copy\" feature is listed as a capability in Amazon Bedrock, allowing models to be replicated across different AWS Regions. This is useful for disaster recovery, compliance, or placing models closer to end-users for lower latency. Despite its mention as a supported feature, the provided sources do not offer a dedicated section or detailed explanation on how to initiate and complete the process of copying a model to another Region.",
    "tags": [
      "model copy",
      "regions",
      "replication",
      "management"
    ]
  },
  {
    "id": 307,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Regions and models (for copying)",
    "content": "The Model copy feature in Amazon Bedrock is supported in various AWS Regions, including US East (N. Virginia), US East (Ohio), and US West (Oregon). The general tables detailing feature support by Region indicate which geographical areas allow this operation. Additionally, the \"Model support by feature\" tables list specific models that are compatible with the Model copy functionality.",
    "tags": [
      "model copy",
      "regions",
      "supported models",
      "compatibility"
    ]
  },
  {
    "id": 308,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Prerequisites (for copying)",
    "content": "The provided sources do not explicitly detail the prerequisites specifically for using the model copy feature in Amazon Bedrock. While general requirements for using Amazon Bedrock typically involve setting up IAM permissions and requesting access to foundation models, the specific conditions or configurations needed before copying a model to another region are not outlined in these excerpts.",
    "tags": [
      "model copy",
      "prerequisites",
      "setup",
      "requirements"
    ]
  },
  {
    "id": 309,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Copy a model to a Region",
    "content": "While the capability to copy a model to a Region is listed as a feature in Amazon Bedrock, the provided sources do not contain specific, step-by-step instructions or API examples on how to perform this operation. The \"Model copy\" feature allows for geographical distribution of models, but the procedural details for initiating such a copy within the AWS Management Console or via API calls are not present in these excerpts.",
    "tags": [
      "model copy",
      "process",
      "regions",
      "distribution"
    ]
  },
  {
    "id": 310,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "View information about model copy jobs",
    "content": "The provided sources do not contain specific information on how to view information about model copy jobs in Amazon Bedrock. While model copy is identified as a supported feature, details such as monitoring the status of a copy operation, retrieving job logs, or checking the completion of a model replication task are not elaborated upon in the given excerpts. Users would typically look for such information to track their cross-region model deployments.",
    "tags": [
      "model copy",
      "jobs",
      "view",
      "status"
    ]
  },
  {
    "id": 311,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Delete a custom model",
    "content": "The provided sources do not contain specific information or instructions on how to delete a custom model in Amazon Bedrock. While the process of customizing models through fine-tuning or distillation is detailed, the management aspect of removing or deprecating a custom model after its lifecycle is not covered in these excerpts. This would typically involve API calls or console steps to remove the model artifact.",
    "tags": [
      "custom model",
      "delete",
      "management",
      "lifecycle"
    ]
  },
  {
    "id": 312,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Code samples for model customization",
    "content": "Amazon Bedrock provides code examples for model customization using AWS SDKs, such as Python (Boto3). These samples illustrate how to perform various customization techniques, including fine-tuning and continued pre-training. Additionally, examples for submitting model distillation jobs are available, which helps in creating smaller, more efficient custom models. These code snippets facilitate programmatic interaction with the Bedrock API for tailoring models to specific use cases.",
    "tags": [
      "model customization",
      "code",
      "examples",
      "SDKs"
    ]
  },
  {
    "id": 313,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Troubleshooting model customization issues",
    "content": "The provided sources include a section heading for \"Troubleshooting model customization issues\". This indicates that Amazon Bedrock offers guidance for resolving problems encountered during the customization process of foundation models. Users might refer to this section for solutions related to common errors, unexpected behaviors, or performance concerns. However, the detailed content outlining these troubleshooting steps or common issues is not present in the given excerpts.",
    "tags": [
      "model customization",
      "troubleshooting",
      "issues",
      "diagnosis"
    ]
  },
  {
    "id": 314,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Permissions issues (model customization)",
    "content": "Within the context of troubleshooting model customization, the provided sources identify \"Permissions issues\" as a potential area of concern. This refers to problems that arise from insufficient or incorrectly configured IAM permissions, which are crucial for Amazon Bedrock to perform actions like creating or monitoring customization jobs. While the heading is present, specific details on how to diagnose or resolve these permission-related challenges during model customization are not provided in the given excerpts.",
    "tags": [
      "model customization",
      "permissions",
      "IAM",
      "troubleshooting"
    ]
  },
  {
    "id": 315,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Data issues (model customization)",
    "content": "The provided sources list \"Data issues\" as a category for troubleshooting problems that might occur during model customization. This suggests that challenges related to the training datasets, such as formatting errors, inconsistencies, or poor data quality, can impact the customization process. Addressing these issues is vital for successful model adaptation. However, the specific types of data problems or their resolutions are not detailed in the provided excerpts.",
    "tags": [
      "model customization",
      "data",
      "datasets",
      "troubleshooting"
    ]
  },
  {
    "id": 316,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Third-party license terms and policy issues (model customization)",
    "content": "The sources include a troubleshooting category for \"Third-party license terms and policy issues\" relevant to model customization. This highlights the importance of adhering to the End User License Agreements (EULA) of foundation model providers when customizing models on Amazon Bedrock. Non-compliance with these terms or related policies can lead to operational issues. However, specific guidance or solutions for navigating these license and policy challenges are not detailed in the provided excerpts.",
    "tags": [
      "model customization",
      "licenses",
      "third-party",
      "EULA",
      "troubleshooting"
    ]
  },
  {
    "id": 317,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Internal error (model customization)",
    "content": "The provided sources list \"Internal error\" as a category for troubleshooting model customization issues. This indicates that unexpected system-level faults can occur during the customization process, potentially disrupting training jobs or model deployment. While the existence of this error type is acknowledged, the given excerpts do not provide specific details on what constitutes an internal error in this context, nor do they offer recommended steps for users to diagnose or resolve such issues.",
    "tags": [
      "model customization",
      "internal error",
      "troubleshooting",
      "system"
    ]
  },
  {
    "id": 318,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Knowledge Bases: Retrieve data to augment responses",
    "content": "Amazon Bedrock Knowledge Bases allow you to augment foundation model responses with information from your own data sources, a process known as Retrieval Augmented Generation (RAG). This capability helps reduce hallucinations and provides more accurate, contextually relevant responses. You can upload data sources to be queried, which then supply additional context to the model for generating responses. Knowledge bases enable building generative AI applications with your enterprise data, ensuring security and privacy.",
    "tags": [
      "knowledge bases",
      "rag",
      "data sources",
      "fms"
    ]
  },
  {
    "id": 319,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "How knowledge bases work",
    "content": "Knowledge bases function through a three-step Retrieval Augmented Generation (RAG) process. First, information is queried and retrieved from your specified data source. Next, this retrieved information is used to augment a prompt, providing the foundation model with richer context. Finally, the foundation model utilizes this additional context to generate a more informed and accurate response.",
    "tags": [
      "rag",
      "retrieval",
      "data sources",
      "fms"
    ]
  },
  {
    "id": 320,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Turning data into a knowledge base",
    "content": "Turning data into a knowledge base involves several key steps to prepare your information for Retrieval Augmented Generation (RAG). This includes setting up permissions to manage knowledge bases, and then building the knowledge base itself by connecting to various data sources. Data can be processed for vector embeddings and parsing to facilitate efficient retrieval. The system enables easy chat interaction with your documents with zero setup.",
    "tags": [
      "data preparation",
      "embeddings",
      "parsing",
      "rag"
    ]
  },
  {
    "id": 321,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Retrieving information from data sources",
    "content": "Retrieving information from data sources is a core component of how Knowledge Bases operate, particularly within the Retrieval Augmented Generation (RAG) workflow. This step involves querying your connected data sources to fetch relevant context that will then be passed to the foundation model. Effective retrieval ensures that the model has access to the most pertinent information to generate accurate and grounded responses.",
    "tags": [
      "data retrieval",
      "rag",
      "data sources",
      "querying"
    ]
  },
  {
    "id": 322,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Customizing your knowledge base",
    "content": "You can customize your knowledge base in Amazon Bedrock to align with specific needs. This includes various configuration options to optimize how data is processed, stored, and retrieved. For instance, you can customize components like the vector embedding models, parsing strategies, and even reranking results during queries to enhance relevance and accuracy. Customization ensures the knowledge base performs optimally for your unique use cases.",
    "tags": [
      "customization",
      "optimization",
      "embeddings",
      "reranking"
    ]
  },
  {
    "id": 323,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Supported models and Regions",
    "content": "Amazon Bedrock Knowledge Bases are supported across various AWS Regions, and the specific models compatible with this feature can differ by region. For instance, certain regions like US East (N. Virginia) and US West (Oregon) broadly support knowledge bases. It is important to consult the Amazon Bedrock documentation for a comprehensive list of currently supported regions and models to ensure compatibility for your specific deployment.",
    "tags": [
      "regions",
      "model support",
      "availability"
    ]
  },
  {
    "id": 324,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Supported models for vector embeddings",
    "content": "For converting information into numerical vector embeddings, Amazon Bedrock supports specific models designed for this purpose within its knowledge base offerings. These embedding models transform text, images, or multimodal inputs into vectors, allowing for similarity comparisons between different objects. Examples include Amazon Titan Text Embeddings V2 (model ID: `amazon.titan-embed-text-v2:0`) and Amazon Titan Multimodal Embeddings G1 (model ID: `amazon.titan-embed-image-v1`).",
    "tags": [
      "embeddings",
      "vector models",
      "titan"
    ]
  },
  {
    "id": 325,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Supported models and Regions for parsing",
    "content": "Amazon Bedrock Knowledge Bases utilize specific models and are available in particular Regions for parsing data. Parsing is essential for extracting structured information from various data sources before it can be effectively used by a knowledge base. The exact models and regions supported for parsing can be found in the detailed Amazon Bedrock documentation, which is crucial for setting up your data sources correctly.",
    "tags": [
      "parsing",
      "data processing",
      "regions"
    ]
  },
  {
    "id": 326,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Supported models and Regions for reranking results during query",
    "content": "To refine the relevance of retrieved information, Amazon Bedrock Knowledge Bases support specific models and are available in certain Regions for reranking query results. Reranking helps improve the quality of responses by ordering the most pertinent retrieved data at the top before it's used to augment the foundation model's prompt. This ensures that the generated responses are highly relevant to the user's query.",
    "tags": [
      "reranking",
      "query optimization",
      "regions",
      "retrieval"
    ]
  },
  {
    "id": 327,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Chat with your document with zero setup",
    "content": "Amazon Bedrock offers a \"Chat with your document with zero setup\" feature, simplifying interaction with your data. This capability allows users to quickly engage with their documents via a knowledge base without extensive initial configuration. It provides an easy way to query and receive answers directly from your uploaded data sources, making it accessible for immediate exploration and use.",
    "tags": [
      "chat",
      "zero setup",
      "document interaction",
      "ease of use"
    ]
  },
  {
    "id": 328,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Set up permissions to create and manage knowledge bases",
    "content": "To create and manage Knowledge Bases in Amazon Bedrock, you must set up specific IAM permissions. This involves configuring your IAM identity with the necessary actions, such as `bedrock:CreateKnowledgeBase` and `bedrock:UpdateKnowledgeBase`, to perform operations on knowledge base resources. These permissions ensure secure access and control over your data sources and knowledge base functionalities.",
    "tags": [
      "permissions",
      "iam",
      "security",
      "access control"
    ]
  },
  {
    "id": 329,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Build a knowledge base by connecting to a data source",
    "content": "Building a knowledge base primarily involves connecting it to your data sources. This process integrates your proprietary data, such as documents or databases, so that the foundation models can use it for Retrieval Augmented Generation (RAG). Amazon Bedrock supports various data source types and provides options for setting up permissions and configuring these connections. You can create a knowledge base by connecting to sources like Amazon S3, Amazon Kendra GenAI index, or Amazon Neptune Analytics graphs.",
    "tags": [
      "data source",
      "connection",
      "rag",
      "build"
    ]
  },
  {
    "id": 330,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Prerequisites",
    "content": "Before building a Knowledge Base in Amazon Bedrock, several prerequisites must be met. These typically include having an AWS account and an IAM role with the necessary permissions for Amazon Bedrock and relevant services like Amazon S3. You must also have access to the specific foundation models you intend to use with the knowledge base. Additionally, data sources need to be prepared and made accessible.",
    "tags": [
      "prerequisites",
      "setup",
      "requirements",
      "iam"
    ]
  },
  {
    "id": 331,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Prerequisites for OpenSearch Managed Clusters",
    "content": "When building a Knowledge Base with an OpenSearch Managed Cluster as a vector store, specific prerequisites are required. This involves ensuring the OpenSearch cluster is properly configured and accessible, including networking and security settings . You must have the necessary permissions to interact with the cluster from Amazon Bedrock.",
    "tags": [
      "opensearch",
      "managed clusters",
      "vector store"
    ]
  },
  {
    "id": 332,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Create a knowledge base",
    "content": "Creating a knowledge base in Amazon Bedrock involves several steps through the console or API. You define the knowledge base, specifying its name and description, and then configure its connection to your data sources. This includes selecting the vector store and embedding model to use . Once created, the knowledge base can be populated by syncing data sources.",
    "tags": [
      "create",
      "knowledge base",
      "setup",
      "configuration"
    ]
  },
  {
    "id": 333,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Sync a data source",
    "content": "After creating a knowledge base, you need to sync a data source to populate it with your information. This process ingests the data from your specified Amazon S3 bucket or other sources into the knowledge base's vector store. Synchronization ensures that the foundation model has access to the most up-to-date information for Retrieval Augmented Generation (RAG).",
    "tags": [
      "sync",
      "data ingestion",
      "data source",
      "update"
    ]
  },
  {
    "id": 334,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Ingest changes directly into a knowledge base",
    "content": "You can ingest changes directly into a knowledge base to keep your information up-to-date without needing a full re-sync. This feature allows for incremental updates to your data sources, ensuring the knowledge base always reflects the latest information. Direct ingestion helps maintain the relevance and accuracy of responses generated through Retrieval Augmented Generation (RAG).",
    "tags": [
      "data ingestion",
      "updates",
      "real-time",
      "rag"
    ]
  },
  {
    "id": 335,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "View information about a data source",
    "content": "Amazon Bedrock allows you to view information about a data source that is connected to your knowledge base. This functionality provides insights into the status, configuration, and details of your integrated data sources. It helps in monitoring the health and content of the data that feeds into your Retrieval Augmented Generation (RAG) applications.",
    "tags": [
      "data source",
      "monitoring",
      "information",
      "view"
    ]
  },
  {
    "id": 336,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Modify a data source",
    "content": "You can modify a data source configuration after it has been connected to your knowledge base. This flexibility allows you to update settings such as the S3 bucket location, parsing options, or permissions associated with the data source. Modifying a data source ensures that your knowledge base adapts to changes in your data landscape or specific processing requirements.",
    "tags": [
      "data source",
      "modify",
      "update",
      "configuration"
    ]
  },
  {
    "id": 337,
    "service": "Amazon Bedrock",
    "category": "Knowledge Bases",
    "title": "Delete a data source",
    "content": "If a data source is no longer needed or relevant, you can delete it from your knowledge base. This action removes the connection and prevents the data from being used in future Retrieval Augmented Generation (RAG) operations. Deleting a data source helps manage resources and ensures that only current and approved information is utilized by your foundation models.",
    "tags": [
      "delete",
      "data source",
      "management",
      "cleanup"
    ]
  },
  {
    "id": 338,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Build a knowledge base by connecting to a structured data store",
    "content": "Amazon Bedrock allows building a knowledge base by connecting to a structured data store. This process involves setting up a query engine and configuring permissions to access your data. Once permissions are in place, you can create the knowledge base itself, which will then be ready to be synced with your structured data. This enables Retrieval Augmented Generation (RAG) by allowing the foundation model to query and retrieve information from your structured data sources.",
    "tags": [
      "knowledge base",
      "structured data",
      "data store",
      "rag"
    ]
  },
  {
    "id": 339,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Set up query engine and configure permissions for structured data store",
    "content": "When building a knowledge base with a structured data store, the initial step is to set up the query engine and configure necessary permissions. This involves granting Amazon Bedrock the appropriate access to interact with your data sources. Proper permission configuration ensures secure and efficient retrieval of information to augment foundation model responses. This is a crucial prerequisite before proceeding to create the knowledge base for structured data.",
    "tags": [
      "query engine",
      "permissions",
      "security",
      "structured data"
    ]
  },
  {
    "id": 340,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Create a knowledge base for structured data store",
    "content": "After setting up the query engine and configuring permissions, you can proceed to create a knowledge base connected to a structured data store. This knowledge base will enable Retrieval Augmented Generation (RAG) capabilities for your generative AI applications. By doing so, foundation models can then augment their responses with information queried and retrieved from your structured data.",
    "tags": [
      "knowledge base",
      "structured data",
      "creation",
      "rag"
    ]
  },
  {
    "id": 341,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Sync a structured data store",
    "content": "Once a knowledge base is created and connected to a structured data store, it needs to be synced. Syncing the data source ensures that the knowledge base has the most current information from your structured data. This process keeps the knowledge base up-to-date, allowing foundation models to provide accurate and relevant responses when queried. Regular syncing is vital for maintaining the effectiveness of RAG applications.",
    "tags": [
      "sync",
      "structured data",
      "update",
      "data management"
    ]
  },
  {
    "id": 342,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Build a knowledge base with an Amazon Kendra GenAI index",
    "content": "Amazon Bedrock supports building a knowledge base by integrating with an Amazon Kendra GenAI index. This integration allows you to leverage Kendra's advanced search and retrieval capabilities to populate your knowledge base. By connecting to a Kendra GenAI index, the knowledge base can augment foundation model responses with highly relevant information from your enterprise search data. This enhances the quality and context of generated AI outputs.",
    "tags": [
      "knowledge base",
      "kendra",
      "genai",
      "index"
    ]
  },
  {
    "id": 343,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Create a knowledge base for Kendra GenAI index",
    "content": "To build a knowledge base using an Amazon Kendra GenAI index, you specifically perform the create knowledge base operation. This action establishes the connection and configuration that allows your Amazon Bedrock knowledge base to utilize the data indexed by Kendra. The resulting knowledge base serves as a rich source of information for Retrieval Augmented Generation (RAG), enhancing the context for foundation models.",
    "tags": [
      "knowledge base",
      "kendra",
      "creation",
      "genai"
    ]
  },
  {
    "id": 344,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Build a knowledge base with Amazon Neptune Analytics graphs",
    "content": "Amazon Bedrock offers the capability to build a knowledge base with Amazon Neptune Analytics graphs. This approach is known as GraphRAG, allowing the foundation model to retrieve information from a graph database. By leveraging graph structures, it can provide richer contextual information and improve the quality of AI-generated responses. This method is particularly useful for highly interconnected data and complex relationships.",
    "tags": [
      "knowledge base",
      "neptune analytics",
      "graphrag",
      "graphs"
    ]
  },
  {
    "id": 345,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "GraphRAG Region availability",
    "content": "GraphRAG functionality, which allows building knowledge bases with Amazon Neptune Analytics graphs, has specific Region availability. Users should consult the documentation for the current list of supported AWS Regions where this feature can be deployed and utilized. This ensures that your infrastructure and data sources are located in a compatible Region for GraphRAG operations and optimal performance.",
    "tags": [
      "graphrag",
      "regions",
      "availability",
      "deployment"
    ]
  },
  {
    "id": 346,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Benefits of using GraphRAG",
    "content": "Using GraphRAG with Amazon Bedrock Knowledge Bases offers several benefits. It leverages the interconnected nature of graph data to provide richer context for foundation models, leading to more accurate and insightful responses. GraphRAG can improve the model's ability to reason over complex relationships in data, making it ideal for sophisticated generative AI applications. This enhances overall response quality and relevance, especially for relational data.",
    "tags": [
      "graphrag",
      "benefits",
      "context",
      "reasoning"
    ]
  },
  {
    "id": 347,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "How GraphRAG works",
    "content": "GraphRAG integrates Amazon Neptune Analytics graphs with Amazon Bedrock's knowledge bases. It works by using the graph data as a source for Retrieval Augmented Generation (RAG). When a query is made, relevant information is retrieved from the graph database and used to augment the prompt provided to the foundation model. This allows the model to generate responses grounded in the factual and relational data within the graph.",
    "tags": [
      "graphrag",
      "how it works",
      "neptune",
      "rag"
    ]
  },
  {
    "id": 348,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "GraphRAG considerations and limitations",
    "content": "While beneficial, GraphRAG with Amazon Bedrock also has considerations and limitations. These might include specific regional availability, data formatting requirements, and potential complexities in graph data modeling. Users should review these factors to ensure compatibility with their use cases and to optimize performance. Understanding these aspects is key to successful implementation of GraphRAG, especially for complex datasets.",
    "tags": [
      "graphrag",
      "limitations",
      "considerations",
      "planning"
    ]
  },
  {
    "id": 349,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Create a knowledge base for Neptune Analytics graphs",
    "content": "To integrate graph data, you specifically create a knowledge base that connects to Amazon Neptune Analytics graphs. This setup enables GraphRAG, providing a powerful mechanism for foundation models to access and reason over structured, interconnected data. By creating this type of knowledge base, your generative AI application can leverage rich contextual information from your graph database to generate more accurate responses.",
    "tags": [
      "knowledge base",
      "neptune",
      "creation",
      "graphrag"
    ]
  },
  {
    "id": 350,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Test your knowledge base with queries and responses",
    "content": "After building a knowledge base, it's essential to test it with queries and responses to ensure optimal performance. This involves submitting various queries to evaluate how effectively the knowledge base retrieves relevant data and how accurately the foundation model generates responses based on that data. Testing helps in refining the knowledge base and associated models for improved Retrieval Augmented Generation (RAG) outcomes.",
    "tags": [
      "test",
      "knowledge base",
      "queries",
      "responses"
    ]
  },
  {
    "id": 351,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Query a knowledge base and retrieve data",
    "content": "A core function of an Amazon Bedrock knowledge base is to query and retrieve data. When a user submits a prompt, the knowledge base searches its connected data sources for relevant information. This retrieved information is then used to augment the prompt, providing the foundation model with richer context. This process directly supports Retrieval Augmented Generation (RAG) to enhance the quality of generated responses.",
    "tags": [
      "query",
      "retrieve data",
      "knowledge base",
      "rag"
    ]
  },
  {
    "id": 352,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Query a knowledge base and generate responses",
    "content": "Beyond just retrieving data, the ultimate goal is to query a knowledge base and generate responses. This involves the foundation model utilizing the context provided by the retrieved information to produce a more informed and accurate output. This process, central to Retrieval Augmented Generation (RAG), allows applications to provide answers that are grounded in specific data sources rather than general training knowledge.",
    "tags": [
      "generate responses",
      "knowledge base",
      "rag",
      "fms"
    ]
  },
  {
    "id": 353,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Generate a query for structured data",
    "content": "When working with a knowledge base connected to a structured data store, it's possible to generate a query for structured data. This capability allows the system to formulate precise queries that can extract specific information from your organized datasets. By generating tailored queries, the knowledge base ensures efficient and accurate retrieval, which is critical for providing factually correct responses through Retrieval Augmented Generation (RAG).",
    "tags": [
      "structured data",
      "generate query",
      "data retrieval",
      "rag"
    ]
  },
  {
    "id": 354,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Query a knowledge base connected to an Amazon Kendra GenAI index",
    "content": "To leverage an Amazon Kendra GenAI index within a knowledge base, you can query it specifically. This allows the knowledge base to tap into Kendra's enterprise search capabilities for highly relevant document and information retrieval. When querying, the knowledge base uses Kendra to find pertinent context, which then augments the foundation model's prompt for generating well-informed responses.",
    "tags": [
      "query",
      "kendra",
      "genai",
      "knowledge base"
    ]
  },
  {
    "id": 355,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Query a knowledge base connected to an Amazon Neptune Analytics graph",
    "content": "When a knowledge base is built with an Amazon Neptune Analytics graph, you can query this graph to retrieve interconnected data. This enables GraphRAG, where the graph's relational context significantly enhances the information used to augment the foundation model's prompt. Querying a graph-backed knowledge base is particularly effective for questions requiring a deep understanding of relationships and complex entity interactions.",
    "tags": [
      "query",
      "neptune",
      "graphrag",
      "relationships"
    ]
  },
  {
    "id": 356,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Configure and customize queries and responses",
    "content": "Amazon Bedrock allows you to configure and customize queries and responses for your knowledge base. This involves fine-tuning how queries are processed and how the foundation model generates its output. Customization can include setting parameters for relevance, length, and style of responses, ensuring they meet specific application requirements. This flexibility helps optimize the user experience and the utility of the AI.",
    "tags": [
      "customize",
      "queries",
      "responses",
      "configuration"
    ]
  },
  {
    "id": 357,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Configure responses for reasoning models",
    "content": "For models capable of reasoning, you can configure responses within your knowledge base setup. This involves tailoring the output generation to better align with the analytical capabilities of these specific models. Configuring reasoning models may involve adjusting parameters to encourage more step-by-step explanations or logical deductions, enhancing the clarity and depth of the AI's answers. This supports more sophisticated AI applications.",
    "tags": [
      "reasoning models",
      "configure responses",
      "logic",
      "fms"
    ]
  },
  {
    "id": 358,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Deploy your knowledge base for your application",
    "content": "After development and testing, you can deploy your knowledge base for your application. This involves making it accessible for your generative AI applications to utilize for Retrieval Augmented Generation (RAG). Deployment ensures that your applications can continuously leverage the knowledge base to provide contextually rich and accurate responses to users. This step transitions the knowledge base from development to operational use.",
    "tags": [
      "deploy",
      "knowledge base",
      "application",
      "integration"
    ]
  },
  {
    "id": 359,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "View information about a knowledge base",
    "content": "You can view information about a knowledge base through the Amazon Bedrock console or API. This allows you to inspect its configuration, connected data sources, and other relevant details. Accessing this information is crucial for monitoring, managing, and understanding the operational status and capabilities of your deployed knowledge bases. It helps in ensuring that the knowledge base is functioning as expected and troubleshooting issues.",
    "tags": [
      "view",
      "information",
      "knowledge base",
      "management"
    ]
  },
  {
    "id": 360,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Modify a knowledge base",
    "content": "Amazon Bedrock allows you to modify a knowledge base after its creation. This includes updating its configuration, changing connected data sources, or adjusting other parameters to improve performance or adapt to new requirements. Modifying a knowledge base is an essential part of its lifecycle, enabling continuous improvement and ensuring it remains relevant to your evolving generative AI applications.",
    "tags": [
      "modify",
      "knowledge base",
      "update",
      "configuration"
    ]
  },
  {
    "id": 361,
    "service": "Amazon Bedrock",
    "category": "Knowledge Base",
    "title": "Delete a knowledge base",
    "content": "When a knowledge base is no longer needed, you have the option to delete it. This action removes the knowledge base and disassociates it from your generative AI applications. Deleting a knowledge base helps in managing resources and cleaning up environments when projects conclude or requirements change. This ensures efficient resource utilization and avoids unnecessary costs and data retention.",
    "tags": [
      "delete",
      "knowledge base",
      "cleanup",
      "resource management"
    ]
  },
  {
    "id": 362,
    "service": "Amazon Bedrock",
    "category": "Reranker Models",
    "title": "Reranker Models: Improve Response Relevance",
    "content": "Reranker models are designed to significantly improve the relevance of responses generated by generative AI applications, especially in scenarios involving retrieval. They work by reordering or scoring retrieved information to ensure that the most pertinent data is presented to the foundation model or directly to the user. This enhancement is crucial for applications that rely on external data sources, such as knowledge bases, to provide accurate and contextually appropriate answers. By refining the order of retrieved results, reranker models help deliver higher-quality and more focused outputs, enhancing overall application efficiency.",
    "tags": [
      "reranker models",
      "response relevance",
      "generative ai",
      "knowledge bases",
      "retrieval augmented generation"
    ]
  },
  {
    "id": 363,
    "service": "Amazon Bedrock",
    "category": "Reranker Models",
    "title": "Supported Regions and Models for Reranker Models",
    "content": "Reranker models on Amazon Bedrock are supported in specific AWS Regions, including US West (Oregon), Asia Pacific (Tokyo), Canada (Central), and Europe (Frankfurt). The available models include Amazon Rerank 1.0 (model ID: `amazon.rerank-v1:0`) and Cohere Rerank 3.5 (model ID: `cohere.rerank-v3-5:0`). These models are designed to process text input and produce text outputs, enhancing the precision of information retrieval for generative AI applications.",
    "tags": [
      "supported regions",
      "rerank models",
      "amazon rerank",
      "cohere rerank",
      "model availability"
    ]
  },
  {
    "id": 364,
    "service": "Amazon Bedrock",
    "category": "Reranker Models",
    "title": "Permissions for Reranker Models",
    "content": "To utilize reranker models within Amazon Bedrock, appropriate IAM permissions are necessary to allow your role or user to perform model invocation actions. Specifically, actions such as `bedrock:InvokeModel` and `bedrock:InvokeModelWithResponseStream` are required to send requests to and receive responses from these models. These permissions ensure that your application can securely interact with the reranker service and integrate its capabilities into your generative AI workflows, safeguarding access to foundation models and other Bedrock resources.",
    "tags": [
      "permissions",
      "iam",
      "invokemodel",
      "access control",
      "security"
    ]
  },
  {
    "id": 365,
    "service": "Amazon Bedrock",
    "category": "Reranker Models",
    "title": "How to Use a Reranker Model",
    "content": "To effectively use a reranker model, it is integrated into a workflow where information retrieval is critical, such as with knowledge bases. After an initial retrieval of documents or passages in response to a user query, the reranker model is invoked to re-score and reorder these results based on their relevance. The refined, highly relevant information is then typically passed to a foundation model to generate a more accurate and contextually grounded final response. This process is generally performed via API calls like `InvokeModel` or `Converse`.",
    "tags": [
      "use reranker",
      "workflow",
      "model invocation",
      "api calls",
      "information retrieval"
    ]
  },
  {
    "id": 366,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Agents: Automate tasks",
    "content": "Amazon Bedrock allows you to build agents that use foundation models (FMs) to perform tasks, make API calls, and optionally query knowledge bases for customers. Agents automate complex workflows by reasoning through how to help a customer and carrying out the necessary steps. This includes build-time configuration and a runtime process. You can call agents from Python code after deploying them with an alias.",
    "tags": [
      "agents",
      "automation",
      "foundation models",
      "api",
      "knowledge bases"
    ]
  },
  {
    "id": 367,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "How Amazon Bedrock Agents work",
    "content": "Amazon Bedrock Agents function through a two-phase process: build-time configuration and runtime process. During build-time, you define the agent's capabilities and how it interacts with external systems. The runtime process involves the agent receiving a request, reasoning through the steps required, and executing actions like making API calls or querying knowledge bases to fulfill the task.",
    "tags": [
      "agents",
      "workflow",
      "build-time",
      "runtime"
    ]
  },
  {
    "id": 368,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Build-time configuration for Agents",
    "content": "The build-time configuration for Amazon Bedrock Agents is where you set up the agent's initial structure and capabilities. This involves defining the foundation model it will use, configuring action groups to specify the tools and APIs it can access, and setting up any initial instructions or knowledge bases. This phase essentially establishes the agent's understanding of its environment and the tasks it can perform before it goes live.",
    "tags": [
      "agents",
      "configuration",
      "action groups",
      "knowledge bases"
    ]
  },
  {
    "id": 369,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Runtime process for Agents",
    "content": "The runtime process for Amazon Bedrock Agents describes how the agent operates once deployed. When a user sends a query, the agent uses its foundation model to understand the request, plans a sequence of actions, and executes those actions. These actions can involve invoking APIs defined in action groups or retrieving information from knowledge bases, ultimately generating a coherent response to the user.",
    "tags": [
      "agents",
      "execution",
      "api calls",
      "response generation"
    ]
  },
  {
    "id": 370,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Supported Regions for Agents",
    "content": "Amazon Bedrock Agents are supported in specific AWS Regions. While the sources do not provide a comprehensive list of supported regions in this section, they indicate that regional support is a factor for agent deployment and usage. This means that when building and deploying agents, users need to ensure their chosen region supports the Agents feature.",
    "tags": [
      "agents",
      "regions",
      "availability"
    ]
  },
  {
    "id": 371,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Tutorial: Building a simple agent",
    "content": "Amazon Bedrock provides a tutorial to guide users through building a simple agent. This process starts with prerequisites, followed by creating a Lambda function and then the Amazon Bedrock agent itself. The tutorial then covers testing, deploying with an alias, and calling the agent from Python code, concluding with steps for cleaning up resources.",
    "tags": [
      "agents",
      "tutorial",
      "lambda",
      "python"
    ]
  },
  {
    "id": 372,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Prerequisites for building a simple agent",
    "content": "Before building a simple Amazon Bedrock agent, users must fulfill certain prerequisites. These typically involve having an AWS account and ensuring the necessary IAM permissions are in place for Amazon Bedrock. Specific to the agent tutorial, this includes setting up permissions that allow for the creation of Lambda functions and the agent itself, forming the foundational access required for the subsequent steps.",
    "tags": [
      "agents",
      "prerequisites",
      "iam",
      "aws account"
    ]
  },
  {
    "id": 373,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Step 1: Create a Lambda function",
    "content": "The first step in building a simple Amazon Bedrock agent involves creating a Lambda function. This function typically defines the backend logic or action group that the agent will be able to invoke. It's a crucial component for enabling the agent to perform specific tasks or interact with external services, setting up the practical actions the agent can take.",
    "tags": [
      "agents",
      "lambda",
      "action group",
      "function"
    ]
  },
  {
    "id": 374,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Step 2: Create an Amazon Bedrock agent",
    "content": "After setting up the necessary Lambda function, the next step is to create the Amazon Bedrock agent. This involves configuring the agent within the Amazon Bedrock service, linking it to its underlying foundation model, and associating it with the previously created Lambda function as an action group. This step formalizes the agent's identity and its initial capabilities within the Bedrock ecosystem.",
    "tags": [
      "agents",
      "creation",
      "foundation model",
      "action group"
    ]
  },
  {
    "id": 375,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Step 3: Test the agent",
    "content": "Once an Amazon Bedrock agent is created, it's essential to test the agent to ensure it functions as expected. This involves sending sample queries or prompts to the agent to verify its ability to understand requests, invoke the correct action groups (like Lambda functions), and generate appropriate responses. Testing helps identify and resolve any issues before deploying the agent for broader use.",
    "tags": [
      "agents",
      "testing",
      "queries",
      "action groups"
    ]
  },
  {
    "id": 376,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Step 4: Deploy the agent with an alias",
    "content": "To make an Amazon Bedrock agent accessible for use in applications, it needs to be deployed with an alias. An alias provides a stable, user-friendly name or identifier for the agent, allowing applications to interact with it consistently. This deployment step finalizes the agent's readiness for integration into various services or client applications.",
    "tags": [
      "agents",
      "deployment",
      "alias",
      "integration"
    ]
  },
  {
    "id": 377,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Step 5: Call the agent from Python code",
    "content": "After deploying an Amazon Bedrock agent with an alias, developers can call the agent from Python code. This typically involves using the AWS SDK for Python (Boto3) to interact with the agent's API endpoint, sending requests and receiving responses programmatically. This step demonstrates how to integrate the agent's functionalities into custom applications using a popular programming language.",
    "tags": [
      "agents",
      "python",
      "boto3",
      "api call"
    ]
  },
  {
    "id": 378,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Step 6: Clean up resources",
    "content": "As a final step in the agent tutorial, it is recommended to clean up resources. This involves deleting the Amazon Bedrock agent, associated Lambda functions, and any other temporary AWS resources created during the tutorial. Cleaning up helps avoid unnecessary charges and maintains an organized AWS environment.",
    "tags": [
      "agents",
      "cleanup",
      "resources",
      "aws"
    ]
  },
  {
    "id": 379,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Additional resources for Agents",
    "content": "For users seeking more in-depth information or further assistance with Amazon Bedrock Agents, additional resources are available. These resources can include comprehensive documentation, advanced tutorials, or community forums to help with more complex agent development and deployment scenarios. They serve as valuable references for continued learning and troubleshooting beyond the basic setup.",
    "tags": [
      "agents",
      "resources",
      "documentation",
      "support"
    ]
  },
  {
    "id": 380,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Building and modifying agents",
    "content": "Amazon Bedrock enables users to build and modify agents to create applications that can reason and carry out tasks for customers. This involves configuring foundation models to make API calls and (optionally) query knowledge bases. The process allows for iterative development, where agents can be continually refined to improve their performance on specific tasks or integrate new functionalities.",
    "tags": [
      "agents",
      "development",
      "customization",
      "application"
    ]
  },
  {
    "id": 381,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Manual agent creation and configuration",
    "content": "Users can create and configure Amazon Bedrock agents by following a structured process. This includes setting up prerequisites like IAM permissions, creating Lambda functions for actions, and then defining the agent itself within the Amazon Bedrock console or via API calls. Manual configuration allows precise control over the agent's foundation model, action groups, and overall behavior.",
    "tags": [
      "agents",
      "manual setup",
      "configuration",
      "api"
    ]
  },
  {
    "id": 382,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Viewing agent information",
    "content": "To view information about an Amazon Bedrock agent, users can access details regarding its configuration and status. This includes reviewing its associated foundation model, action groups, and deployment status. Such information is crucial for monitoring agent performance, debugging issues, and understanding its operational parameters within the Amazon Bedrock environment.",
    "tags": [
      "agents",
      "monitoring",
      "details",
      "status"
    ]
  },
  {
    "id": 383,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Modifying an Amazon Bedrock agent",
    "content": "Amazon Bedrock allows users to modify an agent after its initial creation, adapting it to evolving application requirements or improving its performance. This can involve updating its foundation model, adjusting its action groups, or refining its instructions and access to knowledge bases. Modifications ensure the agent remains relevant and effective in automating tasks and generating desired responses for customers.",
    "tags": [
      "agents",
      "updates",
      "refinement",
      "customization"
    ]
  },
  {
    "id": 384,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Deleting an Amazon Bedrock agent",
    "content": "To delete an Amazon Bedrock agent, users follow specific steps to remove the agent and its associated resources. This is part of the clean up resources process in the agent tutorial, ensuring that deployed agents and their configurations are fully decommissioned. Deleting agents helps manage costs and keeps the AWS environment tidy by removing unnecessary components.",
    "tags": [
      "agents",
      "deletion",
      "cleanup",
      "decommission"
    ]
  },
  {
    "id": 385,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Use action groups to define actions for your agent",
    "content": "Action groups are a key component for Amazon Bedrock Agents, allowing you to define actions that your agent can perform. These groups specify the APIs or Lambda functions that the agent can invoke to interact with external systems or retrieve specific information. By organizing capabilities into action groups, you provide the agent with a structured way to execute tasks and extend its functionality beyond the foundation model itself.",
    "tags": [
      "action groups",
      "agents",
      "api",
      "lambda"
    ]
  },
  {
    "id": 386,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Define actions in the action group",
    "content": "Within an action group, you define specific actions that the Amazon Bedrock agent can execute. Each action corresponds to an API call or a Lambda function that performs a particular task, such as fetching data or initiating a process. This definition typically includes details like the function name, its description, and the input/output schema, allowing the agent to understand how to use the tool effectively.",
    "tags": [
      "action group",
      "api definition",
      "function schema"
    ]
  },
  {
    "id": 387,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Handle fulfillment of the action",
    "content": "For Amazon Bedrock Agents, handling fulfillment of an action involves the agent executing the defined steps to complete a user's request. Once an agent determines it needs to use an action group, it makes the appropriate API call or invokes the Lambda function. The system then processes the response from that action and integrates it back into the agent's reasoning to formulate the final answer to the user.",
    "tags": [
      "action fulfillment",
      "api call",
      "lambda",
      "response"
    ]
  },
  {
    "id": 388,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Add an action group to your agent",
    "content": "To enhance an Amazon Bedrock agent's capabilities, you add an action group to it during its configuration. This step involves associating a pre-defined action group, which contains a set of callable APIs or Lambda functions, with the agent. Adding action groups allows the agent to interact with external systems and perform a wider range of specific tasks necessary for its assigned use cases.",
    "tags": [
      "action group",
      "agent configuration",
      "api",
      "lambda"
    ]
  },
  {
    "id": 389,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Viewing action group information",
    "content": "Users can view information about an action group associated with an Amazon Bedrock Agent to understand its defined capabilities. This includes reviewing the names and descriptions of the actions it contains, along with their respective input and output schemas. Accessing this information helps in managing, debugging, and ensuring that the agent has the correct tools to perform its tasks effectively.",
    "tags": [
      "action group",
      "details",
      "management",
      "schema"
    ]
  },
  {
    "id": 390,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Modifying an action group",
    "content": "The functionality to modify an action group allows users to update or change the definitions of actions available to an Amazon Bedrock Agent. This could involve updating API endpoints, altering input/output parameters, or adding new Lambda functions to an existing group. Modifying action groups is essential for adapting agents to new functionalities or changes in external service integrations.",
    "tags": [
      "action group",
      "update",
      "api",
      "lambda"
    ]
  },
  {
    "id": 391,
    "service": "Amazon Bedrock",
    "category": "Agents: Automate tasks",
    "title": "Deleting an action group",
    "content": "To streamline agent management, users can delete an action group that is no longer needed by an Amazon Bedrock Agent. This process removes the definitions of specific APIs or Lambda functions that the agent previously could invoke. Deleting action groups helps in simplifying an agent's capabilities, removing deprecated integrations, and maintaining a clean configuration.",
    "tags": [
      "action group",
      "removal",
      "cleanup",
      "management"
    ]
  },
  {
    "id": 392,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Use multi-agent collaboration for complex tasks",
    "content": "The provided sources describe Amazon Bedrock's capabilities for building agents that execute tasks using foundation models, API calls, and optionally querying knowledge bases. However, the sources do not contain specific information or features related to multi-agent collaboration or the use of 'collaborator agents' for complex tasks. The documentation focuses on individual agent capabilities for reasoning and carrying out tasks. Therefore, information on direct multi-agent collaboration is not detailed in the provided materials.",
    "tags": [
      "agents",
      "collaboration",
      "tasks",
      "generative ai"
    ]
  },
  {
    "id": 393,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Supported Regions and models for multi-agent collaboration",
    "content": "The provided sources do not contain specific information regarding Supported Regions and models for multi-agent collaboration. While Amazon Bedrock supports Agents and lists regions and models for various features, there is no explicit mention or details about a 'multi-agent collaboration' feature or its specific regional and model availability. Therefore, information on this particular topic is not present in the given materials.",
    "tags": [
      "agents",
      "regions",
      "models",
      "collaboration"
    ]
  },
  {
    "id": 394,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Create multi-agent collaboration",
    "content": "The provided sources describe how to create an Amazon Bedrock agent and outline its capabilities, such as using foundation models, making API calls, and querying knowledge bases. However, there is no specific information or guidance on how to create multi-agent collaboration within Amazon Bedrock. The documentation focuses on configuring and deploying individual agents to perform tasks. Therefore, steps or APIs for establishing multi-agent collaboration are not detailed in the provided materials.",
    "tags": [
      "agents",
      "create",
      "collaboration",
      "api"
    ]
  },
  {
    "id": 395,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Disassociate collaborator agent",
    "content": "The provided sources detail operations related to managing individual Amazon Bedrock agents, such as creating, deploying, and deleting agents. However, there is no explicit information on a feature called 'collaborator agent' or instructions on how to disassociate a collaborator agent from any multi-agent collaboration setup. The concept of multi-agent collaboration is not described in the given materials. Therefore, this specific information is not available in the sources.",
    "tags": [
      "agents",
      "disassociate",
      "collaboration",
      "management"
    ]
  },
  {
    "id": 396,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Disable a multi-agent collaboration",
    "content": "The provided sources describe the functionality and management of Amazon Bedrock agents, including how to deploy and clean up agent resources. However, there is no specific information regarding a 'multi-agent collaboration' feature or instructions on how to disable a multi-agent collaboration. The documentation focuses on the lifecycle and operations of single agents. Consequently, details on disabling such a collaboration are not present in the provided materials.",
    "tags": [
      "agents",
      "disable",
      "collaboration",
      "management"
    ]
  },
  {
    "id": 397,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Configure agent to request information from user",
    "content": "While Amazon Bedrock Agents are designed to carry out tasks based on user messages and generate responses, the provided sources do not explicitly detail how to configure an agent to proactively request information from a user. Agents process prompts and can use tools to gather data for their responses. The documentation mentions controlling agent session context and customizing agent orchestration, but specific mechanisms for agents to actively prompt users for missing information are not outlined. User input is generally expected as part of the initial message.",
    "tags": [
      "agents",
      "user input",
      "configuration",
      "interaction"
    ]
  },
  {
    "id": 398,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Enable user input",
    "content": "The provided sources indicate that Amazon Bedrock agents inherently process user inputs as part of their function to reason through and carry out tasks. Users submit messages to the agent, which then generates responses. However, there is no explicit feature or instruction described for 'enabling user input' as a distinct configuration step. The agent's ability to receive and act upon user messages is fundamental to its operation. Therefore, specific details on how to 'enable' it are not found, as it's assumed to be part of the agent's core interaction model.",
    "tags": [
      "agents",
      "user input",
      "enable",
      "interaction"
    ]
  },
  {
    "id": 399,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Disable user input",
    "content": "The provided sources describe how Amazon Bedrock agents function by receiving and responding to user messages as input. However, there is no explicit feature or instruction detailed for disabling user input for an agent. The agent's interaction model is built around processing user queries to perform tasks. While sessions can be ended or deleted, completely disabling an agent from receiving any form of user input is not a specified configuration in the provided materials.",
    "tags": [
      "agents",
      "user input",
      "disable",
      "interaction"
    ]
  },
  {
    "id": 400,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Augment response generation for your agent with knowledge base",
    "content": "Amazon Bedrock allows agents to augment their response generation by querying knowledge bases. Agents can use foundation models, make API calls, and (optionally) query these knowledge bases to reason through and carry out tasks. This process is known as Retrieval Augmented Generation (RAG), where information is retrieved from a data source, used to augment a prompt for context, and then a better response is obtained from the foundation model. Knowledge bases are also integrated into agent workflows, enabling agents to retrieve data and generate AI responses.",
    "tags": [
      "agents",
      "knowledge base",
      "rag",
      "response generation"
    ]
  },
  {
    "id": 401,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "View information about an agent-knowledge base association",
    "content": "The provided sources explain that Amazon Bedrock agents can query knowledge bases to augment their responses. While there are details on building a knowledge base and creating an agent, and associating a guardrail with a knowledge base node in a flow, the sources do not explicitly outline a method or API operation to view information specifically about an agent-knowledge base association. The general information about agents or knowledge bases might imply their connections, but a dedicated 'view association' feature is not described.",
    "tags": [
      "agents",
      "knowledge base",
      "association",
      "view"
    ]
  },
  {
    "id": 402,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Modify an agent-knowledge base association",
    "content": "The provided sources highlight that Amazon Bedrock agents can be configured to interact with knowledge bases for enhanced response generation. While the process of creating agents and knowledge bases is detailed, and modifying an agent involves updating its configuration, there is no explicit information on how to modify an agent-knowledge base association directly. The sources do not provide specific API calls or console steps for changing an existing link between an agent and a knowledge base after its initial setup. Therefore, such direct modification steps are not available.",
    "tags": [
      "agents",
      "knowledge base",
      "association",
      "modify"
    ]
  },
  {
    "id": 403,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Disassociate a knowledge base from an agent",
    "content": "The provided sources describe how Amazon Bedrock agents can leverage knowledge bases for augmenting responses. Although there's information on creating agents and knowledge bases, the sources do not provide explicit instructions or API operations on how to disassociate a knowledge base from an agent. While deleting an agent or knowledge base would implicitly break the connection, a specific 'disassociate' action for the linkage itself is not detailed in the provided materials.",
    "tags": [
      "agents",
      "knowledge base",
      "disassociate",
      "management"
    ]
  },
  {
    "id": 404,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Retain conversational context using memory",
    "content": "Amazon Bedrock Agents are designed to retain conversational context through sessions, which serve as a memory mechanism. Agents can store conversation history and context in a session to enable coherent multi-turn interactions. This stored information can then be retrieved from a session in subsequent requests, ensuring the agent maintains continuity in dialogue. Sessions can be managed, including storing and retrieving conversation data, and can be ended or deleted when no longer needed.",
    "tags": [
      "agents",
      "memory",
      "conversational context",
      "sessions",
      "history"
    ]
  },
  {
    "id": 405,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Enable agent memory",
    "content": "To enable memory for an Amazon Bedrock agent, you must create a session to store conversational context. This involves initiating a session where the agent can store the conversation history and context for multi-turn interactions. The process of creating a session inherently activates the agent's ability to retain and access past interactions. Sessions can then be used to manage and retrieve this stored memory.",
    "tags": [
      "agents",
      "memory",
      "enable",
      "sessions",
      "conversation history"
    ]
  },
  {
    "id": 406,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "View memory sessions",
    "content": "To view memory sessions for an Amazon Bedrock agent, you can retrieve conversation history and context from a session. This action allows you to access the stored interactions and contextual information that the agent has retained from previous turns. Additionally, CloudWatch Metrics can be utilized to monitor agents, providing operational insights, although this may not directly show the conversational content of individual sessions. The focus for viewing actual memory content is through direct retrieval from sessions.",
    "tags": [
      "agents",
      "memory",
      "sessions",
      "view",
      "conversation history"
    ]
  },
  {
    "id": 407,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Delete session summaries",
    "content": "You can delete session summaries and all associated data for an Amazon Bedrock agent by using the Delete a session and all of its data operation. This action permanently removes the stored conversation history and any other context associated with a specific session. The BedrockSessionSaver LangGraph library can also be used to manage and delete these sessions, providing programmatic control over memory retention. This capability is crucial for managing data lifecycle and privacy for agent interactions.",
    "tags": [
      "agents",
      "memory",
      "sessions",
      "delete",
      "summaries"
    ]
  },
  {
    "id": 408,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Disable agent memory",
    "content": "While there is no explicit 'disable agent memory' feature described in the provided sources, you can effectively disable memory for an Amazon Bedrock agent by choosing to delete a session and all of its data. This action removes the stored conversation history and context, preventing the agent from accessing past interactions for that particular session. Alternatively, by not creating new sessions or ending existing ones, memory retention for future conversations can be prevented.",
    "tags": [
      "agents",
      "memory",
      "disable",
      "sessions",
      "data management"
    ]
  },
  {
    "id": 409,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Sessions: Store and Retrieve Conversations",
    "content": "Sessions in Amazon Bedrock are utilized by agents to effectively store and retrieve conversation history and context. This functionality is crucial for building generative AI applications that require a continuous and coherent dialogue across multiple turns. By remembering previous interactions, agents can deliver more natural and relevant responses, essential for tasks like assisting customers. This approach ensures that the application maintains a consistent understanding of the ongoing conversation.",
    "tags": [
      "sessions",
      "conversations",
      "agents",
      "context"
    ]
  },
  {
    "id": 410,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Sessions Use Case Example",
    "content": "A compelling use case example for sessions involves agents powered by foundation models that execute tasks and can optionally query knowledge bases. These agents need to maintain a state to reason through and carry out tasks for customers effectively. Sessions allow the agent to preserve the conversation history and context over numerous interactions, enabling the agent to provide consistent and relevant assistance by recalling past turns.",
    "tags": [
      "use case",
      "agents",
      "foundation models",
      "knowledge bases"
    ]
  },
  {
    "id": 411,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Sessions Workflow",
    "content": "The workflow for managing sessions in Amazon Bedrock involves several critical steps to ensure conversational continuity. It typically includes initiating a session, diligently storing the conversation history and context within it, and subsequently retrieving this information as needed. The lifecycle also encompasses actions such as ending a session when the user concludes the conversation or deleting its data when it's no longer necessary, promoting efficient application operation.",
    "tags": [
      "workflow",
      "lifecycle",
      "history",
      "context"
    ]
  },
  {
    "id": 412,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Sessions Considerations",
    "content": "Key considerations when implementing sessions include addressing security and robust data management practices. Understanding the mechanisms for storing and retrieving conversation history and context is paramount for reliable application performance. Developers must also plan for the complete session lifecycle, covering creation, storage, retrieval, ending, and deletion, to ensure optimal and secure operation of conversational applications.",
    "tags": [
      "security",
      "data management",
      "lifecycle",
      "best practices"
    ]
  },
  {
    "id": 413,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Session Encryption",
    "content": "Session encryption is a vital component for safeguarding the confidentiality and integrity of conversational data within Amazon Bedrock. It specifically pertains to protecting the conversation history and context that is stored throughout a session. While the sources highlight session encryption as an important consideration, explicit details on its implementation are not provided. Therefore, developers should integrate additional security measures and adhere to AWS security best practices for protecting sensitive information.",
    "tags": [
      "encryption",
      "security",
      "data protection",
      "privacy"
    ]
  },
  {
    "id": 414,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Create a Session",
    "content": "To initiate a new conversational interaction, the first step is to create a session. This action establishes a unique and independent context for the dialogue within Amazon Bedrock. Once successfully created, this session then serves as the container for storing and managing the subsequent conversation history and context. It provides the necessary identifier that links all future user inputs and model responses to that specific, ongoing conversation.",
    "tags": [
      "create session",
      "initiate",
      "conversation",
      "context"
    ]
  },
  {
    "id": 415,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Store Conversation History and Context in a Session",
    "content": "Once a session is active, the system can store conversation history and context in a session. This involves diligently saving all user queries, model responses, and any other relevant contextual information as the conversation progresses. This persistent storage enables the agent or application to maintain a comprehensive understanding of the dialogue across multiple turns. Consequently, subsequent interactions are well-informed by previous exchanges, leading to more coherent and effective responses from the model.",
    "tags": [
      "store data",
      "history",
      "context",
      "conversation"
    ]
  },
  {
    "id": 416,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "CreateInvocation Example",
    "content": "The sources refer to a 'CreateInvocation example' as part of the session management functionalities. Although a specific code snippet is not detailed, this indicates that CreateInvocation is a programmatic operation relevant to interactions within a session. It likely facilitates initiating or managing specific model calls or actions, contributing to the overall conversational flow and task execution within an active session.",
    "tags": [
      "createinvocation",
      "api",
      "code example",
      "invocation"
    ]
  },
  {
    "id": 417,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "PutInvocationSteps Example",
    "content": "Similar to `CreateInvocation`, the 'PutInvocationSteps example' is mentioned within the context of session management. This suggests that PutInvocationSteps is a programmatic API call designed to record or update the various steps undertaken during an invocation process within a session. It helps in detailing the progress or specific actions performed by an agent as it works towards completing a user's task, contributing to a traceable conversational state.",
    "tags": [
      "putinvocationsteps",
      "api",
      "code example",
      "invocation steps"
    ]
  },
  {
    "id": 418,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Retrieve Conversation History and Context from a Session",
    "content": "To support continuous and context-aware dialogue, it is fundamental to retrieve conversation history and context from a session. This process allows the generative AI application to access all previously stored messages, user inputs, and relevant contextual data pertinent to the ongoing conversation. By effectively retrieving this information, the model can generate more informed, relevant, and coherent responses for subsequent turns, which is essential for maintaining the flow of multi-turn conversational experiences.",
    "tags": [
      "retrieve",
      "history",
      "context",
      "conversation",
      "data access"
    ]
  },
  {
    "id": 419,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "End a Session When the User Ends the Conversation",
    "content": "It is considered a best practice to explicitly end a session when the user ends the conversation. This action is important for proper resource management and ensures that all conversational data associated with that specific interaction is formally closed. Concluding a session indicates that no further updates or processing are expected for that dialogue, contributing to efficient system operation and maintaining data hygiene within Amazon Bedrock.",
    "tags": [
      "end session",
      "session management",
      "resource management",
      "data hygiene"
    ]
  },
  {
    "id": 420,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Delete a Session and All of Its Data",
    "content": "For purposes of data governance, compliance, or user privacy, it often becomes necessary to delete a session and all of its data. This operation executes the permanent removal of the entire conversation history and context that was associated with a specific session. By deleting session data, organizations can ensure that no sensitive or otherwise unnecessary information persists beyond its designated retention period, thereby adhering to strict data management policies.",
    "tags": [
      "delete session",
      "data deletion",
      "privacy",
      "compliance"
    ]
  },
  {
    "id": 421,
    "service": "Amazon Bedrock",
    "category": "Sessions",
    "title": "Manage Sessions with BedrockSessionSaver LangGraph Library",
    "content": "To facilitate the robust and efficient management of sessions, developers can leverage the BedrockSessionSaver LangGraph library. This specialized library provides a streamlined approach for programmatically handling various aspects of session data, including its storage, retrieval, and automated refreshing. It is designed to integrate with LangGraph and offers crucial utilities, such as the automatic refresh of short-term Amazon Bedrock API keys, which are essential for maintaining secure and uninterrupted conversational sessions.",
    "tags": [
      "session management",
      "langgraph",
      "library",
      "api keys",
      "automation"
    ]
  },
  {
    "id": 422,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Flows: Build a generative AI workflow",
    "content": "Amazon Bedrock Flows allow you to build generative AI workflows that can orchestrate interactions between various components like large language models, logic, and other Amazon Bedrock features. These flows enable multi-step reasoning and complex task execution, helping developers integrate generative AI into applications with structured processes. They provide a visual way to design, test, and deploy generative AI applications, offering a robust framework for managing complex AI operations. Flows can incorporate elements like prompts, knowledge bases, and agents, streamlining the creation of sophisticated AI solutions.",
    "tags": [
      "generative ai",
      "workflow",
      "orchestration",
      "bedrock"
    ]
  },
  {
    "id": 423,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "How it works",
    "content": "Amazon Bedrock Flows operate by defining a sequence of nodes that represent different steps in a generative AI workflow. Each node performs a specific action, such as executing a prompt, calling a knowledge base, or applying conditional logic. The flow processes inputs through these connected nodes, transforming and enriching data at each stage to achieve a desired output. This orchestration allows for complex, multi-turn interactions and dynamic decision-making within an AI application.",
    "tags": [
      "workflow",
      "nodes",
      "orchestration",
      "ai applications"
    ]
  },
  {
    "id": 424,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Key definitions",
    "content": "Key definitions in Amazon Bedrock Flows include fundamental concepts for building and managing generative AI workflows. These definitions encompass terms related to workflow orchestration, various node types (e.g., prompt nodes, logic nodes), and how inputs and outputs are handled within the flow. Understanding these core terms is crucial for effectively designing and implementing robust generative AI applications.",
    "tags": [
      "definitions",
      "workflow",
      "nodes",
      "concepts"
    ]
  },
  {
    "id": 425,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Define inputs with expressions",
    "content": "In Amazon Bedrock Flows, you can define inputs with expressions to dynamically map data into your workflow nodes. Expressions allow you to specify how input data is extracted, transformed, and utilized by different components within a flow, for example, using `$.data.genre` to access input data. This capability is essential for creating flexible and adaptable workflows that can process varied data structures and integrate seamlessly with external systems. Expressions ensure that information flows correctly between nodes and external interactions.",
    "tags": [
      "inputs",
      "expressions",
      "data mapping",
      "workflow"
    ]
  },
  {
    "id": 426,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Node types",
    "content": "Amazon Bedrock Flows support various node types to construct diverse generative AI workflows. These include Input nodes for receiving initial data, Prompt nodes for interacting with foundation models, and Output nodes for delivering final results. Other node types can facilitate logic, data transformation, or integration with other Amazon Bedrock features such as knowledge bases and agents. Each node type performs a specialized function, enabling complex orchestration within a flow.",
    "tags": [
      "nodes",
      "input",
      "prompt",
      "output",
      "workflow"
    ]
  },
  {
    "id": 427,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Supported Regions and models",
    "content": "The Regions and models supported by Amazon Bedrock Flows are dependent on the specific node types included in your flow. While Flows themselves are available in certain AWS Regions, the underlying models and features used within prompt nodes, knowledge base nodes, or agent nodes must also be supported in the chosen Region. Model support for Amazon Bedrock Flows specifically depends on the models supported by the node types you add to your flow. Developers should consult the Amazon Bedrock documentation for model and feature availability by Region to ensure compatibility.",
    "tags": [
      "regions",
      "models",
      "support",
      "compatibility"
    ]
  },
  {
    "id": 428,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Prerequisites",
    "content": "Before using Amazon Bedrock Flows, you must fulfill certain prerequisites, including setting up IAM permissions for creating and managing flows. This involves configuring a service role with the necessary actions like `bedrock:CreateFlow`, `bedrock:UpdateFlow`, `bedrock:GetFlow`, `bedrock:ListFlows`, and `bedrock:DeleteFlow`. Additionally, if you plan to encrypt your flow, you need to create AWS KMS keys and attach specific key policies allowing Amazon Bedrock to encrypt and decrypt the prompt with the key.",
    "tags": [
      "prerequisites",
      "iam",
      "permissions",
      "kms"
    ]
  },
  {
    "id": 429,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Create and design a flow",
    "content": "You can create and design a flow in Amazon Bedrock to build end-to-end generative AI workflows using the console or API. This process involves defining nodes and connections between them to orchestrate tasks, as shown in the example of creating an input node, prompt node, and output node. When creating a flow, you specify its name, description, and an execution role ARN. The design phase includes configuring input, prompt, and output nodes, along with any other logical steps required for your application.",
    "tags": [
      "create flow",
      "design flow",
      "workflow",
      "nodes",
      "connections"
    ]
  },
  {
    "id": 430,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Create your first flow",
    "content": "To create your first flow, you can use the Amazon Bedrock console or API. In the console, you navigate through the 'Create and design a flow' section. Using the API, you would send a `CreateFlow` request, specifying essential details like a unique name, an execution role ARN, and the definition of your nodes and connections. This initial setup, as demonstrated with a 'MakePlaylist' flow, lays the groundwork for building and testing your generative AI workflow.",
    "tags": [
      "first flow",
      "create",
      "api",
      "console",
      "workflow"
    ]
  },
  {
    "id": 431,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Design a flow",
    "content": "Designing a flow in Amazon Bedrock involves visually or programmatically laying out the workflow logic using interconnected nodes. Each node represents a distinct operation, such as processing user input, calling a foundation model with a prompt, or integrating a knowledge base. You define the inputs and outputs for each node, specifying their types and expressions, and establish connections to control the data flow and sequence of operations. This allows for complex, multi-step generative AI applications.",
    "tags": [
      "design",
      "workflow logic",
      "nodes",
      "inputs",
      "outputs"
    ]
  },
  {
    "id": 432,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Try example flows",
    "content": "Amazon Bedrock provides the option to try example flows to quickly understand and experiment with its generative AI workflow capabilities. These pre-built examples offer practical demonstrations of how different node types and configurations work together in a functional workflow. By exploring these examples, users can gain insights into common use cases and learn effective practices for designing their own custom flows, accelerating their development process.",
    "tags": [
      "examples",
      "learn",
      "workflow",
      "experiment"
    ]
  },
  {
    "id": 433,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Use a template to create a flow",
    "content": "You can use a template to create a flow in Amazon Bedrock, which significantly streamlines the development of generative AI workflows. Templates offer pre-configured structures for common use cases, reducing the initial setup effort and promoting adherence to best practices. By starting with a template, developers can quickly deploy a functional workflow and then customize it to meet specific application requirements, accelerating the overall development cycle.",
    "tags": [
      "template",
      "create flow",
      "quickstart",
      "workflow"
    ]
  },
  {
    "id": 434,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "View information about flows",
    "content": "To view information about flows in Amazon Bedrock, you can programmatically use API operations. For example, `GetFlow` can retrieve details of a specific flow, while `ListFlows` provides a summary of all available flows. These operations return metadata such as the flow's name, description, and definition, allowing for programmatic inspection of your deployed generative AI workflows. While the sources imply console access for some features, these API calls are fundamental for managing flow visibility.",
    "tags": [
      "view flow",
      "getflow",
      "listflows",
      "api"
    ]
  },
  {
    "id": 435,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Modify a flow",
    "content": "You can modify a flow in Amazon Bedrock to update its logic, nodes, or connections after creation. This is achieved using the `UpdateFlow` API operation, where you can adjust various aspects of the flow's definition. Modifying a flow allows developers to iterate on their generative AI applications, adding new functionalities, optimizing existing steps, or changing integrations with other Bedrock resources like knowledge bases or guardrails. This flexibility ensures that workflows can evolve with changing application requirements.",
    "tags": [
      "modify flow",
      "updateflow",
      "iteration",
      "workflow update"
    ]
  },
  {
    "id": 436,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Include guardrails in your flow",
    "content": "You can include guardrails in your flow to implement safeguards and enforce responsible AI policies within your generative AI applications. Guardrails can be associated with a prompt node or a knowledge base node within a flow. When integrated, they apply content filters and denied topics to both user inputs and model responses, helping to detect and filter harmful or inappropriate content. This ensures a safer and more controlled user experience for your AI application.",
    "tags": [
      "guardrails",
      "flow integration",
      "content filtering",
      "safety"
    ]
  },
  {
    "id": 437,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Test a flow",
    "content": "To test a flow in Amazon Bedrock, you can use the console or programmatically via the API. Testing involves invoking the flow with sample inputs and observing its execution to ensure it performs as expected. For flows incorporating prompts, you can test the prompt within the flow. The `InvokeFlow` API operation is specifically used for programmatic testing, allowing you to simulate real-world interactions and validate the workflow's behavior before deployment.",
    "tags": [
      "test flow",
      "invokeflow",
      "debugging",
      "workflow validation"
    ]
  },
  {
    "id": 438,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Track each step in your flow by viewing its trace",
    "content": "The provided sources do not contain specific information on how to track each step in your flow by viewing its trace within the context of Amazon Bedrock Flows. While tracing is mentioned generally for guardrails and model invocation, a detailed mechanism for monitoring the sequential execution and intermediate states of individual nodes within a flow is not described in these documents.",
    "tags": [
      "flow trace",
      "monitoring",
      "debugging",
      "execution steps"
    ]
  },
  {
    "id": 439,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Run a flow asynchronously",
    "content": "The provided sources do not contain specific information on how to run a flow asynchronously for Amazon Bedrock Flows. While asynchronous invocation (`StartAsyncInvoke`) is mentioned for generating videos, there is no direct guidance or API description for executing an entire generative AI workflow built with Flows in an asynchronous manner within these documents.",
    "tags": [
      "async flow",
      "asynchronous",
      "execution",
      "workflow"
    ]
  },
  {
    "id": 440,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Required permissions for running flow executions",
    "content": "To run flow executions, your IAM principal needs specific permissions. This includes the `bedrock:InvokeFlow` action to initiate the workflow. If the flow interacts with other Amazon Bedrock resources like foundation models, knowledge bases, or agents, the executing role must also have the necessary permissions for those respective operations. This ensures the flow can access and utilize all its configured components during runtime.",
    "tags": [
      "permissions",
      "iam",
      "invokeflow",
      "flow execution"
    ]
  },
  {
    "id": 441,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Create and manage a flow execution",
    "content": "You can create and manage a flow execution by initiating a workflow with specific inputs using the `InvokeFlow` API operation. This action starts a new instance of your defined generative AI workflow, processing the input through its sequence of nodes. Managing executions involves monitoring their progress and retrieving results, ensuring that your AI application performs its tasks reliably. The `InvokeFlow` operation is crucial for running your designed workflows.",
    "tags": [
      "flow execution",
      "create",
      "manage",
      "invokeflow"
    ]
  },
  {
    "id": 442,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Flow execution statuses",
    "content": "The provided sources do not contain specific information detailing the various flow execution statuses for Amazon Bedrock Flows. While general AWS services typically include statuses like 'Running,' 'Completed,' or 'Failed' for job or execution processes, the specific set of states and their meanings for Amazon Bedrock Flow executions are not described in these documents.",
    "tags": [
      "status",
      "flow execution",
      "workflow state",
      "monitoring"
    ]
  },
  {
    "id": 443,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Best practices for flow executions",
    "content": "The provided sources do not contain specific information outlining best practices for flow executions in Amazon Bedrock Flows. While general guidance on prompt design and model inference exists, explicit recommendations for optimizing the performance, reliability, or cost-efficiency of running generative AI workflows are not detailed in these documents.",
    "tags": [
      "best practices",
      "flow execution",
      "optimization",
      "efficiency"
    ]
  },
  {
    "id": 444,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Deploy to your application using versions and aliases",
    "content": "The provided sources do not contain specific information on how to deploy Amazon Bedrock Flows to your application using versions and aliases. While versioning is mentioned for prompts and guardrails, and deployment concepts exist for models, explicit details regarding version control and alias management for entire generative AI workflows are not described for Flows in these documents.",
    "tags": [
      "deployment",
      "versions",
      "aliases",
      "flow management"
    ]
  },
  {
    "id": 445,
    "service": "Amazon Bedrock",
    "category": "Prompt Management",
    "title": "Create a Prompt Version",
    "content": "A prompt version is a snapshot of your draft prompt in Prompt management. You can create a version of a prompt using the Amazon Bedrock console by selecting \"Create version\" in the Prompt versions section, or via the Agents for Amazon Bedrock build-time endpoint by sending a `CreatePromptVersion` request. Versions are created incrementally, starting from 1, and each version gets a unique ID and ARN. This allows you to save and manage different iterations of your prompts for deployment to applications.",
    "tags": [
      "prompt",
      "version",
      "management",
      "create"
    ]
  },
  {
    "id": 446,
    "service": "Amazon Bedrock",
    "category": "Prompt Management",
    "title": "View Prompt Version Information",
    "content": "You can view information about a specific version of a prompt created in Prompt management. This can be done through the Amazon Bedrock console by selecting a prompt and then choosing a version in the \"Prompt versions\" section. Alternatively, you can use the Agents for Amazon Bedrock build-time endpoint by sending a `GetPrompt` request, specifying the prompt's ARN or ID and the version number in the `promptVersion` field. The version details page provides information about the version, the prompt message, and its configurations.",
    "tags": [
      "prompt",
      "version",
      "view",
      "information"
    ]
  },
  {
    "id": 447,
    "service": "Amazon Bedrock",
    "category": "Prompt Management",
    "title": "Delete a Prompt Version",
    "content": "To delete a specific version of a prompt in Prompt management, you can use either the Amazon Bedrock console or the API. In the console, navigate to the prompt, select the desired version in the \"Prompt versions\" section, and choose \"Delete\". Using the API, send a `DeletePrompt` request to an Agents for Amazon Bedrock build-time endpoint, providing the prompt's ARN or ID and the specific version number to be deleted. This action permanently removes the selected prompt version.",
    "tags": [
      "prompt",
      "version",
      "delete",
      "management"
    ]
  },
  {
    "id": 448,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Create an Alias (for Agents)",
    "content": "The provided sources mention deploying an agent with an alias as a step in building a simple agent, such as during Step 4: Deploy the agent with an alias. However, there is no direct information on creating aliases for general Amazon Bedrock foundation models or prompts within the scope of this document. For agents, an alias typically refers to a stable reference for a deployed agent version, facilitating updates without changing the integration point.",
    "tags": [
      "alias",
      "agent",
      "deployment",
      "create"
    ]
  },
  {
    "id": 449,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "View Alias Information (for Agents)",
    "content": "While the sources refer to deploying an agent with an alias, they do not provide specific instructions or API calls for viewing information about these aliases for general Amazon Bedrock resources like models or prompts. For agents, an alias acts as a pointer to a specific agent version, which is generally managed as part of the agent's lifecycle.",
    "tags": [
      "alias",
      "agent",
      "view",
      "information"
    ]
  },
  {
    "id": 450,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Modify an Alias (for Agents)",
    "content": "The provided sources do not contain specific details on how to modify an alias for Amazon Bedrock foundation models or prompts. For agents, an alias is associated with a deployed version. Modifying an alias would typically involve updating which specific agent version the alias points to, allowing for seamless updates of the underlying agent without changing the alias name used by applications.",
    "tags": [
      "alias",
      "agent",
      "modify",
      "update"
    ]
  },
  {
    "id": 451,
    "service": "Amazon Bedrock",
    "category": "Agents",
    "title": "Delete an Alias (for Agents)",
    "content": "The provided sources do not specify methods for deleting an alias for Amazon Bedrock foundation models or prompts. In the context of agents, an alias is used during deployment. Deleting an alias would entail removing this reference to a specific agent version, effectively making that alias unavailable for invoking the agent.",
    "tags": [
      "alias",
      "agent",
      "delete",
      "remove"
    ]
  },
  {
    "id": 452,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Invoke Cross-Account Lambda from Flow",
    "content": "Within Amazon Bedrock Flows, you have the capability to invoke a Lambda function that resides in a different AWS account. This feature facilitates cross-account integration, allowing you to extend the functionality of your generative AI applications by leveraging existing serverless compute resources across your AWS environment. This is indicated as part of conversing with a flow, highlighting the flow's ability to interact with external services and logic.",
    "tags": [
      "lambda",
      "flow",
      "cross-account",
      "invoke"
    ]
  },
  {
    "id": 453,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Converse with an Amazon Bedrock Flow",
    "content": "You can converse with a flow in Amazon Bedrock to build interactive generative AI applications that manage multi-turn conversations. This feature enables flows to process and respond to sequential user inputs, maintaining context throughout the interaction. The process involves defining the flow's logic, which can include various Amazon Bedrock resources like prompts, knowledge bases, and agents, to guide the conversation.",
    "tags": [
      "flow",
      "conversation",
      "multi-turn",
      "interact"
    ]
  },
  {
    "id": 454,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Multi-Turn Conversations in Flows",
    "content": "To process a multi-turn conversation in a flow in Amazon Bedrock, the flow must be designed to manage sequential user inputs and maintain conversational context. This involves defining nodes within the flow that can handle messages, incorporate system prompts, and potentially utilize other Amazon Bedrock resources such as knowledge bases or agents to generate relevant responses. Each turn of the conversation builds upon previous interactions, allowing for a coherent and extended dialogue.",
    "tags": [
      "flow",
      "multi-turn",
      "conversation",
      "processing"
    ]
  },
  {
    "id": 455,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Create and Run an Example Flow",
    "content": "You can create and run an example flow in Amazon Bedrock to build end-to-end generative AI workflows. This involves defining the flow's structure, including nodes for various Amazon Bedrock resources like prompts, knowledge bases, and agents, and then establishing connections between them. The process of creating and running an example flow is detailed in the sources. After creation, the flow can be invoked to test its functionality, allowing developers to observe how it processes inputs and generates outputs in a defined sequence.",
    "tags": [
      "flow",
      "create",
      "run",
      "example"
    ]
  },
  {
    "id": 456,
    "service": "Amazon Bedrock",
    "category": "General",
    "title": "Run Amazon Bedrock Code Samples",
    "content": "Amazon Bedrock provides numerous code samples to help users get started and implement various features. These examples are available for different interfaces, including the AWS Command Line Interface (AWS CLI), the AWS SDK for Python (Boto3), and Amazon SageMaker AI notebooks. Specifically, you can run Prompt management code samples to create, list, get, and delete prompts and their versions programmatically. These samples demonstrate how to interact with Amazon Bedrock API operations for tasks like creating music playlists with variables.",
    "tags": [
      "code",
      "samples",
      "api",
      "sdk"
    ]
  },
  {
    "id": 457,
    "service": "Amazon Bedrock",
    "category": "Flows",
    "title": "Delete an Amazon Bedrock Flow",
    "content": "The provided sources outline how to create and update Amazon Bedrock Flows, which are used to build end-to-end generative AI workflows. However, specific instructions or API operations for deleting a flow are not explicitly detailed within the given documentation. Generally, flow management would include capabilities to remove unwanted or deprecated workflows, but the exact method for deletion is not covered here, beyond the mention of deleting a prompt that is part of a flow.",
    "tags": [
      "flow",
      "delete",
      "workflow",
      "remove"
    ]
  },
  {
    "id": 458,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Import a Pre-trained Model",
    "content": "Amazon Bedrock supports the capability to import a pre-trained model, enabling users to integrate models developed and customized outside of the Bedrock service. This feature primarily refers to Custom model import of open-source models that have already undergone training. When importing such models, it is crucial to apply the same inference parameters that were used during their original training or customization. This ensures that the imported model functions as expected within the Bedrock environment, extending the range of available generative AI solutions.",
    "tags": [
      "model import",
      "pre-trained",
      "customization",
      "inference"
    ]
  },
  {
    "id": 459,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Import Pre-trained Open-Source Models",
    "content": "Custom model import in Amazon Bedrock is the specific process that allows users to bring in pretrained open-source models into the service. This feature empowers developers to leverage a wide array of community-driven or externally developed models. By importing these models, users can enhance their generative AI applications on Bedrock without the need to train models from scratch. The seamless integration expands the flexibility and capabilities available to users for various use cases.",
    "tags": [
      "custom model",
      "open-source",
      "pre-trained",
      "import"
    ]
  },
  {
    "id": 460,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Supported Architectures for Model Import",
    "content": "The provided documentation indicates a dedicated section for supported architectures relevant to model import within Amazon Bedrock. This suggests that the service is compatible with specific architectural frameworks for external models. However, the details specifying which particular architectures are supported for the Custom model import feature are not elaborated upon in the given source excerpts.",
    "tags": [
      "architectures",
      "model import",
      "compatibility",
      "support"
    ]
  },
  {
    "id": 461,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Import a Model Source from Amazon S3",
    "content": "Amazon Bedrock facilitates the import of model sources directly from Amazon S3 buckets. This is a critical component for custom model import jobs, as model artifacts are typically stored in S3. The service also provides mechanisms for configuring cross-account access to S3 buckets, ensuring secure and flexible data transfer during the model import process. This allows users to manage their model data efficiently and securely.",
    "tags": [
      "s3",
      "model source",
      "import",
      "cross-account"
    ]
  },
  {
    "id": 462,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Prerequisites for Importing Models",
    "content": "The Amazon Bedrock documentation includes a section outlining the prerequisites for importing models. This signifies that certain conditions and configurations must be in place before a user can successfully initiate a model import job. While the existence of these requirements is noted, the specific details regarding necessary permissions, data formats, or other preparatory steps are not provided within the current source excerpts.",
    "tags": [
      "prerequisites",
      "model import",
      "setup",
      "requirements"
    ]
  },
  {
    "id": 463,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Submitting a Model Import Job",
    "content": "Amazon Bedrock allows users to submit a model import job to integrate pre-trained or customized models into the service. This process typically involves providing the location of the model source, such as an Amazon S3 bucket, and other configurations. The submission initiates the transfer and setup of the external model within Bedrock, making it available for use. Specific API operations or console steps are outlined in the broader documentation for this action.",
    "tags": [
      "job",
      "submission",
      "model import",
      "workflow"
    ]
  },
  {
    "id": 464,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Invoking an Imported Model",
    "content": "After a model has been successfully imported into Amazon Bedrock, it can be invoked to run inference and generate responses. To perform inference with an imported model, the `bedrock:GetImportedModel` action is required. Additionally, permissions for `bedrock:ListImportedModels` are needed to select the model within the Amazon Bedrock console. Users can utilize API operations like InvokeModel and Converse to make calls to these integrated models.",
    "tags": [
      "invoke",
      "imported model",
      "inference",
      "api calls"
    ]
  },
  {
    "id": 465,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Cost Calculation for Custom Models",
    "content": "Amazon Bedrock addresses the cost of running a custom model, with pricing generally based on the volume of input tokens and output tokens processed during inference. To effectively use a custom model, users must purchase Provisioned Throughput, which affects the cost structure. The total cost is influenced by both the data processed and the dedicated capacity allocated for the model. Detailed pricing information is available on the Model providers page in the Amazon Bedrock console.",
    "tags": [
      "cost",
      "custom model",
      "pricing",
      "provisioned throughput"
    ]
  },
  {
    "id": 466,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Code Samples for Custom Model Import",
    "content": "Amazon Bedrock provides code samples for custom model import to assist developers in programmatically integrating their models. These samples demonstrate how to interact with the Bedrock API to execute the import process. While the specific code examples are not detailed within these excerpts, their availability is highlighted in the documentation to guide users in building their applications. These samples assume programmatic access credentials are set up.",
    "tags": [
      "code samples",
      "custom model",
      "import",
      "api"
    ]
  },
  {
    "id": 467,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Import a SageMaker AI-trained Amazon Nova Model",
    "content": "Amazon Bedrock offers specific support for importing SageMaker AI-trained Amazon Nova models. This specialized feature enables users to seamlessly transfer models that have been developed and trained using Amazon SageMaker's capabilities for the Amazon Nova family of models. Integrating these models into Bedrock allows for their continued use and deployment within a fully managed generative AI service, streamlining the workflow for SageMaker users.",
    "tags": [
      "sagemaker",
      "amazon nova",
      "model import",
      "ai-trained"
    ]
  },
  {
    "id": 468,
    "service": "Amazon Bedrock",
    "category": "Model Import",
    "title": "Guidelines and Requirements for Importing SageMaker AI-trained Amazon Nova Models",
    "content": "When performing the import of a SageMaker AI-trained Amazon Nova model, specific guidelines and requirements must be met. These criteria are established to ensure a successful and compatible integration between models trained in SageMaker and the Amazon Bedrock service. Although the sources mention the existence of these guidelines, the detailed list of specific rules or technical specifications is not included in the provided excerpts.",
    "tags": [
      "guidelines",
      "requirements",
      "sagemaker",
      "amazon nova"
    ]
  },
  {
    "id": 469,
    "service": "Amazon Bedrock",
    "category": "Model Customization",
    "title": "Creating a Custom Model",
    "content": "In Amazon Bedrock, creating a custom model involves adjusting a base model's parameters using your own training data. This process, known as model customization, can be achieved through techniques like fine-tuning with labeled data or continued pre-training with unlabeled data. The goal is to improve the model's performance for specific tasks or domains. After customization, Provisioned Throughput must be purchased to utilize the custom model for inference in applications.",
    "tags": [
      "custom model",
      "fine-tuning",
      "pre-training",
      "model customization"
    ]
  },
  {
    "id": 470,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Data automation: Transform data into insights",
    "content": "Bedrock Data Automation is a service designed to transform data into insights efficiently. It achieves this by automating the processing of various data types, enabling businesses to extract valuable information for analysis. This service helps in understanding complex datasets, making them actionable for a wide range of applications. By streamlining data transformation, it facilitates better decision-making and supports generative AI applications.",
    "tags": [
      "data automation",
      "insights",
      "transformation",
      "processing"
    ]
  },
  {
    "id": 471,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "What is Bedrock Data Automation?",
    "content": "Bedrock Data Automation is a feature designed to transform data into insights. It helps users convert raw data from various modalities into structured or enriched information. This transformation is crucial for downstream analytical tasks or to enhance AI model performance. The service aims to simplify and automate complex data processing workflows.",
    "tags": [
      "bedrock data automation",
      "definition",
      "data transformation"
    ]
  },
  {
    "id": 472,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "How Bedrock Data Automation works",
    "content": "Bedrock Data Automation processes data through defined workflows, often involving the creation and execution of blueprints. These blueprints specify how raw data inputs, such as documents or images, should be transformed into desired outputs. The service integrates with other AWS tools and offers both console and API access for managing these operations. It streamlines the entire data transformation pipeline from input to output.",
    "tags": [
      "how it works",
      "blueprints",
      "data processing",
      "workflow"
    ]
  },
  {
    "id": 473,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Bedrock Data Automation projects",
    "content": "Bedrock Data Automation supports the creation of projects to organize and manage data automation workflows. These projects provide a structured environment for defining inputs, transformations, and outputs for different tasks. Users can manage these projects directly through the Bedrock Data Automation Console. Projects serve as containers for various data processing initiatives within the service.",
    "tags": [
      "projects",
      "organization",
      "management",
      "console"
    ]
  },
  {
    "id": 474,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Cross Region support required for Bedrock Data Automation",
    "content": "Cross Region support is a requirement for Bedrock Data Automation. This implies that for data automation tasks, resources and data might need to be accessible or processed across different AWS Regions. This capability ensures flexibility and scalability for handling geographically distributed data or applications. Users must configure their setup to accommodate this cross-region operational model.",
    "tags": [
      "cross region",
      "support",
      "requirement",
      "scalability"
    ]
  },
  {
    "id": 475,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Standard output in Bedrock Data Automation",
    "content": "Standard output in Bedrock Data Automation refers to the default formats and structures in which processed data is delivered. This includes transformed content from various input modalities like documents, images, videos, and audio. The service provides predefined ways to output insights extracted from these diverse data types. This standardization helps in integrating the output with other systems or for direct consumption.",
    "tags": [
      "standard output",
      "output formats",
      "data types"
    ]
  },
  {
    "id": 476,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Documents",
    "content": "Within Bedrock Data Automation, documents are a supported input modality for processing. The service can extract, transform, and analyze information contained within various document formats. This capability allows for insights generation from textual data, reports, or other written materials. Processed document data contributes to the standard output of the system.",
    "tags": [
      "documents",
      "input modality",
      "textual data",
      "processing"
    ]
  },
  {
    "id": 477,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Images",
    "content": "Bedrock Data Automation processes images as a key input modality. It can analyze visual content to extract features, perform object recognition, or categorize images. The service transforms raw image data into structured insights suitable for analysis or use by generative AI models. The processed image data forms part of the standard output, enabling diverse applications.",
    "tags": [
      "images",
      "visual content",
      "processing",
      "output"
    ]
  },
  {
    "id": 478,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Videos",
    "content": "The service supports videos as an input modality, allowing for comprehensive video understanding and analysis. Bedrock Data Automation can process video content for elements like scene detection, object tracking, and contextual understanding. This capability enables the extraction of rich, temporal insights from video files. Transformed video data is then included in the standard output.",
    "tags": [
      "videos",
      "video analysis",
      "multimodal",
      "processing"
    ]
  },
  {
    "id": 479,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Audio",
    "content": "Bedrock Data Automation can process audio as an input modality. This feature allows for the analysis of spoken language, sound events, or other acoustic information. The service transforms raw audio streams or files into structured insights, such as transcribed text or sentiment analysis. Audio processing is an integral part of generating comprehensive data automation outputs.",
    "tags": [
      "audio",
      "sound analysis",
      "input modality",
      "transcription"
    ]
  },
  {
    "id": 480,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Custom output and blueprints",
    "content": "Beyond standard outputs, Bedrock Data Automation offers the flexibility of custom output configurations, primarily through blueprints. Blueprints define specific data transformation logic and desired output formats, tailored to unique use cases. This allows users to precisely control how data is processed and structured for their applications. Custom outputs enable greater versatility for integrating with diverse downstream systems.",
    "tags": [
      "custom output",
      "blueprints",
      "flexibility",
      "configuration"
    ]
  },
  {
    "id": 481,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Blueprints",
    "content": "Blueprints are a core concept in Bedrock Data Automation, acting as reusable templates for data transformation. They encapsulate the logic and steps required to process raw data from various modalities into desired insights. Blueprints allow users to define custom output structures and workflows. These configurable templates are essential for automating complex data tasks efficiently.",
    "tags": [
      "blueprints",
      "templates",
      "reusable",
      "data transformation"
    ]
  },
  {
    "id": 482,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Creating blueprints",
    "content": "The process of creating blueprints in Bedrock Data Automation involves defining the data transformation logic and desired output formats. Users can achieve this through either the Bedrock Data Automation Console or API. This definition includes specifying input sources, processing steps, and how the output should be structured. Careful design ensures the blueprint accurately meets the specific automation requirements.",
    "tags": [
      "creating blueprints",
      "console",
      "api",
      "workflow design"
    ]
  },
  {
    "id": 483,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Using the Bedrock Data Automation Console",
    "content": "The Bedrock Data Automation Console provides a graphical interface for managing data automation tasks. Users can utilize the console to create projects, define blueprints, and initiate blueprint creation. It offers a user-friendly way to interact with the service without extensive coding. The console streamlines the setup and management of data transformation workflows.",
    "tags": [
      "console",
      "graphical interface",
      "management",
      "user-friendly"
    ]
  },
  {
    "id": 484,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Projects in the BDA Console",
    "content": "Within the Bedrock Data Automation Console, users can create and manage projects. These projects serve as organizational units for different data automation efforts. Each project can contain multiple blueprints and associated configurations for transforming various data types. The console provides an intuitive way to oversee and track the progress of these projects.",
    "tags": [
      "console projects",
      "organization",
      "management",
      "workflows"
    ]
  },
  {
    "id": 485,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Creating Blueprints in the BDA Console",
    "content": "Users can create blueprints directly within the Bedrock Data Automation Console. This involves navigating to the relevant section and defining the transformation logic. The console guides users through the steps of specifying input data, configuring processing options, and setting output formats. This visual approach simplifies the blueprint creation process for various data automation needs.",
    "tags": [
      "console blueprints",
      "creation",
      "visual interface",
      "data transformation"
    ]
  },
  {
    "id": 486,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Initiating Blueprint Creation",
    "content": "Initiating blueprint creation in the Bedrock Data Automation Console is typically the first step in defining a new data transformation workflow. This involves selecting the option to create a new blueprint and providing initial details. Users then proceed to configure the specific parameters and logic for their desired data processing. This foundational step sets up the framework for custom data automation.",
    "tags": [
      "blueprint initiation",
      "creation flow",
      "console steps",
      "workflow setup"
    ]
  },
  {
    "id": 487,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Previewing the Blueprint",
    "content": "After configuring a blueprint, users can preview the blueprint within the Bedrock Data Automation Console. This preview allows for reviewing the defined logic and expected behavior before full deployment. It helps in validating the data transformation steps and ensuring the output matches requirements. Previewing is a crucial step for debugging and refining blueprints effectively.",
    "tags": [
      "preview",
      "blueprint validation",
      "debugging",
      "console features"
    ]
  },
  {
    "id": 488,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Managing Blueprints",
    "content": "Managing blueprints in Bedrock Data Automation involves tasks like updating, deploying, and overseeing their lifecycle. The Bedrock Data Automation Console provides tools for these management activities. Users can modify existing blueprints to adapt to new requirements or optimize performance. Effective blueprint management ensures ongoing efficiency and accuracy of data automation processes.",
    "tags": [
      "managing blueprints",
      "lifecycle",
      "updates",
      "console tools"
    ]
  },
  {
    "id": 489,
    "service": "Amazon Bedrock",
    "category": "Data Automation",
    "title": "Using Your Blueprint",
    "content": "Once created and managed, using your blueprint involves applying it to actual data to perform the defined transformations. This can be done through the Bedrock Data Automation Console, for example, by processing documents with the console. Blueprints automate the extraction of insights from raw inputs, generating structured outputs. They serve as the executable logic for data automation workflows.",
    "tags": [
      "using blueprints",
      "execution",
      "data processing",
      "console usage"
    ]
  },
  {
    "id": 490,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Processing Documents with Console",
    "content": "The Bedrock Data Automation Console provides a user-friendly interface for processing documents. Within the console, users can manage Projects and utilize pre-defined or custom Blueprints to execute generative AI workflows. This feature enables efficient document processing by leveraging visual tools to manage and apply automation tasks without needing to write code.",
    "tags": [
      "console",
      "documents",
      "processing",
      "blueprints",
      "data automation"
    ]
  },
  {
    "id": 491,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Using the Bedrock Data Automation API",
    "content": "The Bedrock Data Automation API offers programmatic access to manage and execute data automation tasks. Through this API, users can Create a Data Automation Project, Invoke Data Automation Async for background processing, and Get Data Automation Status to monitor job progression. This provides a structured method to integrate generative AI workflows into existing applications for large-scale operations.",
    "tags": [
      "api",
      "data automation",
      "programmatic",
      "workflows",
      "async"
    ]
  },
  {
    "id": 492,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Create a Data Automation Project",
    "content": "To establish a data automation workflow, you must first Create a Data Automation Project using the API. This project serves as a foundational container for organizing your data processing tasks and configurations within Amazon Bedrock. It enables you to define the necessary environment for subsequent generative AI automation efforts.",
    "tags": [
      "project",
      "creation",
      "data automation",
      "organize",
      "workflow"
    ]
  },
  {
    "id": 493,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Invoke Data Automation Async",
    "content": "The Invoke Data Automation Async operation allows for the asynchronous submission of data automation jobs. Once a request is submitted, the process runs in the background, freeing up the client to perform other tasks. This method is particularly suitable for large-scale data processing scenarios where immediate responses are not a critical requirement.",
    "tags": [
      "async",
      "invocation",
      "data automation",
      "background",
      "processing"
    ]
  },
  {
    "id": 494,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Get Data Automation Status",
    "content": "To track the progress of asynchronous data automation jobs, the Get Data Automation Status operation is utilized. This allows users to check the current state of their processing tasks, including whether a job is running, completed, or has encountered errors. This functionality is essential for monitoring the execution and success of your automation workflows.",
    "tags": [
      "status",
      "monitor",
      "jobs",
      "data automation",
      "progress"
    ]
  },
  {
    "id": 495,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Async Output Response",
    "content": "Following the completion of an Invoke Data Automation Async job, the results are contained within the Async Output Response. This output typically includes the processed data or summaries generated by the specified blueprint. The structure of this response is crucial for retrieving and effectively utilizing the outcomes of your asynchronous generative AI workflows.",
    "tags": [
      "output",
      "async",
      "response",
      "results",
      "data automation"
    ]
  },
  {
    "id": 496,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Tagging Inferences and Resources in Bedrock Data Automation",
    "content": "Tagging Inferences and Resources in Bedrock Data Automation enables the organization and management of your data automation assets. By applying tags, which are key-value pairs, to your resources, you can categorize, filter, and track them effectively. This practice aids in cost allocation, access control, and overall resource management within your Amazon Bedrock environment.",
    "tags": [
      "tagging",
      "resources",
      "management",
      "cost",
      "organization"
    ]
  },
  {
    "id": 497,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Prerequisites for using Bedrock Data Automation",
    "content": "To effectively use Bedrock Data Automation, several prerequisites must be satisfied. These include configuring an AWS account with the necessary IAM permissions for both the user initiating the jobs and the Bedrock Data Automation service role. Additionally, ensuring access to the relevant Foundation Models is crucial for enabling the generative AI capabilities within your automation workflows.",
    "tags": [
      "prerequisites",
      "setup",
      "iam",
      "permissions",
      "access"
    ]
  },
  {
    "id": 498,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Using Amazon Bedrock Data Automation CLI",
    "content": "The Amazon Bedrock Data Automation CLI provides command-line tools for interacting with Data Automation features. This allows users to manage projects, create and use blueprints, and process documents directly from their terminal. The CLI is an efficient way to script and automate data processing tasks, offering a streamlined experience for developers and power users.",
    "tags": [
      "cli",
      "data automation",
      "command line",
      "scripting",
      "automation"
    ]
  },
  {
    "id": 499,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Create your first Data Automation project (CLI)",
    "content": "To initiate the use of Bedrock Data Automation via the CLI, the initial step is to create your first Data Automation project. This project acts as a central hub for all your automation activities. The CLI command for this action facilitates quick setup, allowing you to define the foundational structure for your document processing and generative AI tasks efficiently.",
    "tags": [
      "cli",
      "project",
      "creation",
      "first project",
      "data automation"
    ]
  },
  {
    "id": 500,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Complete parameter reference (CLI)",
    "content": "When utilizing the Amazon Bedrock Data Automation CLI, a complete parameter reference is vital for understanding all available options for each command. This reference details the various arguments and configurations that can be used when creating blueprints, processing documents, or managing projects. Consulting this guide ensures accurate and effective command execution for advanced automation requirements.",
    "tags": [
      "cli",
      "parameters",
      "reference",
      "configuration",
      "commands"
    ]
  },
  {
    "id": 501,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Creating a Blueprint",
    "content": "In Bedrock Data Automation, creating a Blueprint involves defining a reusable workflow for processing documents using generative AI models. This blueprint specifies the sequential steps, models, and logic for a specific task, such as summarization or entity extraction. Blueprints enable standardization and efficient application of complex AI processes across different datasets.",
    "tags": [
      "blueprint",
      "creation",
      "workflow",
      "reusable",
      "data automation"
    ]
  },
  {
    "id": 502,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Using your Blueprint",
    "content": "Once a Blueprint has been created in Bedrock Data Automation, it can be used to execute predefined document processing workflows. This involves applying the blueprint to new input data, allowing the system to automatically perform tasks like data extraction or content generation. Leveraging blueprints ensures consistency and accelerates the deployment of generative AI solutions.",
    "tags": [
      "blueprint",
      "usage",
      "apply",
      "workflow",
      "data automation"
    ]
  },
  {
    "id": 503,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Process Documents (CLI)",
    "content": "The Process Documents command within the Bedrock Data Automation CLI enables users to execute document processing tasks. This typically involves specifying a blueprint and the input documents to be processed. The CLI facilitates submitting these jobs for execution, allowing for automated handling of large volumes of documents according to the defined generative AI workflows.",
    "tags": [
      "cli",
      "documents",
      "processing",
      "data automation",
      "workflow"
    ]
  },
  {
    "id": 504,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Check Processing Status (CLI)",
    "content": "To monitor the progress of document processing jobs initiated through the Bedrock Data Automation CLI, the Check Processing Status command is utilized. This command provides updates on the state of ongoing or completed tasks, indicating whether documents are still being processed, have finished successfully, or have encountered errors. It helps ensure oversight of automated workflows.",
    "tags": [
      "cli",
      "status",
      "monitoring",
      "documents",
      "processing"
    ]
  },
  {
    "id": 505,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Retrieve Results (CLI)",
    "content": "After a document processing job is complete in Bedrock Data Automation, the Retrieve Results command in the CLI allows users to access the generated outputs. These results, which can include extracted data, summaries, or other generative AI model responses, are crucial for downstream applications. This step completes the automation cycle by making the processed information available.",
    "tags": [
      "cli",
      "results",
      "retrieve",
      "output",
      "data automation"
    ]
  },
  {
    "id": 506,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Blueprint Operations CLI",
    "content": "The Amazon Bedrock Data Automation CLI provides a dedicated set of commands for managing Blueprints. Users can perform various actions such as creating, updating, listing, and using blueprints directly from the command line. This allows for flexible and automated management of generative AI workflows and their configurations, streamlining the deployment of data automation solutions.",
    "tags": [
      "cli",
      "blueprint",
      "operations",
      "management",
      "workflow"
    ]
  },
  {
    "id": 507,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Processing through CLI",
    "content": "Processing documents through CLI in Bedrock Data Automation refers to initiating and managing automated generative AI tasks using command-line interface commands. This method offers granular control over operations like creating projects, blueprints, and executing document processing jobs. It's particularly useful for integrating data automation into scripts and larger automated systems for efficient data handling.",
    "tags": [
      "cli",
      "processing",
      "documents",
      "automation",
      "data automation"
    ]
  },
  {
    "id": 508,
    "service": "Amazon Bedrock",
    "category": "Inference",
    "title": "Cross-Region Inference: Distribute Model Traffic",
    "content": "Cross-Region inference is a capability within Amazon Bedrock that allows you to seamlessly manage unplanned traffic bursts by utilizing compute across different AWS Regions. It enables the distribution of model invocation requests across multiple regions, effectively increasing throughput and performance for your generative AI applications. This feature automatically selects the optimal AWS Region within your defined geography to serve your inference request, maximizing available resources and model availability. By using inference profiles that encompass multiple regions, you can achieve enhanced scalability and responsiveness.",
    "tags": [
      "cross-region",
      "inference",
      "traffic",
      "throughput",
      "regions"
    ]
  },
  {
    "id": 509,
    "service": "Amazon Bedrock",
    "category": "Inference Profiles",
    "title": "Use a Cross-Region (System-Defined) Inference Profile",
    "content": "Cross-Region (system-defined) inference profiles are predefined Amazon Bedrock resources that enable cross-Region inference. These profiles are named after the specific model they support and are configured with multiple AWS Regions to which model invocation requests can be routed. When you invoke such a profile from a Source Region, the Amazon Bedrock service intelligently routes your request to one of the Destination Regions defined within that profile. This mechanism ensures efficient traffic distribution and optimal resource utilization across specified regions, allowing for high availability and performance.",
    "tags": [
      "inference profile",
      "system-defined",
      "cross-region",
      "regions",
      "model invocation"
    ]
  },
  {
    "id": 510,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Provisioned Throughput: Increase model throughput",
    "content": "Provisioned Throughput is a feature in Amazon Bedrock that allows you to purchase a dedicated level of throughput for a base or custom foundation model. This enables you to process a higher amount and/or rate of tokens during model inference. By purchasing Provisioned Throughput, a provisioned model is created, which helps improve the efficiency and output of your FM-based applications and offers discounted rates. It ensures consistent performance for your generative AI applications. This dedicated capacity optimizes the processing of input and output tokens for your chosen model.",
    "tags": [
      "throughput",
      "provisioned",
      "model inference",
      "efficiency",
      "cost"
    ]
  },
  {
    "id": 511,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Regions and models",
    "content": "Model inference using Provisioned Throughput is supported by various foundation models in Amazon Bedrock, including several Amazon Nova, Amazon Titan, and Anthropic Claude 3 models. The specific regions where Provisioned Throughput is available can vary by model, but generally include regions such as US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Europe (Frankfurt), and South America (São Paulo). For a comprehensive list of supported models and their regional availability for Provisioned Throughput, users can refer to the Amazon Bedrock documentation. This ensures broad access to optimized model performance across different geographic locations.",
    "tags": [
      "regions",
      "models",
      "availability",
      "inference profiles",
      "custom models"
    ]
  },
  {
    "id": 512,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Prerequisites",
    "content": "To use Provisioned Throughput in Amazon Bedrock, your IAM role must have the necessary permissions. If your role has the AmazonBedrockFullAccess AWS managed policy attached, you can proceed. Otherwise, you need to explicitly attach permissions for model invocation actions such as `bedrock:InvokeModel`, `bedrock:InvokeModelWithResponseStream`, `bedrock:Converse`, and `bedrock:ConverseStream`. Additionally, for managing Provisioned Throughput specifically, permissions like `bedrock:GetProvisionedModelThroughput` and `bedrock:ListProvisionedModelThroughputs` are required. For custom models, you must purchase Provisioned Throughput for them before they can be used.",
    "tags": [
      "permissions",
      "IAM",
      "access",
      "custom models",
      "model invocation"
    ]
  },
  {
    "id": 513,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Purchase a Provisioned Throughput",
    "content": "To purchase Provisioned Throughput for a foundation model, you need to increase its model invocation capacity. This process involves creating a provisioned model, which then serves your inference requests. For a custom model, you specify its name or ARN as the `modelId` in a `CreateProvisionedModelThroughput` request. This purchase provides dedicated throughput, allowing for more efficient processing of input and output tokens at potentially discounted rates, crucial for scaling generative AI applications. The provisioned capacity ensures a consistent level of performance for your specific use case.",
    "tags": [
      "purchase",
      "create",
      "capacity",
      "custom models",
      "cost"
    ]
  },
  {
    "id": 514,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "View information about a Provisioned Throughput",
    "content": "You can view information about your Provisioned Throughput in Amazon Bedrock through both the console and the API. In the Amazon Bedrock console, navigate to \"Provisioned Throughput\" from the left navigation pane, then select a specific Provisioned Throughput to find its ARN. Alternatively, using the API, you can send a `GetProvisionedModelThroughput` request for a specific resource or a `ListProvisionedModelThroughputs` request for multiple resources. The `provisionedModelArn` in the API response will contain the relevant identification for your Provisioned Throughput. This allows for easy monitoring and management of your dedicated model resources.",
    "tags": [
      "view",
      "monitor",
      "ARN",
      "console",
      "API"
    ]
  },
  {
    "id": 515,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Modify a Provisioned Throughput",
    "content": "You can modify a Provisioned Throughput that you have purchased in Amazon Bedrock. This process typically involves adjusting the configured capacity or other settings of your provisioned model. The `UpdateProvisionedModelThroughput` API action is available to facilitate these changes, allowing you to adapt your throughput levels as your application's demands evolve. Modifications help optimize performance and cost-efficiency for your ongoing model inference workloads. This flexibility ensures that your provisioned resources align with changing operational needs.",
    "tags": [
      "modify",
      "update",
      "capacity",
      "optimize",
      "API"
    ]
  },
  {
    "id": 516,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Use a Provisioned Throughput",
    "content": "Once a Provisioned Throughput is created, it returns a `provisionedModelArn` which serves as the model ID for invoking the provisioned model. You can use this model ID when running model inference by specifying it in the `modelId` field for operations like `InvokeModel`, `InvokeModelWithResponseStream`, `Converse`, and `ConverseStream`. In the Amazon Bedrock console, you can select your Provisioned Throughput in a playground. Provisioned Throughput can also be utilized for knowledge base vector embedding and response generation, as well as in model evaluation jobs. This integration ensures consistent and dedicated performance across various generative AI applications.",
    "tags": [
      "usage",
      "invoke",
      "inference",
      "model ID",
      "applications"
    ]
  },
  {
    "id": 517,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Delete a Provisioned Throughput",
    "content": "Deleting a Provisioned Throughput involves decommissioning the dedicated model capacity that was purchased for a base or custom model. This action is typically performed when the provisioned resources are no longer required, helping to manage costs effectively. While specific step-by-step instructions for deletion are not detailed here, the resource lifecycle for provisioned capacity is managed, with support extending for a period even after the End-of-Life (EOL) date of a base model. For precise methods to remove Provisioned Throughput, users should consult the latest Amazon Bedrock documentation regarding resource management.",
    "tags": [
      "delete",
      "decommission",
      "resource management",
      "cost control",
      "lifecycle"
    ]
  },
  {
    "id": 518,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Code examples",
    "content": "Code examples for using Provisioned Throughput typically involve integrating the `provisionedModelArn` into your model invocation requests. Amazon Bedrock provides examples using the AWS SDK for Python (Boto3) and AWS Command Line Interface (CLI) for operations like `InvokeModel` and `Converse`. These examples demonstrate how to submit prompts and generate responses, treating the `provisionedModelArn` as the `modelId`. You can find numerous code samples in the Amazon Bedrock documentation, which illustrate how to interact with models, including those backed by Provisioned Throughput, across various programming languages. These resources help developers quickly get started with their generative AI applications.",
    "tags": [
      "SDK",
      "CLI",
      "Python",
      "API calls",
      "invocation"
    ]
  },
  {
    "id": 519,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Tagging Amazon Bedrock Resources",
    "content": "Amazon Bedrock allows you to attach tags to your resources, including batch inference jobs, for organizational and cost-tracking purposes. Tags are custom key-value pairs that help you categorize and identify different resources within your AWS environment. By applying tags, you can effectively monitor and allocate costs associated with your batch inference operations and other Amazon Bedrock features. This enables better resource management and allows for filtering and searching through your jobs and related assets.",
    "tags": [
      "resource",
      "tagging",
      "cost",
      "management",
      "organization"
    ]
  },
  {
    "id": 520,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Tagging Amazon Bedrock Resources via the AWS Management Console",
    "content": "To manage tags for Amazon Bedrock resources, such as batch inference jobs, directly through the AWS Management Console, you can locate the 'Tags' section during resource creation or modification. For new resources like a batch inference job, you can expand the 'Tags' section to add one or more key-value pairs. Similarly, when configuring a guardrail, you can choose to 'Add new tag' to associate it with the resource. For existing guardrails, tags can also be edited via the 'Manage tags' option in the Guardrail Overview.",
    "tags": [
      "console",
      "tagging",
      "gui",
      "management",
      "ui"
    ]
  },
  {
    "id": 521,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Tagging Amazon Bedrock Resources using the API",
    "content": "Programmatic tagging of Amazon Bedrock resources, including those relevant to batch inference, is achieved through API operations using AWS SDKs or the AWS CLI. When creating a batch inference job using `CreateModelInvocationJob`, you can include `tags` as an optional parameter in the request body. Similarly, resources like application inference profiles can have their tags modified using `TagResource` or `UntagResource` requests. You can also retrieve existing tags for a job by making a `ListTagsForResource` request. The `CreateGuardrail` and `CreateInferenceProfile` API calls also support including tags in their request bodies.",
    "tags": [
      "api",
      "tagging",
      "programmatic",
      "sdk",
      "cli"
    ]
  },
  {
    "id": 522,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Models Overview",
    "content": "Amazon Titan models are a suite of foundation models offered through Amazon Bedrock, designed to generate various output modalities like text and images. These models are available through a unified API, allowing users to choose the best model for their specific use case. Titan models also support converting input into embeddings, which condense information into numerical vectors for similarity comparisons. You can use them for tasks such as text generation, image creation, and multimodal embeddings. They integrate with Amazon Bedrock's capabilities for building generative AI applications, ensuring security and privacy.",
    "tags": [
      "titan models",
      "foundation models",
      "generative ai",
      "embeddings"
    ]
  },
  {
    "id": 523,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text Models",
    "content": "Amazon Titan Text models are designed for text generation tasks, available through Amazon Bedrock's API operations like InvokeModel and InvokeModelWithResponseStream. These models accept text as input and generate text or chat responses, supporting streaming outputs. Key inference parameters for Titan Text models include `inputText` for the prompt, `maxTokenCount` to control response length, `temperature` for creativity, `topP` for token sampling, and `stopSequences` to define stopping criteria. They are versatile, suitable for conversational AI, summarization, and creative writing.",
    "tags": [
      "titan text",
      "text generation",
      "inference parameters",
      "streaming"
    ]
  },
  {
    "id": 524,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text G1 - Premier",
    "content": "Amazon Titan Text G1 - Premier is a text generation model available in Amazon Bedrock, identifiable by `amazon.titan-text-premier-v1:0`. It supports text input and generates text and chat output, with streaming capabilities enabled. The model offers inference parameters such as `inputText` for prompts, `maxTokenCount` with a maximum of 3,072 tokens, `temperature`, `topP`, and `stopSequences`. It was launched on May 7, 2024, and is supported in regions like US East (N. Virginia). For detailed usage, it typically involves Python SDK calls to the `InvokeModel` operation.",
    "tags": [
      "titan premier",
      "text generation",
      "model id",
      "inference"
    ]
  },
  {
    "id": 525,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text G1 - Express",
    "content": "Amazon Titan Text G1 - Express is a text generation model accessible in Amazon Bedrock, with the model ID `amazon.titan-text-express-v1`. It takes text as input and produces text and chat outputs, supporting streaming responses. Inference parameters include `inputText`, `maxTokenCount` up to 8,192 tokens, `temperature` (default 0.5), `topP` (default 0.9), and `stopSequences`. The model is widely supported across various AWS regions, including US East (N. Virginia) and US West (Oregon), and was launched on November 29, 2023. It is often recommended for initial experimentation in tutorials due to its accessibility.",
    "tags": [
      "titan express",
      "text generation",
      "model id",
      "streaming"
    ]
  },
  {
    "id": 526,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text G1 - Lite",
    "content": "Amazon Titan Text G1 - Lite is a text generation model available on Amazon Bedrock, identified by the model ID `amazon.titan-text-lite-v1`. This model supports text input and generates text and chat outputs, with streaming capabilities. Its inference parameters include `inputText` for prompts, `maxTokenCount` up to 4,096 tokens, `temperature` (default 0.5), `topP` (default 0.9), and `stopSequences`. The model is supported in multiple regions like US East (N. Virginia) and US West (Oregon), and was launched on November 29, 2023. It is often used for exploring text playgrounds in the console.",
    "tags": [
      "titan lite",
      "text generation",
      "model id",
      "console"
    ]
  },
  {
    "id": 527,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text Model Customization",
    "content": "Amazon Titan Text models can be customized to improve performance for specific tasks or domains using training data. Customization techniques include fine-tuning, which uses labeled data, and continued pre-training, which uses unlabeled data to adjust model parameters. For Titan Text Premier, specific hyperparameters for customization include `epochCount`, `batchSize`, `learningRate`, and `learningRateWarmupSteps`. The number of epochs directly impacts customization cost, as each epoch processes the entire training dataset once.",
    "tags": [
      "model customization",
      "fine-tuning",
      "hyperparameters",
      "training data"
    ]
  },
  {
    "id": 528,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text Prompt Engineering Guidelines",
    "content": "Amazon Titan Text models have specific prompt engineering guidelines to optimize responses. Effective prompt design includes providing simple, clear, and complete instructions, placing the question or instruction at the end, and using separator characters for API calls. For Titan models, adding a newline character (`\\n`) at the end of a prompt can improve performance, especially for classification tasks or questions with answer options. These guidelines help users craft high-quality prompts to condition the Language Model (LLM) to generate desired or better outputs.",
    "tags": [
      "prompt engineering",
      "titan text",
      "guidelines",
      "api calls"
    ]
  },
  {
    "id": 529,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text Embeddings",
    "content": "Amazon Titan Text Embeddings models, like G1 and V2, convert text input into a vector of numerical values (embeddings). These embeddings allow for comparing the similarity between different text objects. The Titan Embeddings G1 - Text model (`amazon.titan-embed-text-v1`) does not support inference parameters, only accepting `inputText`. The V2 model (`amazon.titan-embed-text-v2:0`) offers optional `normalize`, `dimensions`, and `embeddingTypes` (float, binary) parameters for more control over the output embeddings. Responses include the `embedding` vector and `inputTextTokenCount`.",
    "tags": [
      "text embeddings",
      "vector",
      "similarity",
      "titan embeddings"
    ]
  },
  {
    "id": 530,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Multimodal Embeddings G1",
    "content": "Amazon Titan Multimodal Embeddings G1 (`amazon.titan-embed-image-v1`) generates embeddings from combined text and image inputs. It supports text, image, or both as inputs to produce an embeddings vector. The model can average resulting text and image embedding vectors when both modalities are provided in a single request. An optional `embeddingConfig` allows specifying the `outputEmbeddingLength` to 256, 384, or 1024. This model is designed for use cases requiring similarity comparisons across different modalities.",
    "tags": [
      "multimodal embeddings",
      "text image",
      "vector",
      "embedding length"
    ]
  },
  {
    "id": 531,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Embedding Length for Titan Models",
    "content": "The embedding length refers to the number of dimensions in the output embedding vector generated by the models. For Amazon Titan Text Embeddings V2, the accepted dimensions for output embeddings are 1024 (default), 512, or 256. The Amazon Titan Multimodal Embeddings G1 model also allows configuring the output embedding length to 256, 384, or 1024, with 1024 being the default. This parameter is crucial for optimizing storage and performance in downstream applications like similarity search and retrieval-augmented generation (RAG).",
    "tags": [
      "embedding length",
      "dimensions",
      "multimodal",
      "text embeddings"
    ]
  },
  {
    "id": 532,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Fine-tuning Titan Models",
    "content": "Fine-tuning is a model customization process where training data is used to adjust a base model's parameters to create a custom model, improving its performance on specific tasks or domains. This technique is supported for Amazon Titan models, including the Titan Text models and Titan Image Generator G1 models. For Titan Text Premier, fine-tuning involves hyperparameters such as `epochCount`, `batchSize`, and `learningRate`. For Titan Multimodal Embeddings G1, `epochs`, `batchSize`, and `learningRate` are also configurable.",
    "tags": [
      "fine-tuning",
      "model customization",
      "titan models",
      "training data"
    ]
  },
  {
    "id": 533,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Preparing Datasets for Titan Model Customization",
    "content": "For model customization of Amazon Titan models, such as fine-tuning, preparing datasets is a crucial step. Fine-tuning typically requires labeled data, consisting of input-output pairs specific to the desired task. Continued pre-training, another customization technique, utilizes unlabeled data. These datasets are used to adjust the model's parameters, allowing it to adapt to specific domains or improve performance on targeted tasks. High-quality, relevant data ensures the effectiveness of the customization process.",
    "tags": [
      "datasets",
      "model customization",
      "fine-tuning",
      "training data"
    ]
  },
  {
    "id": 534,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Hyperparameters for Titan Model Customization",
    "content": "Hyperparameters are values that can be adjusted during model customization to control the training process and the resulting custom model. For Amazon Titan Text Premier, key hyperparameters include `epochCount` (1-5 iterations), `batchSize` (1), and `learningRate` (1.00E-07 to 1.00E-05). Amazon Titan Multimodal Embeddings G1 also supports configurable hyperparameters like `epochs` (1-100) and `batchSize` (256-9,216), and `learningRate`. These settings allow fine-grained control over how the model adapts to new data during fine-tuning or continued pre-training.",
    "tags": [
      "hyperparameters",
      "model customization",
      "titan models",
      "training"
    ]
  },
  {
    "id": 535,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Image Generator G1 Models Overview",
    "content": "Amazon Titan Image Generator G1 models, including V1 and V2, are foundational models offered by Amazon Bedrock for image generation and modification tasks. These models take text or image inputs and produce image outputs. They support various use cases such as text-to-image generation, inpainting, outpainting, image variation, and image conditioning. Users can interact with these models through the API or the image playground in the console. The models are available in several AWS Regions, including US East (N. Virginia) and US West (Oregon).",
    "tags": [
      "image generation",
      "titan image",
      "multimodal",
      "image modification"
    ]
  },
  {
    "id": 536,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Features of Titan Image Generator G1 Models",
    "content": "Amazon Titan Image Generator G1 models offer several key features for image manipulation. These include text-to-image generation, creating images from descriptive text prompts. Inpainting allows modifying specific areas within an image based on a mask or mask prompt. Outpainting extends an image beyond its original borders. Users can also generate image variations from existing images, with options to specify text prompts and similarity strength. Additionally, V2 models support image conditioning (e.g., Canny edge detection, segmentation map) and color-guided generation, enabling more controlled image creation.",
    "tags": [
      "image features",
      "text-to-image",
      "inpainting",
      "outpainting",
      "image variation"
    ]
  },
  {
    "id": 537,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Parameters for Titan Image Generator G1 Models",
    "content": "When making `InvokeModel` calls with Amazon Titan Image Generator G1 models, various inference parameters can be set to control the output. Common parameters in the `imageGenerationConfig` object include `quality` (standard/premium), `numberOfImages`, `height`, `width`, `cfgScale` (prompt adherence), and `seed` (randomness). Task-specific parameters exist; for text-to-image, `text` (required prompt) and `negativeText` (what to exclude) are used. Inpainting and outpainting add parameters like `image`, `maskPrompt`, or `maskImage`. Image variation uses `images` (list of base64-encoded images) and `similarityStrength`.",
    "tags": [
      "inference parameters",
      "image generation",
      "cfgScale",
      "seed"
    ]
  },
  {
    "id": 538,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Fine-tuning Titan Image Generator G1 Models",
    "content": "Fine-tuning is a supported capability for Amazon Titan Image Generator G1 models. This process involves providing specific training data to adjust the model's internal parameters, enabling it to generate images that align more closely with custom styles, themes, or content requirements. By fine-tuning, users can adapt a base image generation model to their unique use cases, leading to improved performance and more tailored outputs. This allows for greater control and specialization beyond the model's default capabilities.",
    "tags": [
      "fine-tuning",
      "image generation",
      "model customization",
      "training"
    ]
  },
  {
    "id": 539,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Output Format for Titan Image Generator G1 Models",
    "content": "The output from Amazon Titan Image Generator G1 models, when making an `InvokeModel` call, is returned in a specific format. The primary output is an array of base64-encoded image strings within an `images` field. If an error occurs during generation, an `error` string field may also be present. For successful generation, these base64 strings can be decoded and transformed into actual image files, typically JPEG or PNG. The model can generate a specified `numberOfImages` in the configured `height` and `width`.",
    "tags": [
      "image output",
      "base64",
      "image format",
      "response body"
    ]
  },
  {
    "id": 540,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Watermark Detection for Titan Image Generator G1",
    "content": "Amazon Titan Image Generator G1 models include a capability for watermark detection. This feature helps in identifying whether generated images contain watermarks. While the sources do not provide explicit details on how watermark detection works or how users interact with this feature, its inclusion suggests a mechanism to manage or inform users about watermarking aspects of the generated content. Watermark detection is part of the comprehensive overview of the Titan Image Generator models, indicating it as a notable aspect of their functionality.",
    "tags": [
      "watermark detection",
      "image generation",
      "titan image",
      "features"
    ]
  },
  {
    "id": 541,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Prompt Engineering Guidelines for Titan Image Generator G1",
    "content": "Amazon Titan Image Generator G1 models come with prompt engineering guidelines to help users achieve desired image outputs. For text-to-image generation, a strong, descriptive prompt that clearly defines elements, colors, and subjects leads to better results. Prompts for image generation should be concise, with a maximum of 512 characters for the `text` field and 512 characters for `negativeText`. Negative prompts are crucial for defining what not to include in the image. Following these guidelines helps in effectively communicating artistic intent to the model.",
    "tags": [
      "prompt engineering",
      "image generation",
      "guidelines",
      "negative prompt"
    ]
  },
  {
    "id": 542,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Security Overview in Amazon Bedrock",
    "content": "Amazon Bedrock provides a robust framework for security, encompassing various measures to protect your generative AI applications. Key aspects include data protection and identity and access management, ensuring that your models and data are secure. It also addresses compliance validation, incident response, resilience, and infrastructure security to maintain a secure operating environment. Furthermore, Amazon Bedrock incorporates features for configuration and vulnerability analysis and prompt injection security to safeguard against common threats.",
    "tags": [
      "security",
      "overview",
      "protection",
      "compliance"
    ]
  },
  {
    "id": 543,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Data Protection in Amazon Bedrock",
    "content": "Data protection in Amazon Bedrock focuses on safeguarding your information through multiple layers of defense. This includes data encryption, which secures data at rest and in transit. Additionally, you can protect your data using an Amazon Virtual Private Cloud (VPC), isolating your network resources and preventing unauthorized access. These measures collectively ensure that your proprietary data remains confidential and integrity is maintained within the service.",
    "tags": [
      "data",
      "protection",
      "encryption",
      "vpc"
    ]
  },
  {
    "id": 544,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Data Encryption in Amazon Bedrock",
    "content": "Amazon Bedrock utilizes data encryption to protect your data. This includes encryption for knowledge base evaluation jobs, ensuring sensitive information handled during evaluations is secured. You can leverage AWS Key Management Service (KMS) keys for encrypting various Amazon Bedrock resources, including guardrails, providing an extra layer of control over your cryptographic keys. This capability supports both AWS managed keys and customer-managed keys for flexible security solutions.",
    "tags": [
      "encryption",
      "kms",
      "data",
      "security"
    ]
  },
  {
    "id": 545,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Protecting Data with Amazon VPC",
    "content": "You can protect your data using an Amazon Virtual Private Cloud (VPC) in Amazon Bedrock, enhancing network security and control. For batch inference jobs, configuring a VPC ensures that data access to and from Amazon S3 buckets is restricted to your private network, preventing exposure over the internet. This is achieved by creating VPC interface endpoints with AWS PrivateLink to establish private connections. You must also attach appropriate VPC permissions to your batch inference service role to allow it to access the configured VPC resources.",
    "tags": [
      "vpc",
      "network",
      "privatelink",
      "batch"
    ]
  },
  {
    "id": 546,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Identity and Access Management (IAM)",
    "content": "Identity and access management (IAM) is crucial for controlling who can access your Amazon Bedrock resources and what actions they can perform. It involves defining policies that grant or deny permissions to identities, such as users and roles. This framework ensures that interactions with Amazon Bedrock are authenticated and authorized according to your security requirements. Understanding how Amazon Bedrock integrates with IAM is fundamental for secure operation.",
    "tags": [
      "iam",
      "access",
      "management",
      "authentication"
    ]
  },
  {
    "id": 547,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "IAM Audience in Amazon Bedrock",
    "content": "The audience in Amazon Bedrock's Identity and Access Management (IAM) refers to the users, groups, and roles that interact with the service. These identities require specific permissions to perform actions like requesting access to foundation models or running inference jobs. Defining the appropriate audience and managing their access ensures a secure and compliant environment for your generative AI applications.",
    "tags": [
      "iam",
      "audience",
      "permissions",
      "users"
    ]
  },
  {
    "id": 548,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Authenticating Identities",
    "content": "In Amazon Bedrock, authenticating identities is essential for programmatic access outside the AWS Management Console. You can use AWS credentials or Amazon Bedrock API keys for authentication. Long-term API keys are suitable for exploration and development, expiring after 30 days. For production applications, it is strongly recommended to use more secure alternatives such as IAM roles or short-term temporary credentials, with short-term API keys lasting up to 12 hours.",
    "tags": [
      "authentication",
      "iam",
      "api-keys",
      "credentials"
    ]
  },
  {
    "id": 549,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Managing Access Using Policies",
    "content": "Managing access using policies in Amazon Bedrock involves attaching IAM policies to users or roles to define their permissions. These policies specify which Amazon Bedrock actions an identity can perform, such as invoking models or creating resources. You can tailor permissions by limiting actions, specifying resources, and using condition keys for fine-grained control. This approach ensures that identities only have the necessary access to perform their tasks, adhering to security best practices.",
    "tags": [
      "iam",
      "policies",
      "access",
      "permissions"
    ]
  },
  {
    "id": 550,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Amazon Bedrock and IAM Integration",
    "content": "Amazon Bedrock seamlessly integrates with AWS Identity and Access Management (IAM) to provide granular control over resource access. This integration allows you to manage permissions for users and roles, determining what actions they can take within Amazon Bedrock. By creating and attaching IAM policies, you can authorize interactions with foundation models, API operations, and other Bedrock features. This ensures that your generative AI applications operate within a secure and well-defined access perimeter.",
    "tags": [
      "iam",
      "integration",
      "access",
      "permissions"
    ]
  },
  {
    "id": 551,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Identity-Based Policy Examples",
    "content": "Amazon Bedrock documentation provides numerous identity-based policy examples to help you configure precise access controls. These examples demonstrate how to create IAM policies that allow or deny specific actions on Amazon Bedrock resources, such as invoking models or managing batch inference jobs. Policies can be tailored to restrict access to particular models, regions, or even specific guardrail versions using condition keys. This level of detail enables robust security configurations for your generative AI workflows.",
    "tags": [
      "iam",
      "policies",
      "examples",
      "permissions"
    ]
  },
  {
    "id": 552,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "AWS Managed Policies for Bedrock",
    "content": "For simplified access management, Amazon Bedrock offers AWS managed policies that you can attach to your IAM users or roles. Key examples include `AmazonBedrockFullAccess`, which grants comprehensive permissions for all Amazon Bedrock actions, including SageMaker AI and Marketplace actions. Another important policy is `AmazonBedrockLimitedAccess`, used for long-term API keys to provide basic Amazon Bedrock action permissions. These managed policies help streamline the initial setup of permissions for Amazon Bedrock.",
    "tags": [
      "aws-managed",
      "policies",
      "iam",
      "access"
    ]
  },
  {
    "id": 553,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Service Roles in Amazon Bedrock",
    "content": "Service roles are IAM roles that Amazon Bedrock assumes to perform actions on your behalf within your AWS account. For instance, to run an automatic model evaluation job, you must create a service role with the necessary permissions for Amazon Bedrock to access S3 buckets and perform evaluations. Similarly, for batch inference jobs, a service role is required to manage and execute the tasks. You can either let the console automatically create a service role or create a custom one with specific permissions.",
    "tags": [
      "service-roles",
      "iam",
      "automation",
      "permissions"
    ]
  },
  {
    "id": 554,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Configuring Access to S3 Buckets",
    "content": "To facilitate data transfer and storage, it is necessary to configure access to S3 buckets for Amazon Bedrock resources. This is particularly important for batch inference jobs where input and output data reside in S3 buckets. When using S3 buckets across different accounts, you must configure a bucket policy to grant the batch inference service role explicit access to the data. Additionally, for console-based model evaluation jobs, Cross-Origin Resource Sharing (CORS) permissions must be enabled on specified S3 buckets.",
    "tags": [
      "s3",
      "access",
      "buckets",
      "permissions"
    ]
  },
  {
    "id": 555,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Troubleshooting in Amazon Bedrock",
    "content": "Amazon Bedrock includes support for troubleshooting various issues that may arise during operation. For instance, when working with Luma AI models, common issues like a 'Failed' job status can be resolved by checking for proper S3 bucket write permissions and ensuring the bucket is in the same region as the Bedrock service. Accessing service quotas information can also help diagnose and address limits impacting your Bedrock applications.",
    "tags": [
      "troubleshooting",
      "errors",
      "support",
      "issues"
    ]
  },
  {
    "id": 556,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Cross-Account S3 Access for Custom Model Import",
    "content": "For custom model import jobs in Amazon Bedrock, you can configure cross-account access to Amazon S3 buckets. This allows you to import customized open-source models into Bedrock even if their data resides in an S3 bucket owned by a different AWS account. Proper configuration of S3 bucket policies and IAM roles is essential to grant the necessary permissions for Bedrock to access the model artifacts securely across accounts.",
    "tags": [
      "cross-account",
      "s3",
      "model-import",
      "permissions"
    ]
  },
  {
    "id": 557,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Configuring Cross-Account S3 Access",
    "content": "Configuring cross-account access to Amazon S3 buckets is vital when your Amazon Bedrock resources need to interact with data in other AWS accounts. This process typically involves setting up IAM roles and bucket policies to explicitly grant permissions for cross-account operations. This ensures that your Amazon Bedrock jobs, such as batch inference or custom model import, can securely read from or write to S3 buckets owned by different accounts, maintaining data isolation and control.",
    "tags": [
      "cross-account",
      "s3",
      "access",
      "permissions"
    ]
  },
  {
    "id": 558,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Cross-Account S3 Access with Custom KMS Key",
    "content": "To further secure cross-account access to Amazon S3 buckets in Amazon Bedrock, you can configure it to work with a custom AWS KMS key. This allows you to encrypt data in the S3 bucket using your own customer-managed key, even when accessing it from a different AWS account. Modifying both the IAM role permissions and the AWS KMS key policy is crucial to grant Bedrock the necessary `kms:CreateGrant` and `kms:Decrypt` permissions for encryption and decryption operations. This approach provides enhanced control over your data's cryptographic protection.",
    "tags": [
      "cross-account",
      "s3",
      "kms",
      "encryption"
    ]
  },
  {
    "id": 559,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Compliance Validation in Amazon Bedrock",
    "content": "Compliance validation in Amazon Bedrock refers to the processes and tools in place to help ensure that your use of the service meets relevant regulatory and industry standards. This involves adhering to best practices for data handling, access control, and model governance. Amazon Bedrock provides capabilities to build generative AI applications with security, privacy, and responsible AI principles in mind. Organizations can leverage Bedrock's features to demonstrate and maintain compliance for their AI initiatives.",
    "tags": [
      "compliance",
      "validation",
      "governance",
      "standards"
    ]
  },
  {
    "id": 560,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Incident Response in Amazon Bedrock",
    "content": "Incident response capabilities are an integral part of Amazon Bedrock's security framework, enabling effective handling of security events. This includes procedures and mechanisms to detect, analyze, contain, and recover from security incidents swiftly. By leveraging monitoring tools like CloudWatch metrics for Guardrails and Agents, you can track runtime activities and detect anomalies. This proactive monitoring and response mechanism helps minimize the impact of potential security breaches and maintain operational continuity.",
    "tags": [
      "incident",
      "response",
      "security",
      "monitoring"
    ]
  },
  {
    "id": 561,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Resilience in Amazon Bedrock",
    "content": "Resilience in Amazon Bedrock focuses on ensuring the continuous availability and performance of your generative AI applications, even in the face of disruptions. This involves designing systems that can withstand failures and recover quickly. Amazon Bedrock, as a fully managed service, inherently provides a resilient infrastructure for running high-performing foundation models. This robustness helps applications maintain their functionality and deliver consistent user experiences.",
    "tags": [
      "resilience",
      "availability",
      "reliability",
      "infrastructure"
    ]
  },
  {
    "id": 562,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Infrastructure Security in Amazon Bedrock",
    "content": "Infrastructure security in Amazon Bedrock pertains to the protection of the underlying hardware, software, networking, and facilities that support the service. Amazon Web Services (AWS) is responsible for the security *of* the cloud, implementing measures such as physical access controls, network segregation, and system hardening. This foundational security ensures that the environment where Amazon Bedrock operates is protected against unauthorized access and vulnerabilities, providing a secure platform for your AI workloads.",
    "tags": [
      "infrastructure",
      "security",
      "aws",
      "platform"
    ]
  },
  {
    "id": 563,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Cross-Service Confused Deputy Prevention",
    "content": "Cross-service confused deputy prevention is a security control implemented in Amazon Bedrock to mitigate a specific type of vulnerability. This vulnerability occurs when a service, acting on behalf of an authorized user, is tricked into performing actions on resources it should not have access to, potentially granting unintended permissions. Amazon Bedrock employs mechanisms to prevent such scenarios, ensuring that service-to-service interactions adhere strictly to defined authorization boundaries and operate with the principle of least privilege.",
    "tags": [
      "confused-deputy",
      "cross-service",
      "security",
      "iam"
    ]
  },
  {
    "id": 564,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Configuration and Vulnerability Analysis",
    "content": "Amazon Bedrock includes features for configuration and vulnerability analysis, which are crucial for maintaining the security posture of your generative AI applications. This involves continuously monitoring and assessing the configurations of your Bedrock resources to identify and rectify potential security gaps. Regular analysis helps in detecting vulnerabilities that could be exploited, allowing you to proactively strengthen your security controls. This ensures that your deployments remain robust and protected against emerging threats.",
    "tags": [
      "vulnerability",
      "analysis",
      "configuration",
      "security"
    ]
  },
  {
    "id": 565,
    "service": "Amazon Bedrock",
    "category": "Security",
    "title": "Prompt Injection Security",
    "content": "Prompt injection security is a critical consideration in Amazon Bedrock, aiming to protect against malicious user inputs designed to bypass model safety and developer instructions. These attacks can lead to harmful content generation or unauthorized actions. Amazon Bedrock Guardrails offers prompt attack filters that require input tags to be present in user prompts to filter out such attacks. Using a dynamic, random string as a tag suffix for each request is recommended to make the tag structure unpredictable and mitigate prompt injection risks.",
    "tags": [
      "prompt-injection",
      "security",
      "guardrails",
      "attacks"
    ]
  },
  {
    "id": 566,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Amazon Bedrock Performance",
    "content": "Amazon Bedrock offers various ways to monitor performance across your generative AI applications. This includes utilizing CloudWatch Logs and Amazon S3 for detailed model invocation insights, as well as CloudWatch metrics to track the performance of Guardrails, Agents, and runtime operations. Additionally, Amazon EventBridge can be configured to monitor changes in job states, providing real-time alerts and automation possibilities. These integrated tools are essential for analyzing usage, latency, and token counts, enabling effective optimization of your AI solutions. Proactive monitoring helps ensure the health and efficiency of your Amazon Bedrock resources.",
    "tags": [
      "bedrock",
      "performance",
      "monitoring",
      "cloudwatch"
    ]
  },
  {
    "id": 567,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Model Invocation Monitoring with CloudWatch Logs and S3",
    "content": "You can monitor model invocation performance by setting up destinations in both Amazon CloudWatch Logs and Amazon S3. This dual approach allows for comprehensive capture and storage of model invocation logs, which contain critical data points. These logs include details such as input and output tokens, latency metrics, and other essential information for analyzing and optimizing your foundation model usage. Configuring these destinations is a fundamental step in gaining visibility into the operational aspects of your AI applications.",
    "tags": [
      "model",
      "invocation",
      "cloudwatch",
      "s3",
      "logging"
    ]
  },
  {
    "id": 568,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Set up an Amazon S3 Destination for Model Invocation Logs",
    "content": "To monitor model invocation, you can specifically designate an Amazon S3 bucket as a destination for your logs. This setup involves configuring an S3 bucket to receive and store detailed model invocation logs. Utilizing S3 for this purpose ensures long-term storage and facilitates subsequent analysis of inference data. The logs stored in S3 provide valuable insights into model inputs, outputs, and performance over time, aiding in auditing and review processes.",
    "tags": [
      "s3",
      "destination",
      "logging",
      "invocation",
      "setup"
    ]
  },
  {
    "id": 569,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Set up a CloudWatch Logs Destination for Model Invocation",
    "content": "Setting up an Amazon CloudWatch Logs destination is a direct method for collecting model invocation logs in Amazon Bedrock. This configuration involves directing real-time inference data to a designated CloudWatch log group. By using CloudWatch, you can effectively monitor live performance, create custom alarms for specific events, and visualize metrics related to your foundation model invocations. This destination is crucial for a detailed operational analysis and quick identification of potential issues during model inference.",
    "tags": [
      "cloudwatch",
      "logs",
      "destination",
      "monitoring",
      "inference"
    ]
  },
  {
    "id": 570,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Model Invocation Logging via the Console",
    "content": "You can easily enable model invocation logging directly through the Amazon Bedrock console. This graphical interface offers a straightforward way to configure logging settings for your foundation models without needing to write code. The console allows you to specify either CloudWatch Logs or Amazon S3 as destinations for your invocation data. This simplifies the process of tracking model usage and performance metrics, making it accessible for users who prefer a visual setup.",
    "tags": [
      "console",
      "logging",
      "invocation",
      "bedrock",
      "gui"
    ]
  },
  {
    "id": 571,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Model Invocation Logging via the API",
    "content": "For programmatic control, model invocation logging can be configured using the Amazon Bedrock API. This method offers developers the flexibility to integrate logging setup directly into their applications or automation scripts. Through API calls, you can specify CloudWatch Logs or Amazon S3 as destinations for your model invocation data, enabling advanced customization. This approach supports automated management of logging configurations, which is ideal for large-scale deployments or CI/CD pipelines.",
    "tags": [
      "api",
      "logging",
      "invocation",
      "programmatic",
      "automation"
    ]
  },
  {
    "id": 572,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitor Knowledge Bases using CloudWatch Logs",
    "content": "You can monitor the performance and operation of your Amazon Bedrock knowledge bases by integrating them with CloudWatch Logs. This capability allows you to gain deep insights into how your knowledge bases are performing and interacting with queries. By sending knowledge base logs to CloudWatch, you can effectively track details such as incoming user queries, retrieved data passages, and the generated responses from the foundation model. This monitoring is crucial for debugging, optimizing Retrieval Augmented Generation (RAG) performance, and ensuring the accuracy of information provided by your applications.",
    "tags": [
      "knowledgebase",
      "cloudwatch",
      "logging",
      "rag",
      "monitoring"
    ]
  },
  {
    "id": 573,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Knowledge Base Logging via the Console",
    "content": "To set up logging for knowledge bases, the Amazon Bedrock console provides a straightforward and visual interface. Through the console, users can easily enable and configure the logging of all knowledge base activities directly to CloudWatch Logs. This method simplifies the setup process, making it accessible for users who prefer a guided approach rather than programmatic configuration. It ensures that all relevant interactions and operational data with the knowledge base are captured, allowing for later review and analysis of its performance and usage.",
    "tags": [
      "knowledgebase",
      "console",
      "logging",
      "setup",
      "cloudwatch"
    ]
  },
  {
    "id": 574,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Knowledge Base Logging via the CloudWatch API",
    "content": "For developers seeking programmatic control, knowledge base logging can be configured using the CloudWatch API. This advanced method allows for precise management of how logs from Amazon Bedrock knowledge bases are directed to CloudWatch Logs. Utilizing the API facilitates the automation of logging configurations and seamless integration into existing monitoring pipelines. It provides fine-grained control over aspects such as log groups, log streams, and retention policies, which is highly beneficial for complex and dynamic deployments.",
    "tags": [
      "knowledgebase",
      "cloudwatch",
      "api",
      "logging",
      "automation"
    ]
  },
  {
    "id": 575,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Log Types for Knowledge Base Monitoring",
    "content": "For knowledge base monitoring, Amazon Bedrock explicitly supports various log types that offer detailed insights into their operation. These supported log types capture different facets of knowledge base interactions, including the specifics of incoming user queries, the effectiveness of retrieved data, and the characteristics of generated responses. Understanding the available log types is critical for effectively debugging, analyzing performance, and optimizing the behavior of your Retrieval Augmented Generation (RAG) applications within Amazon Bedrock. The sources refer to a section detailing these supported log types.",
    "tags": [
      "log",
      "types",
      "knowledgebase",
      "monitoring",
      "rag"
    ]
  },
  {
    "id": 576,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "User Permissions and Limits for Knowledge Base Logging",
    "content": "When configuring knowledge base logging, specific user permissions and limits are essential considerations. IAM identities (users or roles) must be granted the appropriate permissions to configure and access CloudWatch Logs for Amazon Bedrock knowledge bases. These permissions are crucial for ensuring that only authorized entities can manage logging settings and view sensitive operational data. Additionally, there are service quotas and limits that govern the volume and retention duration of these logs, directly impacting both the cost and data availability for your monitoring needs.",
    "tags": [
      "permissions",
      "limits",
      "knowledgebase",
      "logging",
      "iam"
    ]
  },
  {
    "id": 577,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Amazon Bedrock Performance",
    "content": "Amazon Bedrock offers various ways to monitor the performance of your generative AI applications. This includes observing model invocation activity through CloudWatch Logs and Amazon S3. You can also monitor knowledge bases, Guardrails, and Agents using CloudWatch metrics. Additionally, EventBridge can be used to track changes in job states. These capabilities help ensure the smooth operation and efficiency of your Bedrock resources.",
    "tags": [
      "monitoring",
      "performance",
      "metrics",
      "logs",
      "events"
    ]
  },
  {
    "id": 578,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Model Invocation Monitoring with CloudWatch Logs and Amazon S3",
    "content": "You can effectively monitor the invocation of foundation models in Amazon Bedrock by sending logs to both Amazon CloudWatch Logs and Amazon S3. This dual logging approach allows for comprehensive tracking and analysis of model inference activity. Setting up logging to these destinations provides detailed records of requests and responses. This ensures you have persistent storage and real-time monitoring capabilities for all model invocations. It helps in auditing and debugging your generative AI applications.",
    "tags": [
      "model invocation",
      "logging",
      "cloudwatch",
      "s3"
    ]
  },
  {
    "id": 579,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Setting Up Amazon S3 for Model Invocation Logs",
    "content": "To set up an Amazon S3 destination for model invocation logs, you configure Bedrock to send detailed records of model inference to an S3 bucket. This process ensures that a persistent, scalable, and secure storage solution is available for your log data. Storing logs in S3 allows for long-term retention, historical analysis, and integration with other AWS services for further processing. It is a crucial step for maintaining an audit trail of your foundation model usage and responses. This setup supports compliance and detailed performance reviews.",
    "tags": [
      "s3",
      "destination",
      "logging",
      "storage",
      "audit"
    ]
  },
  {
    "id": 580,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Setting Up CloudWatch Logs for Model Invocation Logs",
    "content": "Setting up a CloudWatch Logs destination involves configuring Amazon Bedrock to stream model invocation events directly to Amazon CloudWatch Logs. This enables near real-time monitoring, metric creation, and alarm capabilities based on your inference activity. CloudWatch Logs provides centralized log management and analytics, helping you quickly identify operational issues or performance trends. It is an essential component for active monitoring and alerting on the behavior of your foundation models. This ensures immediate visibility into model usage.",
    "tags": [
      "cloudwatch",
      "logs",
      "monitoring",
      "real-time",
      "alerts"
    ]
  },
  {
    "id": 581,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Configuring Model Invocation Logging via Console",
    "content": "Model invocation logging can be configured easily through the Amazon Bedrock console. This graphical interface simplifies the process of enabling and managing the logging of foundation model inferences. Users can specify CloudWatch Logs and Amazon S3 as destinations for their invocation logs. The console provides a user-friendly way to set up these monitoring capabilities without requiring programmatic interaction. It's an intuitive method for initial setup and ongoing management.",
    "tags": [
      "console",
      "logging",
      "model invocation",
      "setup"
    ]
  },
  {
    "id": 582,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Configuring Model Invocation Logging via API",
    "content": "For programmatic control, model invocation logging can be configured using the Amazon Bedrock API. This method allows developers to integrate logging setup directly into their applications and automation workflows. You can specify CloudWatch Logs and Amazon S3 as destinations for your inference logs through API calls. The API provides flexibility for advanced configurations and streamlined management in large-scale deployments. This enables automated and precise control over logging.",
    "tags": [
      "api",
      "logging",
      "model invocation",
      "programmatic"
    ]
  },
  {
    "id": 583,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Knowledge Bases with CloudWatch Logs",
    "content": "Amazon Bedrock enables you to monitor the activity of your knowledge bases directly through CloudWatch Logs. This includes logging queries and responses to understand usage patterns and performance. By consolidating these logs in CloudWatch, you gain insights into how users interact with your knowledge bases. This monitoring helps in optimizing the effectiveness and reliability of your Retrieval Augmented Generation (RAG) applications. It's a key feature for maintaining operational visibility.",
    "tags": [
      "knowledge bases",
      "cloudwatch",
      "logs",
      "rag",
      "monitoring"
    ]
  },
  {
    "id": 584,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Configuring Knowledge Base Logging via Console",
    "content": "Configuring knowledge base logging through the Amazon Bedrock console provides a straightforward way to enable monitoring for your RAG applications. Users can access graphical interface options to set up CloudWatch Logs for their knowledge bases. This console-based approach simplifies the process of tracking interactions, queries, and responses. It offers an intuitive method for managing logging settings for knowledge base resources. This ensures easy access to operational insights.",
    "tags": [
      "console",
      "knowledge bases",
      "logging",
      "setup"
    ]
  },
  {
    "id": 585,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Configuring Knowledge Base Logging via CloudWatch API",
    "content": "For automated and fine-grained control, knowledge base logging can be configured programmatically using the CloudWatch API in conjunction with Amazon Bedrock. This allows developers to integrate the setup of logging for RAG applications into their existing codebases. By leveraging the API, you can precisely define how knowledge base events are captured and managed within CloudWatch Logs. This method is ideal for sophisticated deployment and management strategies. It provides robust automation capabilities.",
    "tags": [
      "api",
      "knowledge bases",
      "logging",
      "cloudwatch"
    ]
  },
  {
    "id": 586,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Supported Log Types for Amazon Bedrock Monitoring",
    "content": "Amazon Bedrock supports various log types to ensure comprehensive monitoring of your generative AI applications. These primarily include model invocation logs, which capture details about inputs, outputs, and inference parameters from foundation models. Additionally, knowledge base logs are supported, detailing queries and responses for Retrieval Augmented Generation (RAG) usage. The service also provides CloudWatch metrics for Guardrails and Agents, alongside EventBridge notifications for job state changes. This extensive logging helps in diagnostics and performance analysis.",
    "tags": [
      "log types",
      "model invocation",
      "knowledge bases",
      "guardrails",
      "agents"
    ]
  },
  {
    "id": 587,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "User Permissions and Limits for Monitoring in Bedrock",
    "content": "To effectively monitor Amazon Bedrock resources, users require specific IAM permissions for logging destinations like Amazon S3 and CloudWatch Logs. The AmazonBedrockFullAccess AWS managed policy can grant broad access, but least privilege is recommended for security. For logging model invocation or knowledge base activities, permissions for `bedrock:InvokeModel` or `bedrock:InvokeModelWithResponseStream` are generally needed. While specific limits on logging frequency or volume weren't detailed under \"User permissions and limits\", Bedrock has overall service quotas that can be requested for increase. Identity and Access Management (IAM) is central to controlling who can access and configure monitoring features.",
    "tags": [
      "permissions",
      "iam",
      "security",
      "limits",
      "quotas"
    ]
  },
  {
    "id": 588,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Examples of Knowledge Base Logs",
    "content": "The sources indicate that Amazon Bedrock provides examples of knowledge base logs. These examples are typically found within the user guide to help users understand the structure and content of the logs generated by knowledge bases. While the specific examples are not detailed in the provided excerpts, their existence suggests that users can review them for insights into knowledge base activity and performance. This helps in debugging and optimizing knowledge base operations.",
    "tags": [
      "knowledge base",
      "logs",
      "examples",
      "debugging"
    ]
  },
  {
    "id": 589,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Common Debugging Queries for Knowledge Base Logs",
    "content": "The sources mention that Amazon Bedrock offers examples of common queries designed to debug knowledge base logs. These queries serve as practical tools for identifying issues and troubleshooting problems within knowledge base operations. Although the specific queries are not detailed in the provided text, their availability is crucial for users seeking to diagnose and resolve performance or accuracy issues efficiently. They guide users in analyzing log data effectively.",
    "tags": [
      "knowledge base",
      "logs",
      "debugging",
      "queries"
    ]
  },
  {
    "id": 590,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Guardrails with CloudWatch Metrics",
    "content": "Amazon Bedrock Guardrails can be monitored using CloudWatch metrics. This capability allows users to track the performance and effectiveness of their guardrails in real-time. By utilizing CloudWatch, users can gain insights into how guardrails are filtering harmful content and enforcing policies for their generative AI applications. This helps ensure the desired safety and privacy controls are consistently applied across models.",
    "tags": [
      "guardrails",
      "cloudwatch",
      "metrics",
      "monitoring"
    ]
  },
  {
    "id": 591,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Agents with CloudWatch Metrics",
    "content": "Amazon Bedrock Agents can be monitored using CloudWatch Metrics. This feature enables users to observe the operational performance and behavior of their agents. Through CloudWatch, developers can gain valuable insights into how their agents are executing tasks and interacting with enterprise systems and data sources. This monitoring capability is essential for maintaining the reliability and efficiency of AI-driven applications built with Amazon Bedrock Agents.",
    "tags": [
      "agents",
      "cloudwatch",
      "metrics",
      "monitoring"
    ]
  },
  {
    "id": 592,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Runtime Metrics",
    "content": "Amazon Bedrock provides runtime metrics which are crucial for understanding the operational performance of foundation models during inference. These metrics offer detailed insights into the behavior and resource utilization of models as they generate responses to prompts. Analyzing runtime metrics helps users identify bottlenecks, optimize model configurations, and ensure efficient and responsive generative AI applications. This data is vital for performance tuning and maintaining service quality.",
    "tags": [
      "runtime",
      "metrics",
      "performance",
      "monitoring"
    ]
  },
  {
    "id": 593,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "CloudWatch Metrics for Amazon Bedrock",
    "content": "CloudWatch metrics are available for Amazon Bedrock, offering a comprehensive view of service health and performance. These metrics enable users to monitor various aspects of their generative AI applications, including model invocation, Guardrails activity, and Agents performance. By integrating with CloudWatch, Amazon Bedrock allows for the collection, visualization, and alarming of operational data. This empowers users to proactively manage their Bedrock resources and ensure optimal functionality.",
    "tags": [
      "cloudwatch",
      "metrics",
      "monitoring",
      "performance"
    ]
  },
  {
    "id": 594,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Job State Changes with EventBridge",
    "content": "Amazon Bedrock allows users to monitor job state changes by utilizing EventBridge. This integration enables the automation of workflows and notifications based on the lifecycle events of Amazon Bedrock jobs. When a job's state changes, EventBridge can trigger various actions, such as sending alerts, initiating other processes, or updating dashboards. This ensures that users are informed about the progress and status of their long-running Bedrock tasks efficiently.",
    "tags": [
      "eventbridge",
      "job states",
      "monitoring",
      "automation"
    ]
  },
  {
    "id": 595,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "How EventBridge Works with Amazon Bedrock",
    "content": "EventBridge for Amazon Bedrock functions by allowing Amazon Bedrock to emit state change events for its jobs. These events are then routed through EventBridge, where users can define rules to match specific event patterns. When a rule is matched, EventBridge can invoke a target action, such as an AWS Lambda function, Amazon SNS topic, or Amazon SQS queue. This mechanism facilitates the creation of reactive and automated systems that respond to the lifecycle of Bedrock operations.",
    "tags": [
      "eventbridge",
      "workflow",
      "automation",
      "events"
    ]
  },
  {
    "id": 596,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Example: Creating an EventBridge Rule for Bedrock State Changes",
    "content": "An example use case in Amazon Bedrock involves creating an EventBridge rule to handle state change events. This process typically includes defining an event pattern that identifies specific Bedrock job states, such as 'FAILED' or 'COMPLETED'. Once the rule is set up, it can trigger an automated response, like sending a notification or invoking another service, whenever a matching state change occurs. This helps users build resilient and responsive applications that react dynamically to Bedrock job progress.",
    "tags": [
      "eventbridge",
      "rules",
      "state changes",
      "example"
    ]
  },
  {
    "id": 597,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Amazon Bedrock API Calls with CloudTrail",
    "content": "Amazon Bedrock API calls can be monitored using CloudTrail. CloudTrail provides a record of actions taken by a user, role, or an AWS service in Amazon Bedrock. This includes API calls made through the Bedrock console, AWS SDKs, and the AWS CLI. By using CloudTrail, organizations can track changes to their Bedrock resources, ensure compliance, and perform security analysis. It offers valuable audit trails for all management and data events.",
    "tags": [
      "cloudtrail",
      "api calls",
      "monitoring",
      "security"
    ]
  },
  {
    "id": 598,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Information in CloudTrail",
    "content": "Amazon Bedrock information in CloudTrail captures details about API activity within the service. This includes records of management events, which pertain to operations on Bedrock resources, and data events, which record operations on resource data or within a resource itself. Each log entry provides comprehensive information such as the identity of the caller, the time of the API call, the source IP address, and request parameters. This data is essential for security analysis and operational troubleshooting.",
    "tags": [
      "cloudtrail",
      "information",
      "api activity",
      "logging"
    ]
  },
  {
    "id": 599,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Data Events in CloudTrail",
    "content": "Amazon Bedrock data events in CloudTrail capture detailed records of data plane operations performed on or within Bedrock resources. These events provide granular insights into actions like model invocation (e.g., submitting prompts and receiving responses). Tracking data events is critical for auditing how models are being used, ensuring data privacy compliance, and detecting unauthorized access to sensitive generative AI interactions. This level of detail enhances security and operational visibility for Bedrock applications.",
    "tags": [
      "cloudtrail",
      "data events",
      "model invocation",
      "auditing"
    ]
  },
  {
    "id": 600,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Management Events in CloudTrail",
    "content": "Amazon Bedrock management events in CloudTrail record control plane operations that modify or manage Bedrock resources. These events include actions such as creating, updating, or deleting foundation models, inference profiles, guardrails, or knowledge bases. By monitoring management events, administrators can track configuration changes, identify potential security breaches related to resource provisioning, and maintain an audit trail of administrative activities. This ensures governance and compliance over Bedrock infrastructure.",
    "tags": [
      "cloudtrail",
      "management events",
      "resource management",
      "auditing"
    ]
  },
  {
    "id": 601,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Understanding Amazon Bedrock Log File Entries",
    "content": "Understanding Amazon Bedrock log file entries is crucial for effective monitoring and debugging of generative AI applications. These entries contain detailed information about API calls, job states, and model interactions. Each log file entry typically includes details like the event time, caller identity, source IP address, and request parameters, allowing for a thorough analysis of activities within Amazon Bedrock. Properly interpreting these logs aids in troubleshooting, security auditing, and performance optimization.",
    "tags": [
      "log files",
      "entries",
      "understanding",
      "debugging"
    ]
  },
  {
    "id": 602,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Code Examples for Amazon Bedrock",
    "content": "Amazon Bedrock provides extensive code examples to help developers integrate foundation models into their applications. These examples cover various tools, including the AWS Command Line Interface (CLI), AWS SDKs (such as Python Boto3, JavaScript, Java), and Amazon SageMaker AI notebooks. They demonstrate core operations like listing available foundation models, submitting prompts for inference, and generating responses using the `InvokeModel` or `Converse` APIs. Model-specific examples are available for providers like Amazon Titan, Anthropic Claude, and Cohere, illustrating how to handle diverse input/output modalities and inference parameters effectively.",
    "tags": [
      "sdk",
      "api",
      "cli",
      "python",
      "javascript"
    ]
  },
  {
    "id": 603,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Basics",
    "content": "Amazon Bedrock is a fully managed service that offers high-performing foundation models (FMs) from leading AI companies and Amazon through a unified API. It allows users to easily experiment with prompts and configurations, privately customize FMs with their own data using techniques like fine-tuning and Retrieval Augmented Generation (RAG). Key terminology includes Foundation Model (FM), an AI model trained on massive datasets, and Model Inference, the process of an FM generating an output (response) from an input (prompt). To get started, users can explore pricing, familiarize themselves with terminology, and utilize tutorials in the console or API.",
    "tags": [
      "foundation models",
      "generative ai",
      "managed service",
      "api",
      "customization"
    ]
  },
  {
    "id": 604,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Application Scenarios",
    "content": "Amazon Bedrock supports various generative AI application scenarios through its comprehensive capabilities. Users can experiment with prompts and configurations in playgrounds to generate diverse responses. It enables augmenting response generation by creating knowledge bases from data sources and employing Retrieval Augmented Generation (RAG). Developers can build intelligent agents that reason through and execute complex tasks using foundation models and enterprise systems. Furthermore, Bedrock allows adapting models to specific tasks and domains through fine-tuning or continued pre-training, enhancing performance for use cases such as text classification, question answering, and code generation.",
    "tags": [
      "rag",
      "agents",
      "fine-tuning",
      "prompt engineering",
      "generative applications"
    ]
  },
  {
    "id": 605,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Runtime Basics",
    "content": "Amazon Bedrock Runtime encompasses the API operations dedicated to sending inputs to foundation models and receiving their generated responses. The primary API calls for model inference are `InvokeModel` and `Converse`, with streaming counterparts `InvokeModelWithResponseStream` and `ConverseStream`. The `Converse` API is recommended for managing multi-turn conversations due to its unified request structure across models. These APIs facilitate processing the input prompt and delivering the model's output, which can be text, images, or embeddings, depending on the model's supported modalities. Users can configure inference parameters like `maxTokenCount` and `temperature` to control the nature of the generated output.",
    "tags": [
      "api",
      "model invocation",
      "inference",
      "prompts",
      "responses"
    ]
  },
  {
    "id": 606,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Runtime Use Cases",
    "content": "Amazon Bedrock Runtime enables a wide array of inference scenarios, including text generation, image generation, and the creation of embeddings from various inputs. It supports advanced multimodal interactions, allowing models to process combinations of text and image inputs to produce text responses. Users can implement tool use (also known as function calling) with the `Converse` API, empowering models to interact with external tools and APIs to fulfill complex requests. The Runtime also seamlessly integrates with Amazon Bedrock Guardrails to enforce content moderation policies on both user inputs and model outputs during inference. Additionally, features like prompt caching can be leveraged to optimize latency for repetitive inference calls.",
    "tags": [
      "inference",
      "multimodal",
      "tool use",
      "guardrails",
      "api calls"
    ]
  },
  {
    "id": 607,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Nova Models Overview",
    "content": "Amazon Nova is a family of foundation models within Amazon Bedrock, encompassing variants such as Nova Canvas, Nova Premier, Nova Pro, and Nova Lite. These models are characterized by their support for diverse input and output modalities, including text, image, video, and speech, with specific capabilities varying by model. For instance, Nova Lite and Nova Pro can generate text responses from video inputs, whether the video is from an S3 location or provided as a base64-encoded string. Dedicated prompt engineering guides are available to help users effectively leverage Nova models for their specific generative AI tasks.",
    "tags": [
      "multimodal",
      "video",
      "speech",
      "text",
      "image"
    ]
  },
  {
    "id": 608,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Nova Canvas Model",
    "content": "Amazon Nova Canvas is a specialized foundation model within the Amazon Nova family, identified by the model ID `amazon.nova-canvas-v1:0`. This model is designed for image generation tasks, accepting both text and image as input modalities. It enables users to create new images by providing textual descriptions or modifying existing images based on prompts. While it facilitates creative image generation, streaming of responses is not a supported feature for this model. Amazon Nova Canvas is particularly useful for applications requiring visual content creation from diverse inputs.",
    "tags": [
      "image generation",
      "text-to-image",
      "nova models",
      "canvas"
    ]
  },
  {
    "id": 609,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Nova Reel Model",
    "content": "Amazon Nova Reel is an Amazon Bedrock foundation model specifically engineered for video generation. This model processes text and image inputs to produce dynamic video outputs, allowing for the creation of visual narratives. Its model ID is `amazon.nova-reel-v1:0` and it is available in regions such as `us-east-1`. Currently, streaming is not supported for Nova Reel. To initiate video generation tasks with this model, users typically employ the `StartAsyncInvoke` API operation, designed for asynchronous processing of long-running requests like video creation. A dedicated prompt engineering guide is provided for optimal use of this model.",
    "tags": [
      "video generation",
      "multimodal",
      "text-to-video",
      "nova models"
    ]
  },
  {
    "id": 610,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Image Generator Models",
    "content": "Amazon Titan Image Generator G1 models, including V1 and V2, are foundation models available on Amazon Bedrock for generating and modifying images. These models support a variety of image manipulation tasks such as text-to-image generation, inpainting (modifying parts of an image), outpainting (extending an image), and creating image variations. Users can fine-tune the output using inference parameters like `text` (for prompts), `negativeText` (for exclusion), `cfgScale`, `height`, `width`, and `seed`. These models also integrate with Amazon Bedrock Guardrails to help filter inappropriate or harmful content in generated images.",
    "tags": [
      "image generation",
      "text-to-image",
      "inpainting",
      "titan models",
      "image editing"
    ]
  },
  {
    "id": 611,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text Models",
    "content": "Amazon Titan Text models, which include Titan Text Lite, Titan Text Express, and Titan Text Premier, are foundation models available on Amazon Bedrock designed for diverse text generation tasks. These models are highly capable of handling use cases such as chat, question-answering, summarization, and code generation. When invoking these models for inference, users can specify parameters like `maxTokenCount` to control response length, `temperature` to adjust creativity, `topP`, and `stopSequences` to guide the output. This flexibility allows for the generation of varied and contextually appropriate textual content.",
    "tags": [
      "text generation",
      "chatbots",
      "summarization",
      "titan models",
      "inference"
    ]
  },
  {
    "id": 612,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Titan Text Embeddings Models",
    "content": "Amazon Titan Text Embeddings models, encompassing G1 - Text and V2, are essential foundation models on Amazon Bedrock that transform text inputs into dense numerical embedding vectors. These embeddings enable various machine learning tasks, particularly semantic similarity comparisons and applications like Retrieval Augmented Generation (RAG). The Titan Text Embeddings V2 model offers enhanced control through optional parameters such as `dimensions` (e.g., 1024, 512, 256) and `embeddingTypes` (float, binary), allowing customization of the output vector format. The `InvokeModel` operation is used to generate these embeddings, making it a crucial component for advanced AI applications.",
    "tags": [
      "embeddings",
      "text",
      "semantic search",
      "rag",
      "titan models",
      "vectors"
    ]
  },
  {
    "id": 613,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Anthropic Claude Models Overview",
    "content": "Anthropic Claude models, including Opus, Sonnet, and Haiku variants, are a prominent family of foundation models available on Amazon Bedrock, known for their advanced conversational capabilities. They offer distinct APIs: the Text Completions API for single-turn text generation and the Messages API designed for building dynamic, multi-turn conversational applications. Claude models support multimodal prompts (combining text and images) and integrate tool use (function calling) where the model requests external tool execution. Features like Extended Thinking expose the model's internal reasoning, and streaming responses provide real-time interaction.",
    "tags": [
      "conversational ai",
      "chatbots",
      "multimodal",
      "tool use",
      "llm",
      "streaming"
    ]
  },
  {
    "id": 614,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Cohere Command Models",
    "content": "Cohere Command models, including Command, Command R, and Command R+, are text generation foundation models offered on Amazon Bedrock, suitable for tasks like summarization and conversational AI. They support both synchronous `InvokeModel` and streaming `InvokeModelWithResponseStream` operations. Users can control the generation process through inference parameters such as `prompt`, `max_tokens`, `temperature`, `p`, `k`, and `stop_sequences`. The Command R and R+ models are particularly optimized for conversational applications and offer robust tool use capabilities, allowing models to interact with external functions to retrieve information or perform actions.",
    "tags": [
      "text generation",
      "conversational ai",
      "tool use",
      "inference parameters",
      "llm"
    ]
  },
  {
    "id": 615,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "DeepSeek Models",
    "content": "DeepSeek models, such as DeepSeek-R1, are available on Amazon Bedrock for a variety of text generation tasks. These models support both the `InvokeModel` and `Converse` APIs, providing inference parameters like `max_tokens`, `temperature`, and `top_p` to fine-tune the generated outputs. A distinctive characteristic of DeepSeek-R1 is that its reasoning capability is always enabled, allowing the model to return `reasoningContent` within its response, offering insights into its internal thought process. The model maintains an 8192-token context window that accommodates both input and output, including its internal reasoning tokens.",
    "tags": [
      "text generation",
      "reasoning",
      "llm",
      "inference parameters",
      "api"
    ]
  },
  {
    "id": 616,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Meta Llama Models",
    "content": "Meta Llama models, including Llama 2, Llama 3, and Llama 4 variants, are offered as foundation models on Amazon Bedrock, primarily for text generation. These models support both the `InvokeModel` and `Converse` API operations for inference. When interacting with Llama 3 models, prompts are typically structured using a specific instruction format that includes special tags like `<|begin_of_text|>` and `<|start_header_id|>user<|end_header_id|>`. Users can control the length of the generated responses by adjusting parameters such as `max_gen_len`, making it possible to produce either detailed narratives or concise summaries. The models are suitable for various conversational and analytical tasks.",
    "tags": [
      "text generation",
      "llm",
      "instruction tuning",
      "inference parameters",
      "conversational ai"
    ]
  },
  {
    "id": 617,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Mistral AI Models",
    "content": "Mistral AI offers a suite of powerful foundation models on Amazon Bedrock, including Mistral 7B Instruct, Mixtral 8X7B Instruct, Mistral Large, Mistral Small, and Pixtral Large. These models are highly effective for text generation and support conversational applications through both the `InvokeModel` and `Converse` APIs. A key capability is tool use, which allows the models to suggest and execute external functions to address user queries. Users can refine model outputs using inference parameters such as `max_tokens`, `temperature`, `top_p`, and `top_k`.",
    "tags": [
      "text generation",
      "tool use",
      "conversational ai",
      "llm",
      "inference"
    ]
  },
  {
    "id": 618,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Stable Diffusion Models",
    "content": "Stability AI Stable Diffusion models, including SDXL, Stable Image Core, Stable Image Ultra, and Stable Diffusion 3 Large, are available on Amazon Bedrock for advanced image generation tasks. These models primarily facilitate text-to-image generation and image-to-image transformations. Users can exert fine-grained control over the creative process by utilizing inference parameters such as `text_prompts` (including `negative_prompt` for exclusions), `cfg_scale`, `seed`, and `steps`. The `InvokeModel` operation is used to send generation requests, returning base64-encoded image outputs.",
    "tags": [
      "image generation",
      "text-to-image",
      "stable diffusion",
      "inference parameters",
      "image editing"
    ]
  },
  {
    "id": 619,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Agents Basics",
    "content": "Amazon Bedrock Agents enable the construction of generative AI applications that can reason through and execute complex tasks for users. These agents leverage foundation models, perform API calls to interact with enterprise systems, and can optionally query knowledge bases to gather necessary information. Their core purpose is to automate multi-step processes, effectively functioning as intelligent virtual assistants. The foundational steps for using Agents involve creating a Lambda function for custom logic, defining the agent within Amazon Bedrock, and then testing and deploying it for operational use .",
    "tags": [
      "agents",
      "automation",
      "generative ai",
      "tasks",
      "workflow"
    ]
  },
  {
    "id": 620,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Agents Application Scenarios",
    "content": "Amazon Bedrock Agents are deployed in scenarios that demand complex reasoning and task automation, such as developing sophisticated virtual assistants or specialized coaching applications. Agents can access and process information from knowledge bases to enrich their responses and execute actions by making programmatic API calls to integrated enterprise systems. A typical application workflow for agents involves creating a Lambda function to encapsulate custom business logic, configuring the agent's behavior within Bedrock, thorough testing, and finally deploying it with an alias for stable invocation. This capability allows for building powerful generative AI solutions that intelligently interact with an organization's data and operational systems.",
    "tags": [
      "automation",
      "knowledge bases",
      "api calls",
      "task execution",
      "generative applications"
    ]
  },
  {
    "id": 621,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Agents Runtime Fundamentals",
    "content": "Amazon Bedrock Agents Runtime refers to the specific API operations used for direct interaction with deployed Amazon Bedrock Agents. It's crucial to note that while general Amazon Bedrock API keys facilitate exploration of core Bedrock and Bedrock Runtime functionalities, they are not compatible with Agents for Amazon Bedrock or Agents for Amazon Bedrock Runtime API operations. This policy necessitates that authentication for Agent Runtime operations adheres to more stringent security protocols, typically leveraging AWS-wide temporary credentials or IAM roles explicitly configured for agent access, which aligns with best practices for secure production environments.",
    "tags": [
      "agents runtime",
      "authentication",
      "api access",
      "security",
      "credentials"
    ]
  },
  {
    "id": 622,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Agents Runtime Interaction Scenarios",
    "content": "Once an Amazon Bedrock Agent is successfully deployed with an alias, it becomes callable programmatically from various applications. A common interaction scenario involves invoking the agent using Python code through the AWS SDK. This enables external applications to seamlessly leverage the agent's pre-defined capabilities to perform tasks and generate intelligent responses. The agent's alias provides a stable and consistent endpoint for these programmatic invocations, facilitating its integration into diverse systems that require automated, AI-driven functionalities.",
    "tags": [
      "agent invocation",
      "programmatic access",
      "python sdk",
      "deployment",
      "integration"
    ]
  },
  {
    "id": 623,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Abuse Detection",
    "content": "Amazon Bedrock incorporates abuse detection capabilities to ensure responsible and secure use of generative AI applications. This feature actively identifies and flags content that violates predefined policies or ethical guidelines. By leveraging abuse detection, developers can maintain the integrity and safety of their AI interactions, preventing misuse of the foundation models. It helps in adhering to responsible AI principles and fostering a trustworthy environment for users. This critical safeguard assists in managing the overall security posture of AI applications.",
    "tags": [
      "abuse",
      "detection",
      "security",
      "responsible",
      "ai"
    ]
  },
  {
    "id": 624,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Creating Resources with AWS CloudFormation",
    "content": "You can create resources with AWS CloudFormation within the Amazon Bedrock ecosystem. AWS CloudFormation is a service that allows you to model and provision all your cloud infrastructure resources in a declarative way. This means you can define your Amazon Bedrock resources, such as models or agents, in a template file. Using CloudFormation enables automation, consistency, and repeatability in deploying and managing your generative AI applications' infrastructure. It streamlines the setup process and helps maintain version control for your cloud resources.",
    "tags": [
      "cloudformation",
      "resources",
      "provisioning",
      "automation"
    ]
  },
  {
    "id": 625,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock and AWS CloudFormation Templates",
    "content": "Amazon Bedrock resources are compatible with AWS CloudFormation templates. These templates are structured files, typically in JSON or YAML, that describe the desired state of your AWS resources, including those utilized by Amazon Bedrock. By using these templates, you can programmatically define and manage your generative AI components. This integration facilitates infrastructure as code practices, enabling consistent deployments across different environments and simplifying collaboration among development teams. It helps ensure that your Bedrock architectures are reproducible and version-controlled.",
    "tags": [
      "cloudformation",
      "templates",
      "bedrock",
      "infrastructure"
    ]
  },
  {
    "id": 626,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Learning More About AWS CloudFormation",
    "content": "To learn more about AWS CloudFormation, further documentation is available. AWS CloudFormation helps users spend less time managing their resources and more time focusing on their applications by providing a common language to describe and provision cloud infrastructure. It supports the entire lifecycle of your resources, from creation to deletion, in an orderly manner. Understanding CloudFormation is beneficial for efficiently managing and automating your AWS environment, including Amazon Bedrock deployments. This knowledge is key for robust and scalable cloud solutions.",
    "tags": [
      "cloudformation",
      "learn",
      "documentation",
      "aws"
    ]
  },
  {
    "id": 627,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Access Denied Exception",
    "content": "The AccessDeniedException is one of the error codes found in the Troubleshooting Amazon Bedrock API Error Codes section of the Amazon Bedrock User Guide. This exception typically indicates that the caller lacks the necessary permissions to perform the requested action. While the User Guide mentions this error code, the provided excerpts do not include a specific explanation for its dedicated section. Users encountering this error should review their IAM policies and assigned roles to ensure appropriate access for the operations they are attempting.",
    "tags": [
      "permissions",
      "access",
      "iam",
      "error"
    ]
  },
  {
    "id": 628,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Incomplete Signature",
    "content": "The IncompleteSignature error is listed within the Troubleshooting Amazon Bedrock API Error Codes section of the Amazon Bedrock User Guide. This error typically suggests an issue with the cryptographic signature included in the service request. While the provided source highlights the importance of cryptographically signing service requests, it does not offer a specific explanation for this error code itself. This can occur if the signature is malformed or missing, hindering the service from authenticating the request. Resolving it usually involves reviewing the authentication process and signature generation steps.",
    "tags": [
      "authentication",
      "signature",
      "api",
      "security"
    ]
  },
  {
    "id": 629,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Internal Failure",
    "content": "The InternalFailure error is listed under Troubleshooting Amazon Bedrock API Error Codes in the Amazon Bedrock User Guide. This error generally indicates an unexpected server-side issue within the Amazon Bedrock service. While the source identifies this as an API error code, the provided excerpts do not contain a specific explanation or details about its root causes or resolution steps. It suggests a temporary problem that may resolve itself or require further investigation by AWS. This type of error is typically retriable and outside the user's direct control.",
    "tags": [
      "server error",
      "internal",
      "api",
      "troubleshooting"
    ]
  },
  {
    "id": 630,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Invalid Action",
    "content": "The InvalidAction error is an API error code listed in the Troubleshooting Amazon Bedrock API Error Codes section. This error indicates that the requested API operation is not recognized or is not valid for the Amazon Bedrock service. The provided source excerpts list this error but do not offer a specific explanation of what constitutes an invalid action or common scenarios that lead to it. It suggests that the API call might be referencing a non-existent operation or using incorrect syntax for a valid one. Users should verify the API documentation for correct action names and parameters.",
    "tags": [
      "api call",
      "invalid",
      "syntax",
      "error"
    ]
  },
  {
    "id": 631,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Invalid Client Token ID",
    "content": "The InvalidClientTokenId error is found within the Troubleshooting Amazon Bedrock API Error Codes section. This error indicates that the client token ID provided in the request is invalid or cannot be authenticated by AWS. While the source emphasizes obtaining credentials for programmatic access, it does not provide a specific explanation for this error code in its dedicated section. This usually points to an issue with the AWS access key ID being incorrect, expired, or improperly configured. Users should verify their AWS credentials and configuration.",
    "tags": [
      "credentials",
      "authentication",
      "api key",
      "security"
    ]
  },
  {
    "id": 632,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Not Authorized",
    "content": "The NotAuthorized error is listed under Troubleshooting Amazon Bedrock API Error Codes. This error typically signifies that the authenticated user or role lacks the necessary authorization to perform the requested action on the specified resource. Although the source identifies this error, the provided excerpts do not include a specific explanation or scenarios for its occurrence. It often requires reviewing IAM policies attached to the principal making the request to ensure they grant the correct permissions for the action and resource.",
    "tags": [
      "authorization",
      "permissions",
      "iam",
      "access"
    ]
  },
  {
    "id": 633,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Request Expired",
    "content": "The RequestExpired error is listed in the Troubleshooting Amazon Bedrock API Error Codes section. This error typically occurs when the request timestamp is outside the allowed time window, indicating that the request is too old when it reaches the server. While Amazon Bedrock's API tools can retry requests, the provided source excerpts do not offer a specific explanation for this error code itself. It often points to a system clock skew between the client and AWS servers, or a request being held in a queue for too long. Synchronizing client system clocks usually resolves this issue.",
    "tags": [
      "timestamp",
      "timeout",
      "api",
      "error"
    ]
  },
  {
    "id": 634,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Service Unavailable",
    "content": "The ServiceUnavailable error is listed under Troubleshooting Amazon Bedrock API Error Codes in the Amazon Bedrock User Guide. This error indicates that the Amazon Bedrock service is temporarily unable to process the request, often due to high load or transient issues. While the source lists this as an API error code, the provided excerpts do not offer a specific explanation or guidance for this particular error. It is typically a temporary condition that resolves itself, and requests can often be retried after a short delay.",
    "tags": [
      "service",
      "unavailable",
      "retry",
      "api"
    ]
  },
  {
    "id": 635,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Throttling Exception",
    "content": "The ThrottlingException is listed in the Troubleshooting Amazon Bedrock API Error Codes section. This error indicates that the request rate has exceeded the allowed quota for the Amazon Bedrock service. The provided source identifies this error but does not give a specific explanation for it. This commonly occurs during periods of high demand and means that the service is temporarily restricting requests to maintain stability. Users can address this by implementing retry logic with exponential backoff or by requesting a quota increase.",
    "tags": [
      "throttling",
      "quotas",
      "rate limit",
      "api"
    ]
  },
  {
    "id": 636,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Validation Error",
    "content": "The ValidationError is an API error code listed within the Troubleshooting Amazon Bedrock API Error Codes section. This error signifies that one or more input parameters in the API request are invalid, missing, or improperly formatted. Although the source lists this error code, the provided excerpts do not offer a specific explanation or examples for typical validation issues. It often requires carefully reviewing the request body and parameters against the model-specific inference parameters or API specifications to correct the discrepancy.",
    "tags": [
      "validation",
      "parameters",
      "input",
      "error"
    ]
  },
  {
    "id": 637,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Resource Not Found",
    "content": "The ResourceNotFound error is listed under Troubleshooting Amazon Bedrock API Error Codes in the Amazon Bedrock User Guide. This error indicates that the specified resource (e.g., a model, endpoint, or other Bedrock resource) does not exist or cannot be found. The provided source lists this error but does not offer a specific explanation of its common causes. This can happen if a resource ID is incorrect, the resource has been deleted, or it's in a different region. Users should verify the resource identifiers and region being used.",
    "tags": [
      "resource",
      "not found",
      "id",
      "api"
    ]
  },
  {
    "id": 638,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Amazon Bedrock Service Quotas Overview",
    "content": "Amazon Bedrock imposes quotas to manage resource usage, including the frequency and size of requests you can make to its models. These quotas are Region-dependent, meaning limits can vary across different AWS Regions. You can find specific quotas for InvokeModel requests and tokens per minute at the Amazon Bedrock service quotas page. These limits help ensure fair access and operational stability across the service. If your application requires higher limits, you have the option to request an increase.",
    "tags": [
      "quotas",
      "limits",
      "service",
      "regions"
    ]
  },
  {
    "id": 639,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Understanding Token Counting in Amazon Bedrock",
    "content": "In Amazon Bedrock, tokens are fundamental units used to measure input and output for foundation models. These tokens are counted for both input prompts and generated responses, forming the basis of usage and billing. Pricing is based on the volume of input tokens and output tokens, and on whether Provisioned Throughput has been purchased. Understanding how tokens are counted is crucial for managing costs and optimizing prompt design, as pricing is directly tied to token usage.",
    "tags": [
      "tokens",
      "counting",
      "usage",
      "billing"
    ]
  },
  {
    "id": 640,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Managing Token Quotas",
    "content": "Token quota management involves understanding how Amazon Bedrock manages the maximum number of tokens allowed per request . This process is vital for ensuring your applications stay within the allocated limits and avoid being throttled . Effective management requires monitoring both input and output token counts and adjusting your application's interaction with the models accordingly. By doing so, you can optimize your generative AI workloads for consistent performance and cost-effectiveness.",
    "tags": [
      "quota management",
      "throttling",
      "usage",
      "limits"
    ]
  },
  {
    "id": 641,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Impact of max_tokens Parameter",
    "content": "The `max_tokens` parameter directly controls the maximum number of tokens a foundation model will generate in its response. Setting this parameter too low can lead to truncated or incomplete outputs, preventing the model from fully addressing the prompt. Conversely, an excessively high `max_tokens` value might result in unnecessarily long responses, potentially increasing both latency and cost. Understanding this impact is key to balancing response completeness with efficient resource utilization.",
    "tags": [
      "max_tokens",
      "parameters",
      "response length",
      "cost"
    ]
  },
  {
    "id": 642,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Optimizing max_tokens for Performance and Cost",
    "content": "Optimizing the `max_tokens` parameter is essential for efficient and cost-effective model inference in Amazon Bedrock . To achieve this, you should analyze the typical length of desired responses for your specific use cases and adjust the parameter accordingly . Avoid setting `max_tokens` much higher than necessary to prevent inflated output token costs and longer generation times . Conversely, if responses are often cut short, incrementally increasing this value can improve response quality without excessive overhead. This fine-tuning balances response fidelity with operational efficiency .",
    "tags": [
      "optimization",
      "max_tokens",
      "cost",
      "performance"
    ]
  },
  {
    "id": 643,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Monitoring Token Usage and Cost",
    "content": "To effectively monitor token usage and associated costs in Amazon Bedrock, you should observe the `inputTextTokenCount` and `outputTokenCount` values returned in model responses. These metrics provide the number of tokens consumed by the prompt and generated in the response, respectively. The cost of processing these input and output tokens is directly related to these counts. By tracking these metrics, developers can gain insights into their model interactions, manage budgets, and make informed decisions about prompt design and model selection.",
    "tags": [
      "monitoring",
      "usage",
      "cost",
      "token counts"
    ]
  },
  {
    "id": 644,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Token Counting Support Across Models and Regions",
    "content": "Token counting is a feature supported by all models in Amazon Bedrock. This ensures that you can monitor the number of input and output tokens regardless of the specific foundation model you choose. Regionally, token counting is broadly available in numerous AWS Regions, including US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Mumbai), and Europe (Frankfurt). This extensive support allows for consistent usage and cost tracking across diverse deployments of your generative AI applications.",
    "tags": [
      "supported models",
      "regions",
      "token counting",
      "availability"
    ]
  },
  {
    "id": 645,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Retrieving Token Counts from Model Responses",
    "content": "When you interact with Amazon Bedrock models, the token count for both input and output is returned within the model's response. For example, in responses from Amazon Titan Text models, fields like `inputTextTokenCount` (for the prompt) and `tokenCount` (for the output) provide these metrics. Similarly, for Anthropic Claude messages and Meta Llama 3 Instruct, the response includes `input_tokens` and `output_tokens` or `prompt_token_count` and `generation_token_count` fields, respectively. This direct inclusion in the response simplifies usage analysis and cost management.",
    "tags": [
      "token count",
      "API response",
      "input tokens",
      "output tokens"
    ]
  },
  {
    "id": 646,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Example: Retrieving Token Counts with Python SDK (Boto3)",
    "content": "You can retrieve token counts using the AWS SDK for Python (Boto3) after invoking a model. For instance, in an example for Amazon Titan Text Premier, the generated response includes an `inputTextTokenCount` and a `tokenCount` for the `outputText`. For DeepSeek-R1, a `converse` API call returns `usage` metrics, including `inputTokens`, `outputTokens`, and `totalTokens`. These values are extracted from the JSON response body to monitor usage. This allows programmatic access to token metrics for analysis.",
    "tags": [
      "example",
      "python",
      "boto3",
      "API"
    ]
  },
  {
    "id": 647,
    "service": "Amazon Bedrock",
    "category": "Batch Inference",
    "title": "Requesting Quota Increases",
    "content": "If your generative AI application requires higher limits than the default Amazon Bedrock quotas, you can request an increase. This process is managed through the AWS Management Console, specifically within the Service Quotas section. When submitting a request, it is important to clearly state your use case and the specific quota you need increased. This allows AWS to review your operational requirements and potentially grant higher limits, enabling your applications to scale effectively without interruption.",
    "tags": [
      "quota increase",
      "limits",
      "service quotas",
      "request"
    ]
  }
]